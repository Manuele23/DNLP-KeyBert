{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662213f8",
   "metadata": {},
   "source": [
    "# KeyBERT with Sentiment-aware Embedding Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1106b08",
   "metadata": {},
   "source": [
    "This notebook introduces a **sentiment-aware extension** of the KeyBERT keyword extraction model, which integrates sentiment information directly into the candidate selection and ranking process. Unlike simple post-hoc reranking approaches, this method incorporates sentiment consistency during both candidate filtering and final keyword scoring.\n",
    "\n",
    "### Theoretical Approach\n",
    "\n",
    "Traditional KeyBERT extracts candidate keywords purely based on semantic similarity between the document embedding and candidate embeddings. This extension enhances the process by considering the **emotional coherence** between candidates and the document, operationalized as continuous sentiment polarity scores.\n",
    "\n",
    "Given:\n",
    "-  $\\text{sim}_{sem}$ : cosine similarity between document and candidate embeddings,\n",
    "-  $s_{doc} \\in [0,1]$: continuous sentiment polarity score of the document,\n",
    "-  $s_{cand} \\in [0,1]$: continuous sentiment polarity score of a candidate keyword,\n",
    "\n",
    "we define the **sentiment alignment score** as:\n",
    "\n",
    "$$\n",
    "\\text{align}(s_{doc}, s_{cand}) = 1 - |s_{doc} - s_{cand}|\n",
    "$$\n",
    "\n",
    "which equals 1 for perfect polarity match and decreases linearly to 0 for maximal polarity difference.\n",
    "\n",
    "The overall combined score used to filter and rank candidates is:\n",
    "\n",
    "$$\n",
    "\\text{score}_{final} = w_{sentiment} \\times \\text{align}(s_{doc}, s_{cand}) + (1 - w_{sentiment}) \\times \\text{sim}_{sem}\n",
    "$$\n",
    "\n",
    "where $$ w_{sentiment} \\in [0,1] $$ is a tunable weight balancing sentiment alignment and semantic similarity.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Integrated sentiment filtering**: Sentiment is incorporated early to filter out candidates that are sentimentally incongruent with the document, not only at reranking stage.\n",
    "- **Continuous sentiment modeling**: Uses probability-weighted sentiment polarity scores from a pretrained transformer classifier, enabling nuanced sentiment comparisons.\n",
    "- **Flexible weighting parameter**: The parameter \\( w_{sentiment} \\) allows task-specific tuning of the relative importance of sentiment versus semantic relevance.\n",
    "- **Candidate generation enhancement**: The candidate pool is initially large and filtered by combined semantic and sentiment scores, improving quality and relevance.\n",
    "\n",
    "### Advantages Over Post-hoc Reranking\n",
    "\n",
    "- Unlike reranking approaches that adjust keyword order **after** candidate generation, this method filters candidates **before** ranking, reducing noise and irrelevant candidates early.\n",
    "- Sentiment influences the candidate pool itself, resulting in more coherent and contextually appropriate keyword extraction.\n",
    "- The approach remains compatible with any KeyBERT-compatible embedding model and sentiment classification backend.\n",
    "\n",
    "### Intended Applications\n",
    "\n",
    "This sentiment-aware KeyBERT extension is especially suited for sentiment-rich domains such as:\n",
    "\n",
    "- Product and service reviews\n",
    "- Social media opinion mining\n",
    "- Customer feedback analysis\n",
    "- Any text where emotional tone is critical to understanding key themes\n",
    "\n",
    "It enables the extraction of keywords that are both topically relevant and emotionally aligned, enhancing interpretability and downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2118e",
   "metadata": {},
   "source": [
    "### Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "510ac332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "torch is already installed.\n",
      "Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "keybert is already installed.\n",
      "transformers is already installed.\n",
      "sentence_transformers is already installed.\n",
      "collections is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"numpy\", \"torch\", \"scikit-learn\", \"keybert\", \"transformers\", \"sentence_transformers\", \"collections\"\n",
    "    \n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5d59f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Fundamental package for numerical computing in Python\n",
    "from typing import Tuple  # Used for type hinting tuples in function signatures\n",
    "\n",
    "import torch  # Core PyTorch library for tensor computations\n",
    "import torch.nn.functional as F  # Functional interface for activation functions, etc.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Extract text n-gram candidates\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Compute cosine similarity between embeddings\n",
    "\n",
    "from keybert import KeyBERT as KB  # KeyBERT keyword extraction base class\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,  # Tokenizer for preparing input text for transformer models\n",
    "    AutoModelForSequenceClassification  # Transformer model for classification tasks\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # Sentence transformer for generating sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c140c4",
   "metadata": {},
   "source": [
    "# Classes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f387a9e",
   "metadata": {},
   "source": [
    "## SentimentModel Class: Transformer-based Sentiment Probability Predictor\n",
    "\n",
    "The `SentimentModel` class is a wrapper around a pretrained HuggingFace transformer model designed for sentiment classification. It provides a convenient interface to obtain **probability distributions over sentiment classes** for batches of input texts.\n",
    "\n",
    "### Purpose and Functionality\n",
    "\n",
    "- **Model loading:**  \n",
    "  Upon initialization, the class loads both the tokenizer and the sequence classification model specified by the `model_name`.  \n",
    "  By default, it uses `\"nlptown/bert-base-multilingual-uncased-sentiment\"`, a multilingual BERT model fine-tuned for 5-class sentiment classification (1 to 5 stars).\n",
    "\n",
    "- **Device management:**  \n",
    "  The model and tokenizer are moved to the specified device (`cpu` or `cuda`).  \n",
    "  Input validation ensures that `cuda` is only used if a compatible GPU is available.\n",
    "\n",
    "- **Batch sentiment prediction:**  \n",
    "  The core method `predict_proba` takes a list of texts and:  \n",
    "  1. Tokenizes and encodes them into the format expected by the transformer.  \n",
    "  2. Performs a forward pass through the model without computing gradients (efficient inference).  \n",
    "  3. Applies a softmax to the output logits to obtain a probability distribution over the sentiment classes for each text.  \n",
    "  4. Returns a NumPy array of shape `(batch_size, num_classes)` containing the class probabilities.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Allows seamless integration of sentiment analysis into larger NLP pipelines.\n",
    "- Outputs probabilistic sentiment scores, enabling nuanced, continuous sentiment representations rather than hard labels.\n",
    "- Supports batch processing for efficiency.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "**Text:**  \n",
    "_I absolutely loved this movie! It was fantastic._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.01 0.02 0.05 0.12 0.80]\n",
    "\n",
    "\n",
    "**Text:**  \n",
    "_The plot was boring and predictable._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.70 0.20 0.07 0.02 0.01]\n",
    "\n",
    "\n",
    "**Text:**  \n",
    "_The movie was okay, nothing special but not bad either._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.05 0.10 0.65 0.15 0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ca48c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    \"\"\"\n",
    "    Wrapper class for a HuggingFace transformer sentiment classification model.\n",
    "\n",
    "    This class loads a pretrained sentiment classification model and tokenizer,\n",
    "    and provides a method to compute the probability distribution over sentiment classes\n",
    "    for a batch of input texts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str, optional\n",
    "        The identifier of the pretrained sentiment model on HuggingFace Hub.\n",
    "        Default is \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        a 5-class sentiment classifier (1 to 5 stars).\n",
    "\n",
    "    device : str, optional\n",
    "        The device to run the model on. Typical values: \"cpu\" or \"cuda\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\", \n",
    "            device=\"cpu\"):\n",
    "        \n",
    "        # Set the device for model computation\n",
    "        if device not in [\"cpu\", \"cuda\"]:\n",
    "            raise ValueError(\"Device must be 'cpu' or 'cuda'.\")\n",
    "        \n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer associated with the pretrained model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Load the pretrained sequence classification model on the given device\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"\n",
    "        Compute the probability distribution over sentiment classes for input texts.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list of str\n",
    "            List of input texts for which to compute sentiment probabilities.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Array of shape (len(texts), num_classes) where each row corresponds\n",
    "            to the probability distribution over sentiment classes for that text.\n",
    "        \"\"\"\n",
    "        # Tokenize and encode the input texts, handling padding and truncation for batching\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Perform forward pass without gradient computation for efficiency\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits  # raw model outputs before softmax\n",
    "            \n",
    "            # Convert logits to probabilities using softmax along class dimension\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9c9ed",
   "metadata": {},
   "source": [
    "## KeyBERTSentimentAware Class: Sentiment-Integrated Keyword Extraction\n",
    "\n",
    "This class extends the base KeyBERT model by integrating sentiment analysis directly into the keyword extraction pipeline. It enhances the traditional semantic-only approach by incorporating continuous sentiment polarity scores for both the entire document and each candidate keyword.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Candidate Extraction:**  \n",
    "  Uses `CountVectorizer` to extract a broad pool of candidate keywords (n-grams) from the document text.  \n",
    "  **Note:** This initial candidate generation is purely statistical and **does not incorporate sentiment information**.\n",
    "\n",
    "- **Sentiment Analysis:**  \n",
    "  Leverages a pretrained transformer sentiment classification model to compute **continuous sentiment polarity scores** ranging from 0 (very negative) to 1 (very positive) for both the document and each candidate.\n",
    "\n",
    "- **Combined Scoring and Filtering:**  \n",
    "  Calculates a weighted score combining:\n",
    "  - Semantic similarity (cosine similarity between embeddings).\n",
    "  - Sentiment alignment (1 minus the absolute difference between candidate and document sentiment).\n",
    "\n",
    "  Candidates with combined scores below a threshold are **filtered out before final ranking**, effectively integrating sentiment as a filter immediately after candidate extraction.\n",
    "\n",
    "  The weighting is controlled by `weight_sentiment`:\n",
    "  - `weight_sentiment=1.0` means keywords are ranked purely by sentiment alignment.\n",
    "  - `weight_sentiment=0.0` means keywords are ranked purely by semantic similarity.\n",
    "\n",
    "### Candidate Selection in KeyBERT vs Sentiment-Aware Extension\n",
    "\n",
    "In the original KeyBERT model, the candidate keywords are extracted purely based on **statistical properties** of the text. Specifically, KeyBERT uses a tool like `CountVectorizer` to identify n-grams (contiguous sequences of words) that appear frequently or are relevant according to basic frequency statistics. This means:\n",
    "\n",
    "- The **candidate pool is generated without any semantic or sentiment understanding**.\n",
    "- All candidates are treated equally in this phase, regardless of their emotional tone or contextual relevance beyond raw occurrence patterns.\n",
    "\n",
    "This purely **statistical candidate extraction** can lead to a large number of candidates that are relevant but may not align emotionally with the overall document sentiment. For example, in a strongly negative review, KeyBERT might still generate positive-sounding candidates simply because those phrases appear often, potentially misrepresenting the sentiment conveyed.\n",
    "\n",
    "To address this limitation, our sentiment-aware extension introduces a **joint filtering mechanism** that combines both semantic relevance and sentiment alignment **immediately after the initial statistical candidate extraction**:\n",
    "\n",
    "1. We first extract a large pool of candidates statistically using `CountVectorizer` to ensure broad coverage.\n",
    "\n",
    "2. We compute **continuous sentiment polarity scores** for both the entire document and each candidate keyword using a pretrained transformer sentiment model.\n",
    "\n",
    "3. We calculate a **combined score** for each candidate that balances:\n",
    "   - Semantic similarity to the document (embedding cosine similarity).\n",
    "   - Sentiment alignment with the document's overall polarity (inverted absolute difference between sentiment scores).\n",
    "\n",
    "4. Candidates whose combined score falls below a threshold are **filtered out early**, significantly reducing the pool to those that are both topically and emotionally relevant.\n",
    "\n",
    "This approach allows the model to **avoid candidates that are semantically plausible but sentimentally inconsistent**, leading to more meaningful and context-aware keyword extraction.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Step                        | KeyBERT Base                        | Sentiment-Aware Extension          |\n",
    "|-----------------------------|-----------------------------------|-----------------------------------|\n",
    "| Candidate generation         | Purely statistical (n-gram counts)| Statistical, followed by sentiment-semantic filtering (no sentiment during extraction but sentiment used immediately after to filter) |\n",
    "| Candidate ranking            | Semantic similarity only           | Semantic + sentiment combined     |\n",
    "| Sentiment consideration     | None                              | Integral part of candidate filtering|\n",
    "\n",
    "By incorporating sentiment as an early filtering step (post-statistical extraction), our extension improves the **precision and emotional coherence** of extracted keywords, especially in domains where sentiment plays a crucial role.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `model`: The base semantic embedding model (usually a SentenceTransformer).\n",
    "- `sentiment_model_name`: Identifier of the pretrained sentiment model (default is a 5-class multilingual sentiment classifier).\n",
    "- `weight_sentiment`: Balances importance between sentiment alignment and semantic similarity.\n",
    "- `candidate_pool_size`: Number of candidates initially extracted.\n",
    "- `device`: Compute device, `\"cpu\"` or `\"cuda\"`.\n",
    "\n",
    "### Usage\n",
    "\n",
    "The class allows flexible, context-aware keyword extraction that respects both topical relevance and emotional tone, ideal for analyzing opinion-rich texts such as reviews or social media posts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d9a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyBERTSentimentAware(KB):\n",
    "    \"\"\"\n",
    "    Extension of KeyBERT to integrate sentiment analysis in keyword extraction.\n",
    "\n",
    "    This class overrides and extends parts of KeyBERT's pipeline to:\n",
    "    - Extract a larger candidate pool using CountVectorizer.\n",
    "    - Calculate sentiment polarity scores for the document and candidates,\n",
    "      using a pretrained sentiment classification model with continuous outputs.\n",
    "    - Combine semantic similarity and sentiment alignment scores via a weighting factor weight_sentiment.\n",
    "    - Filter candidate keywords based on this combined score before final ranking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SentenceTransformer\n",
    "        Semantic embedding model used by KeyBERT.\n",
    "\n",
    "    sentiment_model_name : str, optional (default: \"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        Identifier of pretrained sentiment model on HuggingFace Hub.\n",
    "\n",
    "    weight_sentiment : float, optional (default: 0.7)\n",
    "        Weight to balance sentiment alignment vs semantic similarity.\n",
    "        weight_sentiment=1.0 means only sentiment alignment is considered.\n",
    "        weight_sentiment=0.0 means only semantic similarity is considered.\n",
    "\n",
    "    candidate_pool_size : int, optional (default: 100)\n",
    "        Maximum number of initial candidate keywords to extract.\n",
    "\n",
    "    device : str, optional (default: \"cpu\")\n",
    "        Device to run embedding and sentiment models on (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        sentiment_model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        weight_sentiment: float = 0.7,\n",
    "        candidate_pool_size: int = 100,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        # Validate that the specified device is either 'cpu' or 'cuda'\n",
    "        valid_devices = {\"cpu\", \"cuda\"}\n",
    "        if device not in valid_devices:\n",
    "            raise ValueError(f\"Device must be one of {valid_devices}.\")\n",
    "        \n",
    "        # Check CUDA availability if 'cuda' is requested\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "\n",
    "        # Validate input types to ensure correct usage\n",
    "        if not isinstance(model, SentenceTransformer):\n",
    "            raise TypeError(\"model must be an instance of SentenceTransformer.\")\n",
    "        if not isinstance(sentiment_model_name, str):\n",
    "            raise TypeError(\"sentiment_model_name must be a string.\")\n",
    "        if not isinstance(weight_sentiment, float):\n",
    "            raise TypeError(\"weight_sentiment must be a float.\")\n",
    "        if not isinstance(candidate_pool_size, int):\n",
    "            raise TypeError(\"candidate_pool_size must be an integer.\")\n",
    "\n",
    "        # Validate value ranges to prevent logical errors\n",
    "        if not (0.0 <= weight_sentiment <= 1.0):\n",
    "            raise ValueError(\"weight_sentiment must be between 0 and 1 inclusive.\")\n",
    "        if candidate_pool_size <= 0:\n",
    "            raise ValueError(\"candidate_pool_size must be a positive integer.\")\n",
    "\n",
    "        # Initialize the superclass (KeyBERT) with the semantic embedding model\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Assign validated parameters to instance variables\n",
    "        self.weight_sentiment = weight_sentiment\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        self.device = device\n",
    "\n",
    "        # Store the semantic embedding model for embedding computation\n",
    "        self.embedder = model\n",
    "\n",
    "        # Initialize the sentiment model wrapper with the given model name and device\n",
    "        self.sentiment_model = SentimentModel(sentiment_model_name, device=device)\n",
    "\n",
    "        # Define the ordered sentiment labels corresponding to model output classes\n",
    "        self.labels_ordered = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "\n",
    "        # Create a mapping from sentiment labels to continuous numeric scores between 0 and 1\n",
    "        self.label_to_score = {\n",
    "            label: i / (len(self.labels_ordered) - 1)\n",
    "            for i, label in enumerate(self.labels_ordered)\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_doc_polarity_continuous(self, doc: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the document's continuous sentiment polarity score as the weighted sum of\n",
    "        predicted class probabilities multiplied by their numeric mappings.\n",
    "\n",
    "        This method overrides and replaces any default sentiment handling in the base class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            The document text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Continuous sentiment polarity score between 0 (very negative) and 1 (very positive).\n",
    "        \"\"\"\n",
    "        # Get probability distribution over sentiment classes for the document\n",
    "        probs = self.sentiment_model.predict_proba([doc])[0]\n",
    "\n",
    "        # Compute continuous polarity as weighted average of class scores\n",
    "        polarity = sum(\n",
    "            p * self.label_to_score[label]\n",
    "            for p, label in zip(probs, self.labels_ordered)\n",
    "        )\n",
    "        return polarity\n",
    "\n",
    "    def _get_candidate_polarities(self, candidates) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute continuous sentiment polarity scores for each candidate keyword.\n",
    "\n",
    "        This method extends candidate scoring with sentiment, overriding base candidate processing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        candidates : iterable of str\n",
    "            List of candidate keywords.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of polarity scores for each candidate keyword.\n",
    "        \"\"\"\n",
    "        candidates = list(candidates)  # ensure correct input format for tokenizer\n",
    "        \n",
    "        # Batch predict probabilities for all candidates\n",
    "        probs_list = self.sentiment_model.predict_proba(candidates)\n",
    "        \n",
    "        polarities = []\n",
    "        for probs in probs_list:\n",
    "            # Weighted average as continuous polarity score\n",
    "            polarity = sum(\n",
    "                p * self.label_to_score[label]\n",
    "                for p, label in zip(probs, self.labels_ordered)\n",
    "            )\n",
    "            polarities.append(polarity)\n",
    "        return np.array(polarities)\n",
    "\n",
    "    def _select_candidates(\n",
    "            self, \n",
    "            doc: str, \n",
    "            ngram_range: Tuple[int, int] = (1, 3), \n",
    "            threshold: float = 0.4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract initial candidates with CountVectorizer and filter them based on combined\n",
    "        semantic similarity and sentiment alignment scores.\n",
    "\n",
    "        This method replaces the default candidate generation and filtering steps of KeyBERT,\n",
    "        incorporating sentiment filtering before final keyword ranking.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Document text.\n",
    "\n",
    "        ngram_range : tuple of int\n",
    "            N-gram size range for candidate extraction.\n",
    "\n",
    "        threshold : float\n",
    "            Minimum combined score for candidate retention.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of str\n",
    "            Filtered list of candidate keywords.\n",
    "        \"\"\"\n",
    "        # Extract candidates with CountVectorizer (statistical n-grams)\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english',\n",
    "            max_features=self.candidate_pool_size\n",
    "        )\n",
    "        candidates = vectorizer.fit([doc]).get_feature_names_out()\n",
    "\n",
    "        # Compute semantic embeddings for doc and candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity scores\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "\n",
    "        # Combine semantic and sentiment scores with alpha weighting\n",
    "        combined_scores = self.weight_sentiment * sentiment_scores + (1 - self.weight_sentiment) * sim_scores\n",
    "\n",
    "        # Filter candidates that meet threshold on combined score\n",
    "        filtered_candidates = [c for c, s in zip(candidates, combined_scores) if s >= threshold]\n",
    "\n",
    "        return filtered_candidates\n",
    "\n",
    "    def extract_keywords(\n",
    "        self,\n",
    "        doc: str,\n",
    "        top_n: int = 5,\n",
    "        candidate_threshold: float = 0.4,\n",
    "        keyphrase_ngram_range: Tuple[int, int] = (1, 3),\n",
    "        print_doc_polarity: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract top keywords from a document by combining semantic similarity and sentiment alignment.\n",
    "\n",
    "        This method overrides the `extract_keywords` method from KeyBERT base class,\n",
    "        adding sentiment-aware candidate filtering and scoring.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Input document text.\n",
    "\n",
    "        top_n : int\n",
    "            Number of keywords to return.\n",
    "\n",
    "        candidate_threshold : float\n",
    "            Threshold score to filter candidate keywords.\n",
    "\n",
    "        keyphrase_ngram_range : tuple of int\n",
    "            N-gram range for candidate keyword extraction.\n",
    "\n",
    "        print_doc_polarity : bool\n",
    "            Whether to print the document's sentiment polarity score.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (keyword, score) tuples sorted by descending combined score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Select candidates filtered by combined semantic+sentiment scoring\n",
    "        candidates = self._select_candidates(\n",
    "            doc,\n",
    "            ngram_range=keyphrase_ngram_range,\n",
    "            threshold=candidate_threshold\n",
    "        )\n",
    "        if not candidates:\n",
    "            print(\"No candidates passed the sentiment-semantic filter.\")\n",
    "            return []\n",
    "\n",
    "        # Compute semantic embeddings for document and filtered candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity in [-1,1]\n",
    "        from sklearn.preprocessing import normalize\n",
    "        doc_emb_norm = normalize(doc_emb)\n",
    "        cand_emb_norm = normalize(cand_emb)\n",
    "\n",
    "        # Compute continuous sentiment polarity for the document\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "\n",
    "        # Print document polarity if requested\n",
    "        if print_doc_polarity:\n",
    "            # Scale polarity from [0,1] to [0,10]\n",
    "            scaled_pol = doc_pol * 10\n",
    "\n",
    "            # Determine polarity label with neutral zone between 4 and 6 on 0-10 scale\n",
    "            if scaled_pol < 4:\n",
    "                polarity_label = \"Negative\"\n",
    "            elif scaled_pol > 6:\n",
    "                polarity_label = \"Positive\"\n",
    "            else:\n",
    "                polarity_label = \"Neutral\"\n",
    "\n",
    "            print(f\"\\n=== Document Polarity Score: {scaled_pol:.2f} ({polarity_label}) ===\\n\")\n",
    "\n",
    "        # Compute sentiment polarities for candidates\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores (range [-1,1])\n",
    "        sim_scores = cosine_similarity(doc_emb_norm, cand_emb_norm)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores in [0,1]\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "\n",
    "        # Map sentiment alignment from [0,1] to [-1,1]\n",
    "        sentiment_scores_mapped = 2 * sentiment_scores - 1\n",
    "\n",
    "        # Final combined score with weighting factor weight_sentiment in [-1,1]\n",
    "        final_scores = self.weight_sentiment * sentiment_scores_mapped + (1 - self.weight_sentiment) * sim_scores\n",
    "\n",
    "        # Select top_n keywords sorted by combined score descending\n",
    "        top_indices = np.argsort(final_scores)[-top_n:][::-1]\n",
    "\n",
    "        return [(candidates[i], final_scores[i]) for i in top_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d991b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1533b",
   "metadata": {},
   "source": [
    "## Test 1: Basic Keyword Extraction with Sentiment-Aware KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebd768",
   "metadata": {},
   "source": [
    "This first test demonstrates the basic usage of the `KeyBERTSentimentAware` class on a simple document. We extract the top keywords combining semantic similarity and sentiment alignment with default parameter settings.\n",
    "\n",
    "**Objectives:**\n",
    "- Verify that the class instantiates correctly.\n",
    "- Check that keywords are extracted without errors.\n",
    "- Observe the impact of sentiment-aware ranking on keyword scores.\n",
    "\n",
    "We use a short, clearly positive sentence to observe how sentiment affects the keyword selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "48528652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 9.31 (Positive) ===\n",
      "\n",
      "Extracted Keywords and Scores:\n",
      "\n",
      "movie fantastic beautiful \t score: 0.9029\n",
      "movie fantastic      \t score: 0.8555\n",
      "great acting         \t score: 0.8540\n",
      "beautiful visuals great \t score: 0.8231\n",
      "fantastic beautiful visuals \t score: 0.8135\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "# Sample document with positive sentiment\n",
    "doc = \"The movie was fantastic with beautiful visuals and great acting.\"\n",
    "\n",
    "# Extract top 5 keywords\n",
    "keywords = kw_model.extract_keywords(doc, top_n=5, print_doc_polarity=True)\n",
    "\n",
    "print(\"Extracted Keywords and Scores:\\n\")\n",
    "for keyword, score in keywords:\n",
    "    print(f\"{keyword:20s} \\t score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7016e7",
   "metadata": {},
   "source": [
    "## Test 2: Comparing Sentiment-Aware KeyBERT with Base KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e6b8e",
   "metadata": {},
   "source": [
    "In this test, we compare the keywords extracted by the sentiment-aware extension with those from the original KeyBERT model that relies solely on semantic similarity.\n",
    "\n",
    "**Objectives:**\n",
    "- Highlight differences in keyword selection between semantic-only and sentiment-aware approaches.\n",
    "- Understand the effect of integrating sentiment on keyword ranking.\n",
    "- Use the same document for a fair comparison.\n",
    "\n",
    "We use a document containing both positive and negative elements to see how sentiment influences the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e2c80e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE KeyBERT Keywords\n",
      "pacing               0.3888\n",
      "film                 0.3627\n",
      "soundtrack           0.3257\n",
      "predictable          0.3226\n",
      "plot                 0.2605\n",
      "\n",
      "=== Document Polarity Score: 3.22 (Negative) ===\n",
      "\n",
      "Sentiment-Aware KeyBERT Keywords:\n",
      "\n",
      "slow                 0.7235\n",
      "predictable          0.6371\n",
      "plot                 0.4930\n",
      "pacing               0.3587\n",
      "effects              0.3586\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize base KeyBERT model with the same embedding model\n",
    "kw_base = KB(model=embedding_model)\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_sentiment = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc_mixed = (\n",
    "    \"The film had stunning visual effects and an amazing soundtrack, \"\n",
    "    \"but the plot was predictable and the pacing was slow at times.\"\n",
    ")\n",
    "\n",
    "# Extract keywords using base KeyBERT\n",
    "base_keywords = kw_base.extract_keywords(doc_mixed, top_n=5, keyphrase_ngram_range=(1, 1))\n",
    "\n",
    "print(\"BASE KeyBERT Keywords:\\b\")\n",
    "for kw, score in base_keywords:\n",
    "    print(f\"{kw:20s} {score:.4f}\")\n",
    "\n",
    "# Extract keywords using sentiment-aware KeyBERT (with weight_sentiment=0.7)\n",
    "sentiment_keywords = kw_sentiment.extract_keywords(\n",
    "    doc_mixed,\n",
    "    top_n=5,\n",
    "    keyphrase_ngram_range=(1, 1),\n",
    "    print_doc_polarity=True\n",
    ")\n",
    "\n",
    "print(\"Sentiment-Aware KeyBERT Keywords:\\n\")\n",
    "for kw, score in sentiment_keywords:\n",
    "    print(f\"{kw:20s} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d686562",
   "metadata": {},
   "source": [
    "## Test 3: Candidate Filtering with Different Thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da344ff",
   "metadata": {},
   "source": [
    "\n",
    "In this test, we explore how varying the candidate filtering threshold affects the pool of candidate keywords before the final ranking.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the impact of the `candidate_threshold` parameter on candidate selection.\n",
    "- Observe how stricter thresholds reduce candidate pool size and potentially increase keyword relevance.\n",
    "- Use a moderately complex document with mixed sentiment.\n",
    "\n",
    "This test highlights the balance between recall and precision in candidate filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db445b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Threshold: 0.0\n",
      "Number of candidates after filtering: 27\n",
      "Candidates: ['beautiful', 'beautiful cinematography', 'beautiful cinematography strong', 'cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong', 'strong performances', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.2\n",
      "Number of candidates after filtering: 27\n",
      "Candidates: ['beautiful', 'beautiful cinematography', 'beautiful cinematography strong', 'cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong', 'strong performances', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.4\n",
      "Number of candidates after filtering: 26\n",
      "Candidates: ['beautiful cinematography', 'beautiful cinematography strong', 'cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong', 'strong performances', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.6\n",
      "Number of candidates after filtering: 16\n",
      "Candidates: ['cinematography', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow times', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 0.8\n",
      "Number of candidates after filtering: 2\n",
      "Candidates: ['difficult follow times', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 1.0\n",
      "Number of candidates after filtering: 0\n",
      "Candidates: []\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc = (\n",
    "    \"Despite the beautiful cinematography and strong performances, \"\n",
    "    \"the storyline was convoluted and difficult to follow at times.\"\n",
    ")\n",
    "\n",
    "thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nCandidate Threshold: {thresh}\")\n",
    "    candidates = kw_model._select_candidates(doc, threshold=thresh)\n",
    "    print(f\"Number of candidates after filtering: {len(candidates)}\")\n",
    "    print(\"Candidates:\", candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10aa95",
   "metadata": {},
   "source": [
    "## Test 4: Impact of Sentiment Weighting (`weight_sentiment`) on Keyword Ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90d8fd",
   "metadata": {},
   "source": [
    "\n",
    "This test examines how changing the `weight_sentiment` parameter influences the balance between semantic similarity and sentiment alignment in keyword scoring.\n",
    "\n",
    "**Objectives:**\n",
    "- Observe differences in extracted keywords when prioritizing sentiment vs. semantic relevance.\n",
    "- Understand the flexibility of the model in adapting to different use cases by tuning `weight_sentiment`.\n",
    "- Use a document with both positive and negative sentiments to highlight effect.\n",
    "\n",
    "We test three values of `weight_sentiment`: 0.0 (semantic only), 0.5 (balanced), and 1.0 (sentiment only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32000df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "weight_sentiment = 0.0\n",
      "\n",
      "film ending          \t score: 0.5824\n",
      "breathtaking visuals \t score: 0.4919\n",
      "outstanding soundtrack \t score: 0.4868\n",
      "story lead           \t score: 0.4825\n",
      "heartfelt storytelling \t score: 0.4742\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "weight_sentiment = 0.25\n",
      "\n",
      "story lead           \t score: 0.6053\n",
      "film ending          \t score: 0.5888\n",
      "breathtaking visuals \t score: 0.5784\n",
      "narrative arc        \t score: 0.5625\n",
      "audience story       \t score: 0.5625\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "weight_sentiment = 0.5\n",
      "\n",
      "story lead           \t score: 0.7281\n",
      "narrative arc        \t score: 0.6953\n",
      "audience story       \t score: 0.6869\n",
      "characters pacing    \t score: 0.6867\n",
      "lead actors          \t score: 0.6837\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "weight_sentiment = 0.75\n",
      "\n",
      "story lead           \t score: 0.8509\n",
      "lead actors          \t score: 0.8311\n",
      "narrative arc        \t score: 0.8281\n",
      "characters pacing    \t score: 0.8130\n",
      "storytelling         \t score: 0.8124\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "weight_sentiment = 1.0\n",
      "\n",
      "lead actors          \t score: 0.9785\n",
      "audience             \t score: 0.9778\n",
      "story lead           \t score: 0.9736\n",
      "actors               \t score: 0.9643\n",
      "narrative            \t score: 0.9640\n",
      "\n",
      "weight_sentiment = -0.2 caused error: weight_sentiment value -0.2 is out of valid range [0,1].\n",
      "\n",
      "weight_sentiment = 1.2 caused error: weight_sentiment value 1.2 is out of valid range [0,1].\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc = (\"The film featured breathtaking visuals and an outstanding soundtrack that \"\n",
    "       \"truly immersed the audience in the story. The lead actors delivered powerful \"\n",
    "       \"performances, bringing depth and emotion to their characters. However, the pacing \"\n",
    "       \"was somewhat slow in the middle, and certain plot points felt predictable and underdeveloped. \"\n",
    "       \"Despite these shortcomings, the compelling narrative arc and the strong direction kept \"\n",
    "       \"the viewers engaged throughout. The film's ending was uplifting and satisfying, \"\n",
    "       \"leaving a lasting impression. Overall, it was a highly enjoyable experience that combined \"\n",
    "       \"artistic excellence with heartfelt storytelling.\")\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0, -0.2, 1.2]  # Includes invalid weights\n",
    "\n",
    "for w in weights:\n",
    "    try:\n",
    "        kw_model.weight_sentiment = w\n",
    "        # Manually check weight range, raise error if invalid\n",
    "        if not (0.0 <= w <= 1.0):\n",
    "            raise ValueError(f\"weight_sentiment value {w} is out of valid range [0,1].\")\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            doc, \n",
    "            top_n=5, \n",
    "            print_doc_polarity=True, \n",
    "            keyphrase_ngram_range=(1, 2)\n",
    "        )\n",
    "        print(f\"\\nweight_sentiment = {w}\\n\")\n",
    "        for kw, score in keywords:\n",
    "            print(f\"{kw:20s} \\t score: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nweight_sentiment = {w} caused error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c969d36",
   "metadata": {},
   "source": [
    "## Test 5: Keyword Extraction on a Document with Mixed Sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bfa0f",
   "metadata": {},
   "source": [
    "This test evaluates the performance of the `KeyBERTSentimentAware` model on a document containing both positive and negative sentiments. It helps observe how the model balances semantic relevance and sentiment alignment when the document expresses contrasting opinions.\n",
    "\n",
    "**Objectives:**\n",
    "- Verify that the model extracts keywords reflecting both the positive and negative aspects of the text.\n",
    "- Assess the impact of sentiment-aware filtering and scoring in realistic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c89d3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 4.72 (Neutral) ===\n",
      "\n",
      "Keywords extracted from mixed sentiment document:\n",
      "actors performed adequately \t score: 0.8231\n",
      "storyline straightforward \t score: 0.7820\n",
      "adequately storyline straightforward \t score: 0.7623\n",
      "flaws soundtrack complemented \t score: 0.7451\n",
      "adequately storyline \t score: 0.7011\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc_mixed = (\n",
    "    \"The movie had decent cinematography and the actors performed adequately. \"\n",
    "    \"The storyline was straightforward and predictable, with neither surprising twists nor major flaws. \"\n",
    "    \"The soundtrack complemented the scenes suitably, without standing out. \"\n",
    "    \"Overall, the film provided a passable entertainment experience—nothing exceptional,\" \n",
    "    \"but not disappointing either.\"\n",
    ")\n",
    "\n",
    "# Extract top 5 keywords with n-grams up to length 3\n",
    "keywords = kw_model.extract_keywords(doc_mixed, top_n=5, keyphrase_ngram_range=(1, 3), print_doc_polarity=True)\n",
    "\n",
    "print(\"Keywords extracted from mixed sentiment document:\")\n",
    "for kw, score in keywords:\n",
    "    print(f\"{kw:20s} \\t score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3a48",
   "metadata": {},
   "source": [
    "## Test 6:  Comparing Keyword Extraction Between Base KeyBERT and Sentiment-Aware KeyBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d719a1",
   "metadata": {},
   "source": [
    "This test evaluates the difference in keyword extraction between the standard KeyBERT model (which uses only semantic similarity) and our extended Sentiment-Aware KeyBERT model, which integrates sentiment alignment into the keyword selection process.\n",
    "\n",
    "- **Sentiment Polarity Computation:**  \n",
    "  For each review, it calculates a continuous sentiment polarity score between 0 (very negative) and 1 (very positive) using the sentiment model embedded in the Sentiment-Aware KeyBERT. It prints the polarity for each review and the average polarity across all reviews, categorized as Negative, Neutral, or Positive.\n",
    "\n",
    "- **Keyword Extraction and Scoring:**  \n",
    "  For each review and each model (base and sentiment-aware), it extracts the top keywords along with their semantic scores. For the sentiment-aware model, it also calculates the average sentiment polarity for each keyword.\n",
    "\n",
    "- **Ranking Keywords:**  \n",
    "  Keywords are ranked based on a combination of their average semantic score and how frequently they appear across the reviews. This ensures the selection favors keywords that are both important and consistently relevant.\n",
    "\n",
    "- **Diversity Filtering:**  \n",
    "  To avoid redundant keywords that are semantically too similar, the test applies a diversity filter based on cosine similarity of keyword embeddings. Only keywords that are sufficiently distinct from previously selected keywords are retained.\n",
    "\n",
    "- **Result Presentation:**  \n",
    "  The final output lists the top 5 diverse keywords from each model, showing their average scores. For the sentiment-aware model, the average sentiment polarity of each keyword is also displayed to highlight emotional alignment.\n",
    "\n",
    "**Purpose**:\n",
    "\n",
    "This test demonstrates how integrating sentiment information affects the keyword extraction results by:\n",
    "\n",
    "- Highlighting differences in topic selection between purely semantic and sentiment-aware approaches.\n",
    "- Showing how sentiment-aware keywords tend to align better with the emotional tone of the reviews.\n",
    "- Providing insight into the practical impact of sentiment-aware extensions for keyword extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6981df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity per review (0 = negative, 1 = positive):\n",
      " Review #1: Polarity = 0.957 (Positive)\n",
      " Review #2: Polarity = 0.983 (Positive)\n",
      " Review #3: Polarity = 0.877 (Positive)\n",
      " Review #4: Polarity = 0.980 (Positive)\n",
      " Review #5: Polarity = 0.993 (Positive)\n",
      " Review #6: Polarity = 0.025 (Negative)\n",
      "\n",
      "Average sentiment polarity across all reviews: 0.803 (Positive)\n",
      "\n",
      "\n",
      "Top 5 Diverse Keywords - Base KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "emotional depth screenplay                   0.6815\n",
      "uplifting cinematic journey                  0.6554\n",
      "absolute masterpiece                         0.6435\n",
      "narrative excellent acting                   0.6363\n",
      "beautifully crafted film                     0.6284\n",
      "\n",
      "Top 5 Diverse Keywords - Sentiment-Aware KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "absolute masterpiece                         0.8726          0.978\n",
      "breathtaking soundtrack perfectly            0.8676          0.945\n",
      "best films                                   0.8624          0.969\n",
      "truly inspiring movie                        0.8393          0.966\n",
      "excellent acting                             0.8270          0.943\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize base KeyBERT (semantic-only) and sentiment-aware KeyBERT\n",
    "kw_base = KB(model=embedding_model)\n",
    "kw_sentiment = KeyBERTSentimentAware(model=embedding_model, weight_sentiment=0.7)\n",
    "\n",
    "# Define reviews with mixed sentiments to highlight topic differences\n",
    "reviews = [\n",
    "    \"\"\"This film is a stunning achievement in storytelling, with unforgettable characters and a gripping plot.\n",
    "    The visuals are breathtaking, and the soundtrack perfectly complements every scene.\"\"\",\n",
    "\n",
    "    \"\"\"I was completely captivated from start to finish. The performances were heartfelt,\n",
    "    and the director’s vision shines through in every frame. A truly inspiring movie experience.\"\"\",\n",
    "\n",
    "    \"\"\"A beautifully crafted film with rich emotional depth. The screenplay is tight,\n",
    "    and the cinematography creates an immersive atmosphere that kept me hooked.\"\"\",\n",
    "\n",
    "    \"\"\"One of the best films I've seen in years. It combines a compelling narrative with excellent acting,\n",
    "    making it both entertaining and thought-provoking.\"\"\",\n",
    "\n",
    "    \"\"\"An absolute masterpiece! Every element, from the score to the visual effects,\n",
    "    contributes to a powerful and uplifting cinematic journey.\"\"\",\n",
    "\n",
    "    \"\"\"“An utter disaster of a film. The plot was incoherent, characters were completely flat, and the \n",
    "    pacing was excruciatingly slow. Dialogue felt forced and unnatural throughout. \n",
    "    I struggled to stay awake — a total waste of time.”\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Function to convert continuous polarity (0 to 1) into categorical sentiment label\n",
    "def polarity_label(p):\n",
    "    if p < 0.4:\n",
    "        return \"Negative\"\n",
    "    elif p > 0.6:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Compute and print the sentiment polarity for each review individually\n",
    "print(\"Sentiment polarity per review (0 = negative, 1 = positive):\")\n",
    "polarities = []\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    pol = kw_sentiment._get_doc_polarity_continuous(review)\n",
    "    polarities.append(pol)\n",
    "    print(f\" Review #{i}: Polarity = {pol:.3f} ({polarity_label(pol)})\")\n",
    "\n",
    "# Compute and print average polarity over all reviews with categorical label\n",
    "mean_polarity = np.mean(polarities)\n",
    "print(f\"\\nAverage sentiment polarity across all reviews: {mean_polarity:.3f} ({polarity_label(mean_polarity)})\\n\")\n",
    "\n",
    "# Function to accumulate scores and polarities per keyword for a given model\n",
    "def accumulate_keywords(model, reviews):\n",
    "    scores = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    polarities = defaultdict(list)\n",
    "\n",
    "    for review in reviews:\n",
    "        # Extract keywords with their scores\n",
    "        kws = model.extract_keywords(review, top_n=7, keyphrase_ngram_range=(1,3))\n",
    "        for kw, score in kws:\n",
    "            scores[kw] += score\n",
    "            counts[kw] += 1\n",
    "\n",
    "            # For sentiment-aware model: calculate polarity per keyword\n",
    "            if hasattr(model, '_get_candidate_polarities'):\n",
    "                cand_pol = kw_sentiment._get_candidate_polarities([kw])[0]\n",
    "                polarities[kw].append(cand_pol)\n",
    "    return scores, counts, polarities\n",
    "\n",
    "# Accumulate data for both models\n",
    "base_scores, base_counts, base_pols = accumulate_keywords(kw_base, reviews)\n",
    "sent_scores, sent_counts, sent_pols = accumulate_keywords(kw_sentiment, reviews)\n",
    "\n",
    "# Compute average score and average polarity per keyword\n",
    "def compute_averages(scores, counts, polarities):\n",
    "    avg_scores = {kw: scores[kw] / counts[kw] for kw in scores}\n",
    "    avg_pols = {kw: np.mean(polarities[kw]) for kw in polarities} if polarities else {}\n",
    "    return avg_scores, avg_pols\n",
    "\n",
    "base_avg_scores, base_avg_pols = compute_averages(base_scores, base_counts, base_pols)\n",
    "sent_avg_scores, sent_avg_pols = compute_averages(sent_scores, sent_counts, sent_pols)\n",
    "\n",
    "# Ranking function combining average score and normalized frequency to balance importance and consistency\n",
    "def rank_keywords(avg_scores, counts, alpha=0.7):\n",
    "    max_count = max(counts.values()) if counts else 1\n",
    "    ranked = []\n",
    "    for kw in avg_scores:\n",
    "        freq_norm = counts[kw] / max_count  # Normalize frequency to [0,1]\n",
    "        combined_score = alpha * avg_scores[kw] + (1 - alpha) * freq_norm\n",
    "        ranked.append((kw, combined_score))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)  # Sort descending by combined score\n",
    "    return ranked\n",
    "\n",
    "# Rank keywords for both models\n",
    "base_ranked = rank_keywords(base_avg_scores, base_counts)\n",
    "sent_ranked = rank_keywords(sent_avg_scores, sent_counts)\n",
    "\n",
    "# Function to embed keyword phrases and normalize embeddings for cosine similarity\n",
    "def embed_keywords(keywords):\n",
    "    return embedding_model.encode(keywords, convert_to_tensor=True, normalize_embeddings=True).cpu().numpy()\n",
    "\n",
    "# Select top N keywords ensuring semantic diversity by filtering out candidates too similar to already selected ones\n",
    "def select_diverse_keywords(ranked_list, avg_scores, avg_pols, top_n=5, similarity_threshold=0.7):\n",
    "    selected = []\n",
    "    selected_embs = []\n",
    "\n",
    "    all_keywords = [kw for kw, _ in ranked_list]\n",
    "    all_embs = embed_keywords(all_keywords)\n",
    "\n",
    "    for i, (kw, _) in enumerate(ranked_list):\n",
    "        if not selected:\n",
    "            selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "            selected_embs.append(all_embs[i])\n",
    "        else:\n",
    "            emb = all_embs[i]\n",
    "            # Compute max cosine similarity with selected keywords\n",
    "            sims = [np.dot(emb, se) for se in selected_embs]\n",
    "            if max(sims) < similarity_threshold:\n",
    "                selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "                selected_embs.append(emb)\n",
    "        if len(selected) >= top_n:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "# Select top 5 diverse keywords for each model\n",
    "top_base = select_diverse_keywords(base_ranked, base_avg_scores, base_avg_pols, top_n=5)\n",
    "top_sent = select_diverse_keywords(sent_ranked, sent_avg_scores, sent_avg_pols, top_n=5)\n",
    "\n",
    "# Nicely print the final selected keywords with average scores and polarities (only for sentiment-aware)\n",
    "def print_topics(title, topics, show_polarity=True, flop=False):\n",
    "    if flop:\n",
    "        print(f\"\\n{title} (Flop {len(topics)}):\")\n",
    "        # Reverse order for flop to show worst first\n",
    "        topics_to_print = reversed(topics)\n",
    "    else:\n",
    "        print(f\"\\n{title} (Top {len(topics)}):\")\n",
    "        topics_to_print = topics\n",
    "\n",
    "    header = f\"{'Keyword':40s} {'Avg Score':>10s}\"\n",
    "    if show_polarity:\n",
    "        header += f\" {'Avg Polarity':>14s}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for kw, score, pol in topics_to_print:\n",
    "        line = f\"{kw:40s} {score:10.4f}\"\n",
    "        if show_polarity:\n",
    "            line += f\" {pol:14.3f}\"\n",
    "        print(line)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print_topics(\"Top 5 Diverse Keywords - Base KeyBERT\", top_base, show_polarity=False)\n",
    "print_topics(\"Top 5 Diverse Keywords - Sentiment-Aware KeyBERT\", top_sent, show_polarity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "399c8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flop 5 Keywords - Base KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "combines compelling narrative                0.4711\n",
      "heartfelt director                           0.5045\n",
      "director vision shines                       0.5103\n",
      "best films ve                                0.5111\n",
      "dialogue felt                                0.5181\n",
      "\n",
      "Flop 5 Keywords - Sentiment-Aware KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "powerful uplifting                           0.7013          0.900\n",
      "unnatural struggled stay                     0.7101          0.095\n",
      "narrative excellent acting                   0.7114          0.851\n",
      "inspiring movie                              0.7116          0.878\n",
      "unnatural struggled                          0.7161          0.085\n"
     ]
    }
   ],
   "source": [
    "# Select flop 5 keywords (lowest ranked) for each model\n",
    "flop_base = base_ranked[-5:]\n",
    "flop_sent = sent_ranked[-5:]\n",
    "\n",
    "# Disabling diversity filtering for flop: simply take the flop keywords as is\n",
    "flop_base_diverse = [(kw, base_avg_scores[kw], base_avg_pols.get(kw, float('nan'))) for kw, _ in flop_base]\n",
    "flop_sent_diverse = [(kw, sent_avg_scores[kw], sent_avg_pols.get(kw, float('nan'))) for kw, _ in flop_sent]\n",
    "\n",
    "# Print flop keywords nicely\n",
    "print_topics(\"Flop 5 Keywords - Base KeyBERT\", flop_base_diverse, show_polarity=False, flop=True)\n",
    "print_topics(\"Flop 5 Keywords - Sentiment-Aware KeyBERT\", flop_sent_diverse, show_polarity=True, flop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308cd0f",
   "metadata": {},
   "source": [
    "### Analysis of the results\n",
    "\n",
    "The results from comparing the base KeyBERT model and the sentiment-aware KeyBERT extension reveal **notable differences** in keyword extraction behavior. The **base KeyBERT** relies solely on semantic similarity, selecting keywords that are statistically relevant but **ignore the emotional tone** of the text. Thus, even lower-ranked (flop) keywords tend to reflect generic narrative or cinematic elements without sentiment alignment.\n",
    "\n",
    "In contrast, the **sentiment-aware KeyBERT** incorporates sentiment polarity during candidate selection and scoring, combining **semantic relevance** with **sentiment alignment**. It surfaces keywords that better reflect the **positive or negative tone** of individual reviews, as shown by the high average polarity of top keywords. Importantly, this model analyzes the **polarity of each single review** for sentiment alignment, ensuring keywords are contextually emotionally coherent at the review level.\n",
    "\n",
    "Interestingly, even among lower-ranked keywords, the sentiment-aware model maintains a clear distinction between **positive and negative terms**, highlighting its sensitivity to emotional nuance. This integration is especially valuable in tasks like opinion mining and customer feedback analysis, where understanding sentiment is crucial.\n",
    "\n",
    "Moreover, when aggregating keywords across multiple reviews to select overall top and flop topics, the model applies a global reasoning: since the set of reviews is **overall positive** (reflected by the average polarity), keywords with **negative sentiment are penalized** at this stage. This ensures that the final extracted topics are coherent with the **dominant emotional tone** of the audience’s feedback.\n",
    "\n",
    "Overall, this sentiment-aware extension enhances interpretability and better aligns with human perception by avoiding generic keywords and emphasizing those that **resonate emotionally** both at the review and aggregate level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b8da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7e8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
