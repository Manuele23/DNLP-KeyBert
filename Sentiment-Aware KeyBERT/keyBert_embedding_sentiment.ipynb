{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662213f8",
   "metadata": {},
   "source": [
    "# KeyBERT with Sentiment-aware Embedding Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1106b08",
   "metadata": {},
   "source": [
    "This notebook introduces a **sentiment-aware extension** of the KeyBERT keyword extraction model, which integrates sentiment information directly into the candidate selection and ranking process. Unlike simple post-hoc reranking approaches, this method incorporates sentiment consistency during both candidate filtering and final keyword scoring.\n",
    "\n",
    "### Theoretical Approach\n",
    "\n",
    "Traditional KeyBERT extracts candidate keywords purely based on semantic similarity between the document embedding and candidate embeddings. This extension enhances the process by considering the **emotional coherence** between candidates and the document, operationalized as continuous sentiment polarity scores.\n",
    "\n",
    "Given:\n",
    "-  $\\text{sim}_{sem}$ : cosine similarity between document and candidate embeddings,\n",
    "-  $s_{doc} \\in [0,1]$: continuous sentiment polarity score of the document,\n",
    "-  $s_{cand} \\in [0,1]$: continuous sentiment polarity score of a candidate keyword,\n",
    "\n",
    "we define the **sentiment alignment score** as:\n",
    "\n",
    "$$\n",
    "\\text{align}(s_{doc}, s_{cand}) = 1 - |s_{doc} - s_{cand}|\n",
    "$$\n",
    "\n",
    "which equals 1 for perfect polarity match and decreases linearly to 0 for maximal polarity difference.\n",
    "\n",
    "The overall combined score used to filter and rank candidates is:\n",
    "\n",
    "$$\n",
    "\\text{score}_{final} = w_{sentiment} \\times \\text{align}(s_{doc}, s_{cand}) + (1 - w_{sentiment}) \\times \\text{sim}_{sem}\n",
    "$$\n",
    "\n",
    "where $$ w_{sentiment} \\in [0,1] $$ is a tunable weight balancing sentiment alignment and semantic similarity.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Integrated sentiment filtering**: Sentiment is incorporated early to filter out candidates that are sentimentally incongruent with the document, not only at reranking stage.\n",
    "- **Continuous sentiment modeling**: Uses probability-weighted sentiment polarity scores from a pretrained transformer classifier, enabling nuanced sentiment comparisons.\n",
    "- **Flexible weighting parameter**: The parameter \\( w_{sentiment} \\) allows task-specific tuning of the relative importance of sentiment versus semantic relevance.\n",
    "- **Candidate generation enhancement**: The candidate pool is initially large and filtered by combined semantic and sentiment scores, improving quality and relevance.\n",
    "\n",
    "### Advantages Over Post-hoc Reranking\n",
    "\n",
    "- Unlike reranking approaches that adjust keyword order **after** candidate generation, this method filters candidates **before** ranking, reducing noise and irrelevant candidates early.\n",
    "- Sentiment influences the candidate pool itself, resulting in more coherent and contextually appropriate keyword extraction.\n",
    "- The approach remains compatible with any KeyBERT-compatible embedding model and sentiment classification backend.\n",
    "\n",
    "### Intended Applications\n",
    "\n",
    "This sentiment-aware KeyBERT extension is especially suited for sentiment-rich domains such as:\n",
    "\n",
    "- Product and service reviews\n",
    "- Social media opinion mining\n",
    "- Customer feedback analysis\n",
    "- Any text where emotional tone is critical to understanding key themes\n",
    "\n",
    "It enables the extraction of keywords that are both topically relevant and emotionally aligned, enhancing interpretability and downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2118e",
   "metadata": {},
   "source": [
    "### Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510ac332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "torch is already installed.\n",
      "Installing scikit-learn...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "keybert is already installed.\n",
      "transformers is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"numpy\", \"torch\", \"scikit-learn\", \"keybert\", \"transformers\"\n",
    "    \n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d59f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Fundamental package for numerical computing in Python\n",
    "from typing import Tuple  # Used for type hinting tuples in function signatures\n",
    "\n",
    "import torch  # Core PyTorch library for tensor computations\n",
    "import torch.nn.functional as F  # Functional interface for activation functions, etc.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Extract text n-gram candidates\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Compute cosine similarity between embeddings\n",
    "\n",
    "from keybert import KeyBERT as KB  # KeyBERT keyword extraction base class\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,  # Tokenizer for preparing input text for transformer models\n",
    "    AutoModelForSequenceClassification  # Transformer model for classification tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c140c4",
   "metadata": {},
   "source": [
    "# Classes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f387a9e",
   "metadata": {},
   "source": [
    "## SentimentModel Class: Transformer-based Sentiment Probability Predictor\n",
    "\n",
    "The `SentimentModel` class is a wrapper around a pretrained HuggingFace transformer model designed for sentiment classification. It provides a convenient interface to obtain **probability distributions over sentiment classes** for batches of input texts.\n",
    "\n",
    "### Purpose and Functionality\n",
    "\n",
    "- **Model loading:**  \n",
    "  Upon initialization, the class loads both the tokenizer and the sequence classification model specified by the `model_name`.  \n",
    "  By default, it uses `\"nlptown/bert-base-multilingual-uncased-sentiment\"`, a multilingual BERT model fine-tuned for 5-class sentiment classification (1 to 5 stars).\n",
    "\n",
    "- **Device management:**  \n",
    "  The model and tokenizer are moved to the specified device (`cpu` or `cuda`).  \n",
    "  Input validation ensures that `cuda` is only used if a compatible GPU is available.\n",
    "\n",
    "- **Batch sentiment prediction:**  \n",
    "  The core method `predict_proba` takes a list of texts and:  \n",
    "  1. Tokenizes and encodes them into the format expected by the transformer.  \n",
    "  2. Performs a forward pass through the model without computing gradients (efficient inference).  \n",
    "  3. Applies a softmax to the output logits to obtain a probability distribution over the sentiment classes for each text.  \n",
    "  4. Returns a NumPy array of shape `(batch_size, num_classes)` containing the class probabilities.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Allows seamless integration of sentiment analysis into larger NLP pipelines.\n",
    "- Outputs probabilistic sentiment scores, enabling nuanced, continuous sentiment representations rather than hard labels.\n",
    "- Supports batch processing for efficiency.\n",
    "\n",
    "### Example Output\n",
    "\n",
    "**Text:**  \n",
    "_I absolutely loved this movie! It was fantastic._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.01 0.02 0.05 0.12 0.80]\n",
    "\n",
    "\n",
    "**Text:**  \n",
    "_The plot was boring and predictable._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.70 0.20 0.07 0.02 0.01]\n",
    "\n",
    "\n",
    "**Text:**  \n",
    "_The movie was okay, nothing special but not bad either._  \n",
    "Sentiment probabilities (1 to 5 stars): [0.05 0.10 0.65 0.15 0.05]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    \"\"\"\n",
    "    Wrapper class for a HuggingFace transformer sentiment classification model.\n",
    "\n",
    "    This class loads a pretrained sentiment classification model and tokenizer,\n",
    "    and provides a method to compute the probability distribution over sentiment classes\n",
    "    for a batch of input texts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str, optional\n",
    "        The identifier of the pretrained sentiment model on HuggingFace Hub.\n",
    "        Default is \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        a 5-class sentiment classifier (1 to 5 stars).\n",
    "\n",
    "    device : str, optional\n",
    "        The device to run the model on. Typical values: \"cpu\" or \"cuda\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\", \n",
    "            device=\"cpu\"):\n",
    "        \n",
    "        #Â Set the device for model computation\n",
    "        if device not in [\"cpu\", \"cuda\"]:\n",
    "            raise ValueError(\"Device must be 'cpu' or 'cuda'.\")\n",
    "        \n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Load the tokenizer associated with the pretrained model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Load the pretrained sequence classification model on the given device\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"\n",
    "        Compute the probability distribution over sentiment classes for input texts.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        texts : list of str\n",
    "            List of input texts for which to compute sentiment probabilities.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Array of shape (len(texts), num_classes) where each row corresponds\n",
    "            to the probability distribution over sentiment classes for that text.\n",
    "        \"\"\"\n",
    "        # Tokenize and encode the input texts, handling padding and truncation for batching\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Perform forward pass without gradient computation for efficiency\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits  # raw model outputs before softmax\n",
    "            \n",
    "            # Convert logits to probabilities using softmax along class dimension\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9c9ed",
   "metadata": {},
   "source": [
    "## KeyBERTSentimentAware Class: Sentiment-Integrated Keyword Extraction\n",
    "\n",
    "This class extends the base KeyBERT model by integrating sentiment analysis directly into the keyword extraction pipeline. It enhances the traditional semantic-only approach by incorporating continuous sentiment polarity scores for both the entire document and each candidate keyword.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Candidate Extraction:**  \n",
    "  Uses `CountVectorizer` to extract a broad pool of candidate keywords (n-grams) from the document text.  \n",
    "  **Note:** This initial candidate generation is purely statistical and **does not incorporate sentiment information**.\n",
    "\n",
    "- **Sentiment Analysis:**  \n",
    "  Leverages a pretrained transformer sentiment classification model to compute **continuous sentiment polarity scores** ranging from 0 (very negative) to 1 (very positive) for both the document and each candidate.\n",
    "\n",
    "- **Combined Scoring and Filtering:**  \n",
    "  Calculates a weighted score combining:\n",
    "  - Semantic similarity (cosine similarity between embeddings).\n",
    "  - Sentiment alignment (1 minus the absolute difference between candidate and document sentiment).\n",
    "\n",
    "  Candidates with combined scores below a threshold are **filtered out before final ranking**, effectively integrating sentiment as a filter immediately after candidate extraction.\n",
    "\n",
    "  The weighting is controlled by `weight_sentiment`:\n",
    "  - `weight_sentiment=1.0` means keywords are ranked purely by sentiment alignment.\n",
    "  - `weight_sentiment=0.0` means keywords are ranked purely by semantic similarity.\n",
    "\n",
    "### Candidate Selection in KeyBERT vs Sentiment-Aware Extension\n",
    "\n",
    "In the original KeyBERT model, the candidate keywords are extracted purely based on **statistical properties** of the text. Specifically, KeyBERT uses a tool like `CountVectorizer` to identify n-grams (contiguous sequences of words) that appear frequently or are relevant according to basic frequency statistics. This means:\n",
    "\n",
    "- The **candidate pool is generated without any semantic or sentiment understanding**.\n",
    "- All candidates are treated equally in this phase, regardless of their emotional tone or contextual relevance beyond raw occurrence patterns.\n",
    "\n",
    "This purely **statistical candidate extraction** can lead to a large number of candidates that are relevant but may not align emotionally with the overall document sentiment. For example, in a strongly negative review, KeyBERT might still generate positive-sounding candidates simply because those phrases appear often, potentially misrepresenting the sentiment conveyed.\n",
    "\n",
    "To address this limitation, our sentiment-aware extension introduces a **joint filtering mechanism** that combines both semantic relevance and sentiment alignment **immediately after the initial statistical candidate extraction**:\n",
    "\n",
    "1. We first extract a large pool of candidates statistically using `CountVectorizer` to ensure broad coverage.\n",
    "\n",
    "2. We compute **continuous sentiment polarity scores** for both the entire document and each candidate keyword using a pretrained transformer sentiment model.\n",
    "\n",
    "3. We calculate a **combined score** for each candidate that balances:\n",
    "   - Semantic similarity to the document (embedding cosine similarity).\n",
    "   - Sentiment alignment with the document's overall polarity (inverted absolute difference between sentiment scores).\n",
    "\n",
    "4. Candidates whose combined score falls below a threshold are **filtered out early**, significantly reducing the pool to those that are both topically and emotionally relevant.\n",
    "\n",
    "This approach allows the model to **avoid candidates that are semantically plausible but sentimentally inconsistent**, leading to more meaningful and context-aware keyword extraction.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Step                        | KeyBERT Base                        | Sentiment-Aware Extension          |\n",
    "|-----------------------------|-----------------------------------|-----------------------------------|\n",
    "| Candidate generation         | Purely statistical (n-gram counts)| Statistical, followed by sentiment-semantic filtering (no sentiment during extraction but sentiment used immediately after to filter) |\n",
    "| Candidate ranking            | Semantic similarity only           | Semantic + sentiment combined     |\n",
    "| Sentiment consideration     | None                              | Integral part of candidate filtering|\n",
    "\n",
    "By incorporating sentiment as an early filtering step (post-statistical extraction), our extension improves the **precision and emotional coherence** of extracted keywords, especially in domains where sentiment plays a crucial role.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `model`: The base semantic embedding model (usually a SentenceTransformer).\n",
    "- `sentiment_model_name`: Identifier of the pretrained sentiment model (default is a 5-class multilingual sentiment classifier).\n",
    "- `weight_sentiment`: Balances importance between sentiment alignment and semantic similarity.\n",
    "- `candidate_pool_size`: Number of candidates initially extracted.\n",
    "- `device`: Compute device, `\"cpu\"` or `\"cuda\"`.\n",
    "\n",
    "### Usage\n",
    "\n",
    "The class allows flexible, context-aware keyword extraction that respects both topical relevance and emotional tone, ideal for analyzing opinion-rich texts such as reviews or social media posts.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyBERTSentimentAware(KB):\n",
    "    \"\"\"\n",
    "    Extension of KeyBERT to integrate sentiment analysis in keyword extraction.\n",
    "\n",
    "    This class overrides and extends parts of KeyBERT's pipeline to:\n",
    "    - Extract a larger candidate pool using CountVectorizer.\n",
    "    - Calculate sentiment polarity scores for the document and candidates,\n",
    "      using a pretrained sentiment classification model with continuous outputs.\n",
    "    - Combine semantic similarity and sentiment alignment scores via a weighting factor weight_sentiment.\n",
    "    - Filter candidate keywords based on this combined score before final ranking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SentenceTransformer\n",
    "        Semantic embedding model used by KeyBERT.\n",
    "\n",
    "    sentiment_model_name : str, optional (default: \"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        Identifier of pretrained sentiment model on HuggingFace Hub.\n",
    "\n",
    "    weight_sentiment : float, optional (default: 0.7)\n",
    "        Weight to balance sentiment alignment vs semantic similarity.\n",
    "        weight_sentiment=1.0 means only sentiment alignment is considered.\n",
    "        weight_sentiment=0.0 means only semantic similarity is considered.\n",
    "\n",
    "    candidate_pool_size : int, optional (default: 100)\n",
    "        Maximum number of initial candidate keywords to extract.\n",
    "\n",
    "    device : str, optional (default: \"cpu\")\n",
    "        Device to run embedding and sentiment models on (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        sentiment_model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        weight_sentiment: float = 0.7,\n",
    "        candidate_pool_size: int = 100,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        # Call the superclass (KeyBERT) constructor to initialize base functionality\n",
    "        super().__init__(model)\n",
    "        \n",
    "        # Validate and set parameters\n",
    "        self.weight_sentiment = weight_sentiment\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        self.device = device\n",
    "\n",
    "        # Store the semantic embedding model (typically SentenceTransformer)\n",
    "        self.embedder = model\n",
    "\n",
    "        # Initialize the sentiment model wrapper to obtain probabilities for sentiment classes\n",
    "        self.sentiment_model = SentimentModel(sentiment_model_name, device=device)\n",
    "\n",
    "        # Sentiment classes ordered from most negative to most positive\n",
    "        self.labels_ordered = ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "\n",
    "        # Map sentiment labels to continuous numeric values in [0, 1]\n",
    "        self.label_to_score = {\n",
    "            label: i / (len(self.labels_ordered) - 1)\n",
    "            for i, label in enumerate(self.labels_ordered)\n",
    "        }\n",
    "\n",
    "    def _get_doc_polarity_continuous(self, doc: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the document's continuous sentiment polarity score as the weighted sum of\n",
    "        predicted class probabilities multiplied by their numeric mappings.\n",
    "\n",
    "        This method overrides and replaces any default sentiment handling in the base class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            The document text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Continuous sentiment polarity score between 0 (very negative) and 1 (very positive).\n",
    "        \"\"\"\n",
    "        # Get probability distribution over sentiment classes for the document\n",
    "        probs = self.sentiment_model.predict_proba([doc])[0]\n",
    "\n",
    "        # Compute continuous polarity as weighted average of class scores\n",
    "        polarity = sum(\n",
    "            p * self.label_to_score[label]\n",
    "            for p, label in zip(probs, self.labels_ordered)\n",
    "        )\n",
    "        return polarity\n",
    "\n",
    "    def _get_candidate_polarities(self, candidates) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute continuous sentiment polarity scores for each candidate keyword.\n",
    "\n",
    "        This method extends candidate scoring with sentiment, overriding base candidate processing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        candidates : iterable of str\n",
    "            List of candidate keywords.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of polarity scores for each candidate keyword.\n",
    "        \"\"\"\n",
    "        candidates = list(candidates)  # ensure correct input format for tokenizer\n",
    "        \n",
    "        # Batch predict probabilities for all candidates\n",
    "        probs_list = self.sentiment_model.predict_proba(candidates)\n",
    "        \n",
    "        polarities = []\n",
    "        for probs in probs_list:\n",
    "            # Weighted average as continuous polarity score\n",
    "            polarity = sum(\n",
    "                p * self.label_to_score[label]\n",
    "                for p, label in zip(probs, self.labels_ordered)\n",
    "            )\n",
    "            polarities.append(polarity)\n",
    "        return np.array(polarities)\n",
    "\n",
    "    def _select_candidates(\n",
    "            self, \n",
    "            doc: str, \n",
    "            ngram_range: Tuple[int, int] = (1, 3), \n",
    "            threshold: float = 0.4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract initial candidates with CountVectorizer and filter them based on combined\n",
    "        semantic similarity and sentiment alignment scores.\n",
    "\n",
    "        This method replaces the default candidate generation and filtering steps of KeyBERT,\n",
    "        incorporating sentiment filtering before final keyword ranking.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Document text.\n",
    "\n",
    "        ngram_range : tuple of int\n",
    "            N-gram size range for candidate extraction.\n",
    "\n",
    "        threshold : float\n",
    "            Minimum combined score for candidate retention.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of str\n",
    "            Filtered list of candidate keywords.\n",
    "        \"\"\"\n",
    "        # Extract candidates with CountVectorizer (statistical n-grams)\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english',\n",
    "            max_features=self.candidate_pool_size\n",
    "        )\n",
    "        candidates = vectorizer.fit([doc]).get_feature_names_out()\n",
    "\n",
    "        # Compute semantic embeddings for doc and candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity scores\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "\n",
    "        # Combine semantic and sentiment scores with alpha weighting\n",
    "        combined_scores = self.weight_sentiment * sentiment_scores + (1 - self.weight_sentiment) * sim_scores\n",
    "\n",
    "        # Filter candidates that meet threshold on combined score\n",
    "        filtered_candidates = [c for c, s in zip(candidates, combined_scores) if s >= threshold]\n",
    "\n",
    "        return filtered_candidates\n",
    "\n",
    "    def extract_keywords(\n",
    "        self,\n",
    "        doc: str,\n",
    "        top_n: int = 5,\n",
    "        candidate_threshold: float = 0.4,\n",
    "        keyphrase_ngram_range: Tuple[int, int] = (1, 3),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract top keywords from a document by combining semantic similarity and sentiment alignment.\n",
    "\n",
    "        This method overrides the `extract_keywords` method from KeyBERT base class,\n",
    "        adding sentiment-aware candidate filtering and scoring.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Input document text.\n",
    "\n",
    "        top_n : int\n",
    "            Number of keywords to return.\n",
    "\n",
    "        candidate_threshold : float\n",
    "            Threshold score to filter candidate keywords.\n",
    "\n",
    "        keyphrase_ngram_range : tuple of int\n",
    "            N-gram range for candidate keyword extraction.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (keyword, score) tuples sorted by descending combined score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Select candidates filtered by combined semantic+sentiment scoring\n",
    "        candidates = self._select_candidates(\n",
    "            doc,\n",
    "            ngram_range=keyphrase_ngram_range,\n",
    "            threshold=candidate_threshold\n",
    "        )\n",
    "        if not candidates:\n",
    "            print(\"No candidates passed the sentiment-semantic filter.\")\n",
    "            return []\n",
    "\n",
    "        # Compute semantic embeddings for document and filtered candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity for the document\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "\n",
    "        print(f\"Document polarity: {doc_pol:.3f}\")\n",
    "\n",
    "        # Compute sentiment polarities for candidates\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate semantic similarity and sentiment alignment scores\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "\n",
    "        # Final combined score with weighting factor alpha\n",
    "        final_scores = self.weight_sentiment * sentiment_scores + (1 - self.weight_sentiment) * sim_scores\n",
    "\n",
    "        # Select top_n keywords sorted by combined score descending\n",
    "        top_indices = np.argsort(final_scores)[-top_n:][::-1]\n",
    "\n",
    "        return [(candidates[i], final_scores[i]) for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d991b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1533b",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
