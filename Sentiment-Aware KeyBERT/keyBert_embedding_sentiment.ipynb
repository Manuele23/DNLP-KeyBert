{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662213f8",
   "metadata": {},
   "source": [
    "# KeyBERT with Sentiment-aware Embedding Fusion\n",
    "\n",
    "This notebook implements a **sentiment-aware embedding-level extension** of KeyBERT, where sentiment information is directly injected into the document and candidate embeddings **before** similarity is computed.\n",
    "\n",
    "*The goal is to guide keyword selection by embedding not just semantic content, but also the **emotional tone** of the text, so that emotionally coherent topics naturally emerge.*\n",
    "\n",
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "Unlike post-hoc reranking strategies, this method modifies the **core embedding computation** used by KeyBERT. Specifically, we create a combined representation:\n",
    "\n",
    "- `E_sem`: the original semantic embedding of the text (e.g., using `MiniLM`)\n",
    "- `s`: the sentiment vector (either raw probabilities or a non-linear transformation)\n",
    "- `E_final`: the sentiment-aware embedding, combining `E_sem` and `s` through either linear or non-linear fusion.\n",
    "\n",
    "We support two orthogonal options:\n",
    "\n",
    "1. **Sentiment Vector Type**\n",
    "   - `\"linear\"`: raw sentiment probabilities `[p_neg, p_neu, p_pos]`\n",
    "   - `\"nonlinear\"`: transformed via a learned MLP projection\n",
    "\n",
    "2. **Combination Formula**\n",
    "   - `\"concat\"`: `E_final = [E_sem ; β × s]`\n",
    "   - `\"add\"`: `E_final = E_sem + β × s_proj`\n",
    "   - `\"nonlinear\"`: `E_final = E_sem + s_proj + E_sem * s_proj`\n",
    "\n",
    "---\n",
    "\n",
    "## Characteristics\n",
    "\n",
    "- **Modular**: easily switch between sentiment vector types and combination formulas.\n",
    "- **Prior-aware**: modifies candidate scores before similarity is computed, influencing topic formation directly.\n",
    "- **Extensible**: can plug into any KeyBERT-based pipeline or BERTopic-style clustering.\n",
    "- **Evaluation-ready**: supports metric-based validation for ablation studies.\n",
    "\n",
    "---\n",
    "\n",
    "This implementation is meant to **systematically evaluate the impact of injecting sentiment into the embedding space**, to discover whether richer emotional context improves keyword extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2118e",
   "metadata": {},
   "source": [
    "### Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510ac332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keybert is already installed.\n",
      "Installing sentence-transformers...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "transformers is already installed.\n",
      "torch is already installed.\n",
      "numpy is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"keybert\", \"sentence-transformers\", \"transformers\", \"torch\", \"numpy\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c140c4",
   "metadata": {},
   "source": [
    "# Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8677e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch neural network module — used to define the MLP that projects sentiment vectors\n",
    "import torch.nn as nn\n",
    "\n",
    "# SentenceTransformer is used to generate dense semantic embeddings for full documents or keywords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# HuggingFace Transformers: \n",
    "# - AutoTokenizer tokenizes input text for the sentiment model\n",
    "# - AutoModelForSequenceClassification runs the sentiment classification model (e.g., RoBERTa)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff2040",
   "metadata": {},
   "source": [
    "## SentimentEmbedder: Fusing Semantics and Sentiment for Keyword Extraction\n",
    "\n",
    "This cell defines the `SentimentEmbedder` class, a **modular and extensible embedding model** that enhances the standard KeyBERT pipeline by incorporating sentiment information directly into the embedding space. Instead of applying sentiment corrections after keyword extraction (post-hoc), this approach injects **emotional context directly into the vector representation**, enabling **prior-aware keyword ranking**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Sentiment Computation Scope\n",
    "\n",
    "The sentiment is computed for **both the document (e.g., a review)** and **each candidate keyword**. This is because `SentimentEmbedder.encode()` is called for both sides of the similarity computation in KeyBERT:\n",
    "\n",
    "- When generating the embedding for the full document.\n",
    "- When generating embeddings for all candidate keywords.\n",
    "\n",
    "This enables the model to favor keywords that are not only semantically related but also **emotionally aligned** with the document, enhancing both interpretability and contextual coherence.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "KeyBERT ranks keywords by measuring the cosine similarity between the semantic embedding of a document and the candidate phrases. However, this ignores the **emotional polarity** of the text, which may be critical in applications such as reviews, opinion mining, or narrative modeling. \n",
    "\n",
    "To address this, the `SentimentEmbedder` class extends the sentence embedding by fusing in a **sentiment-aware component** via one of several fusion strategies, ensuring that **semantic and affective dimensions are jointly encoded**.\n",
    "\n",
    "---\n",
    "\n",
    "### Structure and Behavior\n",
    "\n",
    "The class is compatible with any `KeyBERT` pipeline, and supports the following configurable components:\n",
    "\n",
    "- **`base_model`**: the semantic encoder (e.g., `all-MiniLM-L6-v2`) used to generate standard sentence embeddings.\n",
    "- **`sentiment_model`**: a HuggingFace classifier that outputs probabilities over sentiment classes: `[negative, neutral, positive]`.\n",
    "- **`sentiment_mode`**:\n",
    "  - `\"linear\"`: directly uses raw sentiment probabilities scaled by a factor `β`.\n",
    "  - `\"nonlinear\"`: applies an MLP (Multi-Layer Perceptron) projection to obtain a dense, continuous representation aligned with the semantic space.\n",
    "- **`combination_mode`**:\n",
    "  - `\"concat\"`: appends the sentiment vector to the semantic embedding.\n",
    "  - `\"add\"`: performs element-wise addition between the two vectors (requires matching dimensionality).\n",
    "  - `\"nonlinear\"`: combines `add` with an element-wise product for richer interaction:  \n",
    "    $$\n",
    "    E_\\text{final} = E_\\text{sem} + E_\\text{sent} + (E_\\text{sem} * E_\\text{sent})\n",
    "    $$\n",
    "- **`beta`**: scales the influence of the sentiment vector in `\"linear\"` mode.\n",
    "- **`device`**: specifies the computation device (`\"cpu\"` or `\"cuda\"`).\n",
    "\n",
    "### Output Format\n",
    "\n",
    "The `encode()` method returns a matrix of shape `(batch_size, dim)` or `(batch_size, dim + 3)` depending on the fusion strategy. These vectors can be directly used by KeyBERT or any cosine-similarity-based retrieval system.\n",
    "\n",
    "---\n",
    "\n",
    "### Continuous Sentiment Embedding (Nonlinear Mode)\n",
    "\n",
    "In `\"nonlinear\"` mode, the sentiment classifier output is passed through a small MLP ending in a `Sigmoid()` activation. This vector is:\n",
    "\n",
    "1. **Rescaled to the range [0, 10]**, representing sentiment intensity.\n",
    "2. **Normalized to the interval [-1, +1]**, to match the distributional range of semantic embeddings.\n",
    "\n",
    "This enables **compatibility with semantic vectors** and ensures that sentiment influences ranking without overpowering the semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### Applicability\n",
    "\n",
    "This architecture supports both **intrinsic evaluation** (embedding structure, keyword quality) and **extrinsic downstream tasks** (clustering, classification, sentiment-coherent retrieval), and is general enough to be adapted to other affective signals (e.g., emotion, subjectivity, sarcasm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    A KeyBERT-compatible embedding model that fuses semantic and sentiment information\n",
    "    into a single embedding vector, using configurable strategies.\n",
    "\n",
    "    Parameters\n",
    "\n",
    "    base_model : str\n",
    "        Name of the SentenceTransformer model to use for semantic embeddings.\n",
    "        Default is \"all-MiniLM-L6-v2\", a lightweight and efficient model.\n",
    "\n",
    "    sentiment_model : str\n",
    "        HuggingFace model identifier used for sentiment analysis.\n",
    "        Must be a classification model with 3 outputs: [negative, neutral, positive].\n",
    "\n",
    "    use_sentiment : bool\n",
    "        If True, sentiment information will be integrated into the final embedding.\n",
    "        If False, the model will behave like a standard semantic-only embedder.\n",
    "\n",
    "    sentiment_mode : str\n",
    "        Specifies how the sentiment vector is handled:\n",
    "        - \"linear\": use raw sentiment probabilities [p_neg, p_neu, p_pos]\n",
    "        - \"nonlinear\": pass the sentiment vector through a small MLP projection\n",
    "\n",
    "    combination_mode : str\n",
    "        Defines how semantic and sentiment embeddings are fused:\n",
    "        - \"concat\": concatenate the vectors\n",
    "        - \"add\": add them element-wise (requires same dimensionality)\n",
    "        - \"nonlinear\": add + element-wise product for richer interaction\n",
    "\n",
    "    beta : float\n",
    "        Scaling factor for the sentiment vector, controlling its influence.\n",
    "        Typical values range between 0.1 and 0.5.\n",
    "\n",
    "    device : str\n",
    "        Device to run the models on: \"cpu\" or \"cuda\".\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_model=\"all-MiniLM-L6-v2\",\n",
    "                 sentiment_model=\"finiteautomata/bertweet-base-sentiment-analysis\", # HuggingFace model for sentiment analysis\n",
    "                 use_sentiment=True,\n",
    "                 sentiment_mode=\"linear\",         # \"linear\" or \"nonlinear\"\n",
    "                 combination_mode=\"concat\",       # \"concat\", \"add\", \"nonlinear\"\n",
    "                 beta=0.5,                        # strength of sentiment influence\n",
    "                 device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize input parameters\n",
    "        self.device = device                                # Device used for computation (\"cpu\" or \"cuda\")\n",
    "        self.use_sentiment = use_sentiment                  # Whether to include sentiment in the embedding\n",
    "        self.sentiment_mode = sentiment_mode                # How sentiment is processed: \"linear\" or \"nonlinear\"\n",
    "        self.combination_mode = combination_mode            # How semantic and sentiment vectors are fused\n",
    "        self.beta = beta                                    # Scaling factor controlling sentiment influence\n",
    "\n",
    "        # Load the semantic model (e.g., MiniLM or other SentenceTransformer)\n",
    "        self.base = SentenceTransformer(base_model, device=device)  # Pretrained semantic encoder\n",
    "        self.dim = self.base.get_sentence_embedding_dimension()     # Dimensionality of the semantic embeddings\n",
    "\n",
    "        # If sentiment is used, load the sentiment model and define its transformation\n",
    "        if use_sentiment:\n",
    "\n",
    "            # Load the tokenizer corresponding to the sentiment model\n",
    "            # It transforms raw text into token IDs expected by the transformer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(sentiment_model)\n",
    "            \n",
    "            # Load the pretrained sentiment classification model\n",
    "            # It outputs a 3-class probability distribution: [negative, neutral, positive]\n",
    "            self.sent_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model).to(device)\n",
    "\n",
    "            # Define a small neural network (MLP) to project the 3-dimensional sentiment vector\n",
    "            # into a continuous, dense vector aligned with the semantic embedding space.\n",
    "            # Output of the last layer is passed through a Sigmoid, resulting in values ∈ [0, 1],\n",
    "            # which will later be rescaled into [0, 10] and normalized to match the semantic scale.\n",
    "            self.sent_proj = nn.Sequential(\n",
    "                nn.Linear(3, 32),        # First layer expands the sentiment input to a hidden size\n",
    "                nn.ReLU(),               # Apply non-linearity for expressiveness\n",
    "                nn.Linear(32, self.dim), # Project to same dimension as semantic embedding (e.g., 384)\n",
    "                nn.Sigmoid()             # Ensure all outputs are bounded between 0 and 1\n",
    "            ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    @torch.no_grad()  # Disable gradient tracking since we are only doing inference\n",
    "    def _get_sentiment_vector(self, texts):\n",
    "        \"\"\"\n",
    "        Compute a sentiment-aware vector for a list of texts.\n",
    "        \n",
    "        Output depends on the selected `sentiment_mode`:\n",
    "        - \"linear\"    → returns raw probabilities [p_neg, p_neu, p_pos], scaled by beta\n",
    "        - \"nonlinear\" → projects sentiment into a continuous embedding aligned with the semantic space,\n",
    "                        scaled in [0, 10], then normalized to [-1, +1] to match semantic embedding range\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess the input texts by converting them into model-ready tokenized format.\n",
    "        # This includes:\n",
    "        # - tokenizing the text into subword IDs\n",
    "        # - adding special tokens (e.g., [CLS], [SEP])\n",
    "        # - padding all sequences in the batch to the same length\n",
    "        # - truncating sequences that are too long\n",
    "        # - returning the result as PyTorch tensors\n",
    "        # The output includes both:\n",
    "        # - input_ids: the token indices\n",
    "        # - attention_mask: a mask to distinguish real tokens from padding\n",
    "        inputs = self.tokenizer(\n",
    "            texts, \n",
    "            padding=True,           # Pad all sequences to the same length\n",
    "            truncation=True,        # Truncate longer sequences to fit the model's max length\n",
    "            return_tensors=\"pt\"     # Return as PyTorch tensors\n",
    "        ).to(self.device)           # Move the batch to the specified device (CPU or GPU)\n",
    "\n",
    "\n",
    "        # Perform inference with the sentiment classifier to get logits\n",
    "        logits = self.sent_model(**inputs).logits  # shape: (batch_size, 3)\n",
    "\n",
    "        # Convert logits to probabilities over [negative, neutral, positive]\n",
    "        probs = torch.softmax(logits, dim=1).to(self.device)  # shape: (batch_size, 3)\n",
    "\n",
    "        # LINEAR: return the scaled sentiment probabilities directly\n",
    "        if self.sentiment_mode == \"linear\":\n",
    "           \n",
    "            # Values will be in range [0, beta]\n",
    "            return self.beta * probs  # shape: (batch_size, 3)\n",
    "\n",
    "        # NON-LINEAR: use an MLP to project the 3D sentiment vector into semantic space\n",
    "        elif self.sentiment_mode == \"nonlinear\":\n",
    "            \n",
    "            # Step 1: project to [0, 1] via sigmoid (from MLP)\n",
    "            projected = self.sent_proj(probs)  # shape: (batch_size, dim), values in [0, 1]\n",
    "\n",
    "            # Step 2: rescale to [0, 10] to model sentiment intensity\n",
    "            rescaled = projected * 10\n",
    "\n",
    "            # Step 3: normalize to [-1, +1] so it aligns numerically with the semantic embedding\n",
    "            normalized = (rescaled - 5) / 5  # now values ∈ [-1, +1]\n",
    "\n",
    "            return normalized  # shape: (batch_size, dim)\n",
    "\n",
    "        else:\n",
    "            # Catch invalid input modes\n",
    "            raise ValueError(\"Invalid sentiment_mode: choose 'linear' or 'nonlinear'\")\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, texts, **kwargs):\n",
    "        \"\"\"\n",
    "        Main method required by KeyBERT: returns embeddings for a list of texts.\n",
    "        \n",
    "        The output depends on:\n",
    "        - `use_sentiment`: whether sentiment should be included\n",
    "        - `sentiment_mode`: how the sentiment vector is obtained (\"linear\" or \"nonlinear\")\n",
    "        - `combination_mode`: how to fuse semantic and sentiment embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Get semantic embedding from SentenceTransformer model\n",
    "        base_emb = self.base.encode(\n",
    "            texts, \n",
    "            convert_to_tensor=True, \n",
    "            **kwargs\n",
    "        ).to(self.device)  # shape: (batch_size, dim)\n",
    "\n",
    "        if not self.use_sentiment:\n",
    "            # If sentiment is disabled, return semantic embedding as-is (KeyBERT solution)\n",
    "            return base_emb.cpu().numpy()\n",
    "\n",
    "        # Step 2: Get sentiment vector (either raw or projected)\n",
    "        sent_vec = self._get_sentiment_vector(texts)  # shape depends on mode:\n",
    "                                                      # linear  → (batch_size, 3)\n",
    "                                                      # nonlinear → (batch_size, dim)\n",
    "\n",
    "        # Step 3: Combine semantic and sentiment vectors based on selected strategy\n",
    "        if self.sentiment_mode == \"linear\":\n",
    "            # Linear mode → sentiment vector has shape (batch_size, 3)\n",
    "\n",
    "            if self.combination_mode == \"concat\":\n",
    "                # Concatenate semantic embedding and sentiment probabilities\n",
    "                return torch.cat([base_emb, sent_vec], dim=1).cpu().numpy()  # shape: (batch_size, dim + 3)\n",
    "\n",
    "            elif self.combination_mode in [\"add\", \"nonlinear\"]:\n",
    "                # Cannot add or multiply vectors of mismatched dimensions\n",
    "                raise ValueError(\"Cannot use '{}' combination with 'linear' sentiment vector (dimension mismatch).\".format(self.combination_mode))\n",
    "\n",
    "        elif self.sentiment_mode == \"nonlinear\":\n",
    "            # Nonlinear mode → sentiment vector has same shape as base embedding\n",
    "\n",
    "            if self.combination_mode == \"add\":\n",
    "                # Simple element-wise addition of semantic + sentiment vectors\n",
    "                return (base_emb + sent_vec).cpu().numpy()  # shape: (batch_size, dim)\n",
    "\n",
    "            elif self.combination_mode == \"nonlinear\":\n",
    "                # Nonlinear fusion: sum + element-wise product\n",
    "                return (base_emb + sent_vec + base_emb * sent_vec).cpu().numpy()  # shape: (batch_size, dim)\n",
    "\n",
    "            elif self.combination_mode == \"concat\":\n",
    "                # Concatenate semantic embedding and projected sentiment vector\n",
    "                return torch.cat([base_emb, sent_vec], dim=1).cpu().numpy()  # shape: (batch_size, dim * 2)\n",
    "\n",
    "            else:\n",
    "                # Unsupported combination mode\n",
    "                raise ValueError(\"Invalid combination_mode: choose 'concat', 'add', or 'nonlinear'\")\n",
    "\n",
    "        else:\n",
    "            # Unsupported sentiment mode\n",
    "            raise ValueError(\"Invalid sentiment_mode: choose 'linear' or 'nonlinear'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d991b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc136c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd44791",
   "metadata": {},
   "source": [
    "### Model Preloading\n",
    "\n",
    "This cell preloads the semantic and sentiment models used by the `SentimentEmbedder` class.  \n",
    "Since HuggingFace models are downloaded the first time they are used, this step may take **several minutes**.\n",
    "Running this cell ensures that all models are cached locally, making subsequent runs significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f284e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading semantic model...\n",
      "Semantic model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load semantic model\n",
    "print(\"Loading semantic model...\")\n",
    "sem_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Semantic model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084a185",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318a8e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load sentiment model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/sentiment-roberta-large-english-3-classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1533b",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98035fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23fa667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n",
      " [('fantastic', 0.436), ('product', 0.3526), ('quality', 0.3296), ('exceeded', 0.306), ('loved', 0.2132)]\n",
      "Embedding shape: (1, 384)\n",
      "Embedding preview: [-0.13264063 -0.02692116 -0.07476154 -0.0123532   0.0876295  -0.03277994\n",
      " -0.05629675 -0.03434931  0.13755673 -0.13987309]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sentiment-aware model\n",
    "sent_model = SentimentEmbedder(\n",
    "    sentiment_mode=\"nonlinear\",        # \"nonlinear\" or \"linear\"\n",
    "    combination_mode=\"nonlinear\",      # \"nonlinear\", \"add\" or \"concat\"\n",
    "    beta=0.5,                          # only used in \"linear\" mode\n",
    "    device=\"cpu\"                       # or \"cuda\" if available\n",
    ")\n",
    "\n",
    "# Plug it into KeyBERT\n",
    "kw_model = KeyBERT(model=sent_model)\n",
    "\n",
    "doc = \"I absolutely loved this product. It exceeded my expectations and the quality is fantastic.\"\n",
    "\n",
    "keywords = kw_model.extract_keywords(doc, top_n=5)\n",
    "print(\"Extracted Keywords:\\n\", keywords)\n",
    "\n",
    "# View the shape and some sample values of the sentiment-aware embedding\n",
    "embedding = sent_model.encode([doc])\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Embedding preview:\", embedding[0][:10])  # first 10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ca155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
