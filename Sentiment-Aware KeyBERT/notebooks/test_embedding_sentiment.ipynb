{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff641bd8",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook implements the class defined in `models/embedding_sentiment.py`, extending KeyBERT to incorporate sentiment information directly into the keyword selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662213f8",
   "metadata": {},
   "source": [
    "# KeyBERT with Integrated Sentiment-aware Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1106b08",
   "metadata": {},
   "source": [
    "Unlike post-hoc reranking strategies that adjust keyword scores after semantic filtering, this approach introduces sentiment alignment **early**, during candidate selection and scoring.\n",
    "\n",
    "*The goal is to extract keywords that are not only semantically relevant, but also emotionally aligned with the overall sentiment of the document.*\n",
    "\n",
    "### Theoretical Framework\n",
    "\n",
    "The original KeyBERT pipeline is modified to consider both semantic similarity and sentiment coherence during candidate filtering and ranking.\n",
    "\n",
    "Given:\n",
    "- $\\text{sim}_{sem}$: cosine similarity between document and candidate embeddings\n",
    "- $s_{doc} \\in [0, 1]$: sentiment polarity of the full document\n",
    "- $s_{cand} \\in [0, 1]$: sentiment polarity of each candidate keyword\n",
    "\n",
    "We compute a sentiment alignment score:\n",
    "\n",
    "$$\n",
    "\\text{align}(s_{doc}, s_{cand}) = 1 - |s_{doc} - s_{cand}|\n",
    "$$\n",
    "\n",
    "This is then combined with semantic similarity to form a final score:\n",
    "\n",
    "$$\n",
    "\\text{score}_{final} = (1 - \\alpha) \\cdot \\text{sim}_{sem} + \\alpha \\cdot (2 \\cdot \\text{align} - 1)\n",
    "$$\n",
    "\n",
    "where **α ∈ [0, 1]** controls the trade-off between semantic and sentiment-based scoring.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- **Early sentiment filtering**: Sentiment alignment is used to filter low-quality candidates before ranking.\n",
    "- **Continuous scoring**: Sentiment is computed as a weighted average of class probabilities, enabling smooth comparisons.\n",
    "- **Flexible control**: The parameter `alpha` lets users adjust the influence of sentiment.\n",
    "- **Embedding-compatible**: Works with any `SentenceTransformer` model and HuggingFace sentiment classifier.\n",
    "\n",
    "By using sentiment alignment to filter and score keywords **before ranking**, this approach allows sentiment to directly influence which candidates are considered at all — not just how they are ranked.\n",
    "\n",
    "### When to Use This\n",
    "\n",
    "This strategy is most effective when:\n",
    "- The text contains **strong sentiment signals** (e.g., reviews, social media, opinions).\n",
    "- You want to **prioritize emotional consistency** from the beginning.\n",
    "- Reducing **irrelevant or sentimentally inconsistent keywords early** is important for your use case.\n",
    "\n",
    "It offers more control over the candidate pool and generally produces **more focused, sentiment-aware results**, especially in emotionally polarized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2118e",
   "metadata": {},
   "source": [
    "### Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "510ac332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "typing is already installed.\n",
      "torch is already installed.\n",
      "Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "keybert is already installed.\n",
      "transformers is already installed.\n",
      "sentence_transformers is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"numpy\", \"typing\", \"torch\", \"scikit-learn\", \"keybert\", \"transformers\", \"sentence_transformers\"\n",
    "    \n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d59f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy: fundamental package for numerical operations and array handling\n",
    "import numpy as np\n",
    "\n",
    "# Typing module: used for function signature annotations (e.g., Tuple[int, int])\n",
    "from typing import Tuple\n",
    "\n",
    "# PyTorch: core deep learning framework for tensor operations and model inference\n",
    "import torch\n",
    "\n",
    "# PyTorch functional API: provides functions like softmax, relu, etc.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn utility to extract candidate phrases based on n-gram statistics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# scikit-learn function to compute cosine similarity between vector embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# KeyBERT: base class for keyword extraction using BERT-based embeddings\n",
    "from keybert import KeyBERT as KB\n",
    "\n",
    "# HuggingFace Transformers:\n",
    "# - AutoTokenizer: automatically loads the correct tokenizer for a given model\n",
    "# - AutoModelForSequenceClassification: loads a pretrained classification model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# SentenceTransformers: framework for encoding text into dense embeddings using pretrained transformer models\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c140c4",
   "metadata": {},
   "source": [
    "# Classes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fd294",
   "metadata": {},
   "source": [
    "## SentimentModel: Flexible Transformer-based Sentiment Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f387a9e",
   "metadata": {},
   "source": [
    "The `SentimentModel` class provides a unified interface for performing sentiment analysis using pretrained HuggingFace transformer models. It is designed to return **probability distributions** over sentiment classes and compute **continuous sentiment scores** for downstream tasks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This class supports both 3-class models (e.g., `cardiffnlp/twitter-roberta-base-sentiment`) and 5-class models (e.g., `nlptown/bert-base-multilingual-uncased-sentiment`) by dynamically adapting to the label schema of the specified model.\n",
    "\n",
    "It is used across both reranking and candidate selection pipelines to ensure consistent, interpretable sentiment scoring.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "- **Model Loading:**  \n",
    "  Loads the tokenizer and sequence classification model specified via the `model_name`.  \n",
    "  All models must be compatible with HuggingFace’s `AutoModelForSequenceClassification`.\n",
    "\n",
    "- **Device Handling:**  \n",
    "  Automatically moves the model to the selected device (`cpu` or `cuda`) and validates compatibility.\n",
    "\n",
    "- **Class Probability Prediction:**  \n",
    "  The `predict_proba` method performs:\n",
    "  1. Tokenization and encoding of input text(s)\n",
    "  2. Inference using the transformer model (no gradient tracking)\n",
    "  3. Softmax activation over logits to obtain class probabilities\n",
    "  4. Return of a NumPy array of shape `(batch_size, num_classes)`\n",
    "\n",
    "- **Continuous Sentiment Scoring:**  \n",
    "  A utility method computes a **score in [0, 1]** as a probability-weighted average over mapped class values (e.g., 1-star → 0.0, 5-star → 1.0).\n",
    "\n",
    "### Why Use It\n",
    "\n",
    "- Compatible with any HuggingFace sentiment model\n",
    "- Supports **both batch processing** and fine-grained polarity computation\n",
    "- Enables **smooth integration** with sentiment-aware extensions of KeyBERT\n",
    "- Replaces hard labels with **interpretable, continuous sentiment scores**\n",
    "\n",
    "### Example Output\n",
    "\n",
    "**Text:**  \n",
    "_I absolutely loved this movie! It was fantastic._  \n",
    "→ Probabilities: `[0.01, 0.02, 0.05, 0.12, 0.80]`  \n",
    "→ Score: `0.91`\n",
    "\n",
    "**Text:**  \n",
    "_The plot was boring and predictable._  \n",
    "→ Probabilities: `[0.70, 0.20, 0.07, 0.02, 0.01]`  \n",
    "→ Score: `0.11`\n",
    "\n",
    "**Text:**  \n",
    "_The movie was okay, nothing special but not bad either._  \n",
    "→ Probabilities: `[0.05, 0.10, 0.65, 0.15, 0.05]`  \n",
    "→ Score: `0.51`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca48c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    \"\"\"\n",
    "    A flexible sentiment analysis wrapper supporting multiple HuggingFace models.\n",
    "\n",
    "    This class dynamically adapts to the label schema of the specified model,\n",
    "    allowing for consistent polarity scoring across different sentiment models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\", device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        model_name : str\n",
    "            HuggingFace model identifier.\n",
    "\n",
    "        device : str\n",
    "            Computation device. Should be either 'cpu' or 'cuda'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate the selected device\n",
    "        if device not in [\"cpu\", \"cuda\"]:\n",
    "            raise ValueError(\"Device must be 'cpu' or 'cuda'.\")\n",
    "\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load tokenizer and model from HuggingFace Hub\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "        # Determine label mapping based on the model\n",
    "        self._set_label_mapping()\n",
    "\n",
    "    def _set_label_mapping(self):\n",
    "        \"\"\"\n",
    "        Set the label to score mapping based on the model's label schema.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve the model's configuration to get label mappings\n",
    "        id2label = self.model.config.id2label\n",
    "\n",
    "        # Sort labels by their IDs to maintain order\n",
    "        self.labels_ordered = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "        # Define label to score mapping based on known models\n",
    "        if self.model_name == \"cardiffnlp/twitter-roberta-base-sentiment\":\n",
    "            # Labels: ['negative', 'neutral', 'positive']\n",
    "            self.label_to_score = {\n",
    "                'negative': 0.0,\n",
    "                'neutral': 0.5,\n",
    "                'positive': 1.0\n",
    "            }\n",
    "        elif self.model_name == \"nlptown/bert-base-multilingual-uncased-sentiment\":\n",
    "            # Labels: ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "            self.label_to_score = {\n",
    "                '1 star': 0.0,\n",
    "                '2 stars': 0.25,\n",
    "                '3 stars': 0.5,\n",
    "                '4 stars': 0.75,\n",
    "                '5 stars': 1.0\n",
    "            }\n",
    "        else:\n",
    "            # For unknown models, assign scores evenly across labels\n",
    "            num_labels = len(self.labels_ordered)\n",
    "            self.label_to_score = {\n",
    "                label: idx / (num_labels - 1) for idx, label in enumerate(self.labels_ordered)\n",
    "            }\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"\n",
    "        Compute the probability distribution over sentiment classes for one or more input texts.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        texts : List[str]\n",
    "            List of text strings to analyze.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 2D array of shape (len(texts), num_classes), where each row represents\n",
    "            the predicted softmax probabilities for the corresponding input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize and encode the input text(s) with padding and truncation\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Forward pass through the model without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits  # raw prediction scores before softmax\n",
    "\n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict_score(self, text):\n",
    "        \"\"\"\n",
    "        Compute the continuous sentiment score for a single input text.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float\n",
    "            The sentiment score in the range [0, 1].\n",
    "        \"\"\"\n",
    "\n",
    "        probs = self.predict_proba([text])[0]\n",
    "        score = sum(\n",
    "            prob * self.label_to_score[label]\n",
    "            for prob, label in zip(probs, self.labels_ordered)\n",
    "        )\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e61d4",
   "metadata": {},
   "source": [
    "## KeyBERTSentimentAware Class: Sentiment-Integrated Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9c9ed",
   "metadata": {},
   "source": [
    "This class extends the base KeyBERT model by integrating sentiment analysis directly into the keyword extraction pipeline. It enhances the traditional semantic-only approach by incorporating continuous sentiment polarity scores for both the entire document and each candidate keyword.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Candidate Extraction:**  \n",
    "  Uses `CountVectorizer` to extract a broad pool of candidate keywords (n-grams) from the document text.  \n",
    "  **Note:** see next paragraph for details.   \n",
    "\n",
    "- **Sentiment Analysis:**  \n",
    "  Leverages a transformer-based sentiment classification model (via HuggingFace) to compute **continuous sentiment polarity scores** in the range \\[0, 1\\] for both the document and each candidate keyword.\n",
    "\n",
    "- **Combined Scoring and Filtering:**  \n",
    "  Computes a weighted score that balances:\n",
    "  - **Semantic similarity** (cosine similarity between document and candidate embeddings).\n",
    "  - **Sentiment alignment** (1 minus the absolute difference between candidate and document sentiment).\n",
    "\n",
    "  The sentiment alignment score is then mapped to \\[-1, 1\\] to match the range of cosine similarity.  \n",
    "  Candidates with low combined scores are **filtered out before ranking**, allowing sentiment to guide selection from the earliest stages.\n",
    "\n",
    "  The trade-off is controlled by the `alpha` parameter:\n",
    "  - `alpha = 1.0` → pure sentiment-based filtering and ranking.\n",
    "  - `alpha = 0.0` → pure semantic similarity (equivalent to standard KeyBERT).\n",
    "\n",
    "### Candidate Selection in KeyBERT vs Sentiment-Aware Extension\n",
    "\n",
    "In standard KeyBERT, candidate keywords are extracted solely based on **surface-level frequency statistics** using `CountVectorizer`, without semantic or sentiment awareness. This means:\n",
    "\n",
    "- The candidate pool may include terms that are **frequent but sentimentally mismatched** with the document.\n",
    "- In strongly polarized reviews, irrelevant or misleading keywords may still be selected if they occur frequently.\n",
    "\n",
    "The sentiment-aware extension addresses this by introducing a **joint semantic-sentiment filter** immediately after candidate extraction:\n",
    "\n",
    "1. Extract a wide candidate pool using `CountVectorizer`.\n",
    "2. Compute continuous sentiment polarity scores for both the document and each candidate.\n",
    "3. Calculate a combined score per candidate, balancing semantic relevance and emotional alignment.\n",
    "4. Discard candidates whose combined score falls below a filtering threshold.\n",
    "\n",
    "This process ensures that only keywords that are both **topically and emotionally relevant** progress to the final ranking stage.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Step                      | KeyBERT Base                        | Sentiment-Aware Extension                    |\n",
    "|---------------------------|-------------------------------------|----------------------------------------------|\n",
    "| Candidate generation      | Statistical (n-gram frequency)      | Same                                          |\n",
    "| Filtering before ranking  | None                                | Sentiment-semantic combined score filtering  |\n",
    "| Final ranking             | Cosine similarity only              | Combined semantic + sentiment score          |\n",
    "| Sentiment consideration   | Not used                            | Core part of scoring and filtering           |\n",
    "\n",
    "By integrating sentiment into the early filtering phase, this method increases the **precision and contextual appropriateness** of the extracted keywords, particularly in applications where emotional tone is critical.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `model`: The semantic embedding model (e.g., a `SentenceTransformer` from `sentence-transformers`).\n",
    "- `sentiment_model_name`: HuggingFace identifier for the sentiment classifier (default: 5-class multilingual model).\n",
    "- `alpha`: Weight controlling the influence of sentiment (0 = only semantic, 1 = only sentiment).\n",
    "- `candidate_pool_size`: Maximum number of candidates extracted before filtering.\n",
    "- `device`: Device to run the model (`\"cpu\"` or `\"cuda\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d9a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyBERT extension for sentiment-aware keyword extraction\n",
    "# This class extends KeyBERT to include sentiment-aware keyword extraction\n",
    "# by defining a subclass of KeyBERT that modifies the scoring phase to incorporate\n",
    "# sentiment alignment in a post-processing step. The goal is to boost keywords\n",
    "# that are both semantically relevant and emotionally aligned with the overall\n",
    "# sentiment of the input review.\n",
    "class KeyBERTSentimentAware(KB):\n",
    "    \"\"\"\n",
    "    Extension of KeyBERT to integrate sentiment analysis in keyword extraction.\n",
    "\n",
    "    This class overrides and extends parts of KeyBERT's pipeline to:\n",
    "    - Extract a larger candidate pool using CountVectorizer.\n",
    "    - Calculate sentiment polarity scores for the document and candidates,\n",
    "      using a pretrained sentiment classification model with continuous outputs.\n",
    "    - Combine semantic similarity and sentiment alignment scores via a weighting factor alpha.\n",
    "    - Filter candidate keywords based on this combined score before final ranking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SentenceTransformer\n",
    "        Semantic embedding model used by KeyBERT.\n",
    "\n",
    "    sentiment_model_name : str, optional (default: \"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        Identifier of pretrained sentiment model on HuggingFace Hub.\n",
    "\n",
    "    alpha : float, optional (default: 0.7)\n",
    "        Weight to balance sentiment alignment vs semantic similarity.\n",
    "        alpha=1.0 means only sentiment alignment is considered.\n",
    "        alpha=0.0 means only semantic similarity is considered.\n",
    "\n",
    "    candidate_pool_size : int, optional (default: 100)\n",
    "        Maximum number of initial candidate keywords to extract.\n",
    "\n",
    "    device : str, optional (default: \"cpu\")\n",
    "        Device to run embedding and sentiment models on (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        sentiment_model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        alpha: float = 0.7,\n",
    "        candidate_pool_size: int = 100,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        # Validate that the specified device is either 'cpu' or 'cuda'\n",
    "        valid_devices = {\"cpu\", \"cuda\"}\n",
    "        if device not in valid_devices:\n",
    "            raise ValueError(f\"Device must be one of {valid_devices}.\")\n",
    "        \n",
    "        # Check CUDA availability if 'cuda' is requested\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "\n",
    "        # Validate input types to ensure correct usage\n",
    "        if not isinstance(model, SentenceTransformer):\n",
    "            raise TypeError(\"model must be an instance of SentenceTransformer.\")\n",
    "        if not isinstance(sentiment_model_name, str):\n",
    "            raise TypeError(\"sentiment_model_name must be a string.\")\n",
    "        if not isinstance(alpha, float):\n",
    "            raise TypeError(\"alpha must be a float.\")\n",
    "        if not isinstance(candidate_pool_size, int):\n",
    "            raise TypeError(\"candidate_pool_size must be an integer.\")\n",
    "\n",
    "        # Validate value ranges to prevent logical errors\n",
    "        if not (0.0 <= alpha <= 1.0):\n",
    "            raise ValueError(\"alpha must be between 0 and 1 inclusive.\")\n",
    "        if candidate_pool_size <= 0:\n",
    "            raise ValueError(\"candidate_pool_size must be a positive integer.\")\n",
    "\n",
    "        # Initialize the superclass (KeyBERT) with the semantic embedding model\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Assign validated parameters to instance variables\n",
    "        self._alpha = None\n",
    "        self.alpha = alpha\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        self.device = device\n",
    "\n",
    "        # Store the semantic embedding model for embedding computation\n",
    "        self.embedder = model\n",
    "\n",
    "        # Initialize the sentiment model wrapper with the given model name and device\n",
    "        self.sentiment_model = SentimentModel(sentiment_model_name, device=device)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self._alpha\n",
    "\n",
    "    @alpha.setter\n",
    "    def alpha(self, value):\n",
    "        if not 0.0 <= value <= 1.0:\n",
    "            raise ValueError(f\"alpha must be in [0, 1]. Got {value}\")\n",
    "        self._alpha = value\n",
    "\n",
    "    def _get_doc_polarity_continuous(self, doc: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the document's continuous sentiment polarity score as the weighted sum of\n",
    "        predicted class probabilities multiplied by their numeric mappings.\n",
    "\n",
    "        This method overrides and replaces any default sentiment handling in the base class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            The document text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Continuous sentiment polarity score between 0 (very negative) and 1 (very positive).\n",
    "        \"\"\"\n",
    "        # Get probability distribution over sentiment classes for the document\n",
    "        probs = self.sentiment_model.predict_proba([doc])[0]\n",
    "\n",
    "        # Compute continuous polarity as weighted average of class scores\n",
    "        polarity = sum(\n",
    "            p * self.sentiment_model.label_to_score[label]\n",
    "            for p, label in zip(probs, self.sentiment_model.labels_ordered)\n",
    "        )\n",
    "        return polarity\n",
    "\n",
    "    def _get_candidate_polarities(self, candidates) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute continuous sentiment polarity scores for each candidate keyword.\n",
    "\n",
    "        This method extends candidate scoring with sentiment, overriding base candidate processing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        candidates : iterable of str\n",
    "            List of candidate keywords.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of polarity scores for each candidate keyword.\n",
    "        \"\"\"\n",
    "        candidates = list(candidates)  # ensure correct input format for tokenizer\n",
    "        \n",
    "        # Batch predict probabilities for all candidates\n",
    "        probs_list = self.sentiment_model.predict_proba(candidates)\n",
    "        \n",
    "        polarities = []\n",
    "        for probs in probs_list:\n",
    "            # Weighted average as continuous polarity score\n",
    "            polarity = sum(\n",
    "                p * self.sentiment_model.label_to_score[label]\n",
    "                for p, label in zip(probs, self.sentiment_model.labels_ordered)\n",
    "            )\n",
    "            polarities.append(polarity)\n",
    "        return np.array(polarities)\n",
    "\n",
    "    def _select_candidates(\n",
    "        self, \n",
    "        doc: str, \n",
    "        ngram_range: Tuple[int, int] = (1, 3), \n",
    "        threshold: float = 0.4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract initial candidates with CountVectorizer and filter them based on combined\n",
    "        semantic similarity and sentiment alignment scores.\n",
    "\n",
    "        This method replaces the default candidate generation and filtering steps of KeyBERT,\n",
    "        incorporating sentiment filtering before final keyword ranking.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Document text.\n",
    "\n",
    "        ngram_range : tuple of int\n",
    "            N-gram size range for candidate extraction.\n",
    "\n",
    "        threshold : float\n",
    "            Minimum combined score for candidate retention.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of str\n",
    "            Filtered list of candidate keywords.\n",
    "        \"\"\"\n",
    "        # Extract candidates with CountVectorizer (statistical n-grams)\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english',\n",
    "            max_features=self.candidate_pool_size\n",
    "        )\n",
    "        candidates = vectorizer.fit([doc]).get_feature_names_out()\n",
    "\n",
    "        # Compute semantic embeddings for doc and candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity scores\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "        sentiment_scores_mapped = 2 * sentiment_scores - 1\n",
    "\n",
    "        # Combine semantic and sentiment scores with alpha weighting\n",
    "        combined_scores = self.alpha * sentiment_scores_mapped + (1 - self.alpha) * sim_scores\n",
    "\n",
    "        # Filter candidates that meet threshold on combined score\n",
    "        filtered_candidates = [c for c, s in zip(candidates, combined_scores) if s >= threshold]\n",
    "\n",
    "        return filtered_candidates\n",
    "\n",
    "    def extract_keywords(\n",
    "        self,\n",
    "        doc: str,\n",
    "        top_n: int = 5,\n",
    "        candidate_threshold: float = 0.4,\n",
    "        keyphrase_ngram_range: Tuple[int, int] = (1, 3),\n",
    "        print_doc_polarity: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract top keywords from a document by combining semantic similarity and sentiment alignment.\n",
    "\n",
    "        This method overrides the `extract_keywords` method from KeyBERT base class,\n",
    "        adding sentiment-aware candidate filtering and scoring.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Input document text.\n",
    "\n",
    "        top_n : int\n",
    "            Number of keywords to return.\n",
    "\n",
    "        candidate_threshold : float\n",
    "            Threshold score to filter candidate keywords.\n",
    "\n",
    "        keyphrase_ngram_range : tuple of int\n",
    "            N-gram range for candidate keyword extraction.\n",
    "\n",
    "        print_doc_polarity : bool\n",
    "            Whether to print the document's sentiment polarity score.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of tuples\n",
    "            List of (keyword, score) tuples sorted by descending combined score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Select candidates filtered by combined semantic+sentiment scoring\n",
    "        candidates = self._select_candidates(\n",
    "            doc,\n",
    "            ngram_range=keyphrase_ngram_range,\n",
    "            threshold=candidate_threshold\n",
    "        )\n",
    "        if not candidates:\n",
    "            print(\"No candidates passed the sentiment-semantic filter.\")\n",
    "            return []\n",
    "\n",
    "        # Compute semantic embeddings for document and filtered candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity for the document\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "\n",
    "        # Print document polarity if requested\n",
    "        if print_doc_polarity:\n",
    "            # Scale polarity from [0,1] to [0,10]\n",
    "            scaled_pol = doc_pol * 10\n",
    "\n",
    "            # Determine polarity label with neutral zone between 4 and 6 on 0-10 scale\n",
    "            if scaled_pol < 4:\n",
    "                polarity_label = \"Negative\"\n",
    "            elif scaled_pol > 6:\n",
    "                polarity_label = \"Positive\"\n",
    "            else:\n",
    "                polarity_label = \"Neutral\"\n",
    "\n",
    "            print(f\"\\n=== Document Polarity Score: {scaled_pol:.2f} ({polarity_label}) ===\\n\")\n",
    "\n",
    "        # Compute sentiment polarities for candidates\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores (range [-1,1])\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores in [0,1] and map to [-1,1]\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "        sentiment_scores_mapped = 2 * sentiment_scores - 1\n",
    "\n",
    "        # Final combined score with weighting factor alpha in [-1,1]\n",
    "        final_scores = self.alpha * sentiment_scores_mapped + (1 - self.alpha) * sim_scores\n",
    "\n",
    "        # Select top_n keywords sorted by combined score descending\n",
    "        top_indices = np.argsort(final_scores)[-top_n:][::-1]\n",
    "\n",
    "        return [(candidates[i], final_scores[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d991b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1533b",
   "metadata": {},
   "source": [
    "## Test 1: Basic Keyword Extraction with Sentiment-Aware KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebd768",
   "metadata": {},
   "source": [
    "This first test demonstrates the basic usage of the `KeyBERTSentimentAware` class on a simple document. We extract the top keywords combining semantic similarity and sentiment alignment with default parameter settings.\n",
    "\n",
    "**Objectives:**\n",
    "- Verify that the class instantiates correctly.\n",
    "- Check that keywords are extracted without errors.\n",
    "- Observe the impact of sentiment-aware ranking on keyword scores.\n",
    "\n",
    "We use a short, clearly positive sentence to observe how sentiment affects the keyword selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48528652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 9.31 (Positive) ===\n",
      "\n",
      "Extracted Keywords and Scores:\n",
      "\n",
      "movie fantastic beautiful \t score: 0.9029\n",
      "movie fantastic      \t score: 0.8555\n",
      "great acting         \t score: 0.8540\n",
      "beautiful visuals great \t score: 0.8231\n",
      "fantastic beautiful visuals \t score: 0.8135\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "# Sample document with positive sentiment\n",
    "doc = \"The movie was fantastic with beautiful visuals and great acting.\"\n",
    "\n",
    "# Extract top 5 keywords\n",
    "keywords = kw_model.extract_keywords(doc, top_n=5, print_doc_polarity=True)\n",
    "\n",
    "print(\"Extracted Keywords and Scores:\\n\")\n",
    "for keyword, score in keywords:\n",
    "    print(f\"{keyword:20s} \\t score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7016e7",
   "metadata": {},
   "source": [
    "## Test 2: Comparing Sentiment-Aware KeyBERT with Base KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e6b8e",
   "metadata": {},
   "source": [
    "In this test, we compare the keywords extracted by the sentiment-aware extension with those from the original KeyBERT model that relies solely on semantic similarity.\n",
    "\n",
    "**Objectives:**\n",
    "- Highlight differences in keyword selection between semantic-only and sentiment-aware approaches.\n",
    "- Understand the effect of integrating sentiment on keyword ranking.\n",
    "- Use the same document for a fair comparison.\n",
    "\n",
    "We use a document containing both positive and negative elements to see how sentiment influences the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2c80e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE KeyBERT Keywords\n",
      "pacing               0.3888\n",
      "film                 0.3627\n",
      "soundtrack           0.3257\n",
      "predictable          0.3226\n",
      "plot                 0.2605\n",
      "\n",
      "=== Document Polarity Score: 3.22 (Negative) ===\n",
      "\n",
      "Sentiment-Aware KeyBERT Keywords:\n",
      "\n",
      "slow                 0.7235\n",
      "predictable          0.6371\n",
      "plot                 0.4930\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize base KeyBERT model with the same embedding model\n",
    "kw_base = KB(model=embedding_model)\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_sentiment = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc_mixed = (\n",
    "    \"The film had stunning visual effects and an amazing soundtrack, \"\n",
    "    \"but the plot was predictable and the pacing was slow at times.\"\n",
    ")\n",
    "\n",
    "# Extract keywords using base KeyBERT\n",
    "base_keywords = kw_base.extract_keywords(doc_mixed, top_n=5, keyphrase_ngram_range=(1, 1))\n",
    "\n",
    "print(\"BASE KeyBERT Keywords:\\b\")\n",
    "for kw, score in base_keywords:\n",
    "    print(f\"{kw:20s} {score:.4f}\")\n",
    "\n",
    "# Extract keywords using sentiment-aware KeyBERT (with weight_sentiment=0.7)\n",
    "sentiment_keywords = kw_sentiment.extract_keywords(\n",
    "    doc_mixed,\n",
    "    top_n=5,\n",
    "    keyphrase_ngram_range=(1, 1),\n",
    "    print_doc_polarity=True\n",
    ")\n",
    "\n",
    "print(\"Sentiment-Aware KeyBERT Keywords:\\n\")\n",
    "for kw, score in sentiment_keywords:\n",
    "    print(f\"{kw:20s} {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d686562",
   "metadata": {},
   "source": [
    "## Test 3: Candidate Filtering with Different Thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da344ff",
   "metadata": {},
   "source": [
    "\n",
    "In this test, we explore how varying the candidate filtering threshold affects the pool of candidate keywords before the final ranking.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the impact of the `candidate_threshold` parameter on candidate selection.\n",
    "- Observe how stricter thresholds reduce candidate pool size and potentially increase keyword relevance.\n",
    "- Use a moderately complex document with mixed sentiment.\n",
    "\n",
    "This test highlights the balance between recall and precision in candidate filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db445b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Threshold: 0.0\n",
      "Number of candidates after filtering: 26\n",
      "Candidates: ['beautiful cinematography', 'beautiful cinematography strong', 'cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong', 'strong performances', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.2\n",
      "Number of candidates after filtering: 22\n",
      "Candidates: ['cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.4\n",
      "Number of candidates after filtering: 16\n",
      "Candidates: ['cinematography', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow times', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 0.6\n",
      "Number of candidates after filtering: 8\n",
      "Candidates: ['convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 0.8\n",
      "Number of candidates after filtering: 0\n",
      "Candidates: []\n",
      "\n",
      "Candidate Threshold: 1.0\n",
      "Number of candidates after filtering: 0\n",
      "Candidates: []\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc = (\n",
    "    \"Despite the beautiful cinematography and strong performances, \"\n",
    "    \"the storyline was convoluted and difficult to follow at times.\"\n",
    ")\n",
    "\n",
    "thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nCandidate Threshold: {thresh}\")\n",
    "    candidates = kw_model._select_candidates(doc, threshold=thresh)\n",
    "    print(f\"Number of candidates after filtering: {len(candidates)}\")\n",
    "    print(\"Candidates:\", candidates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10aa95",
   "metadata": {},
   "source": [
    "## Test 4: Impact of Sentiment Weighting (`weight_sentiment`) on Keyword Ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90d8fd",
   "metadata": {},
   "source": [
    "\n",
    "This test examines how changing the `weight_sentiment` parameter influences the balance between semantic similarity and sentiment alignment in keyword scoring.\n",
    "\n",
    "**Objectives:**\n",
    "- Observe differences in extracted keywords when prioritizing sentiment vs. semantic relevance.\n",
    "- Understand the flexibility of the model in adapting to different use cases by tuning `weight_sentiment`.\n",
    "- Use a document with both positive and negative sentiments to highlight effect.\n",
    "\n",
    "We test three values of `weight_sentiment`: 0.0 (semantic only), 0.5 (balanced), and 1.0 (sentiment only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32000df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "alpha = 0.0\n",
      "\n",
      "film ending          \t score: 0.5824\n",
      "breathtaking visuals \t score: 0.4919\n",
      "outstanding soundtrack \t score: 0.4868\n",
      "story lead           \t score: 0.4825\n",
      "heartfelt storytelling \t score: 0.4742\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "alpha = 0.25\n",
      "\n",
      "story lead           \t score: 0.6053\n",
      "film ending          \t score: 0.5888\n",
      "breathtaking visuals \t score: 0.5784\n",
      "narrative arc        \t score: 0.5625\n",
      "audience story       \t score: 0.5625\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "alpha = 0.5\n",
      "\n",
      "story lead           \t score: 0.7281\n",
      "narrative arc        \t score: 0.6953\n",
      "audience story       \t score: 0.6869\n",
      "characters pacing    \t score: 0.6867\n",
      "lead actors          \t score: 0.6837\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "alpha = 0.75\n",
      "\n",
      "story lead           \t score: 0.8509\n",
      "lead actors          \t score: 0.8311\n",
      "narrative arc        \t score: 0.8281\n",
      "characters pacing    \t score: 0.8130\n",
      "storytelling         \t score: 0.8124\n",
      "\n",
      "=== Document Polarity Score: 6.07 (Positive) ===\n",
      "\n",
      "\n",
      "alpha = 1.0\n",
      "\n",
      "lead actors          \t score: 0.9785\n",
      "audience             \t score: 0.9778\n",
      "story lead           \t score: 0.9736\n",
      "actors               \t score: 0.9643\n",
      "narrative            \t score: 0.9640\n",
      "\n",
      "alpha = -0.2 caused error: alpha must be in [0, 1]. Got -0.2\n",
      "\n",
      "alpha = 1.2 caused error: alpha must be in [0, 1]. Got 1.2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc = (\"The film featured breathtaking visuals and an outstanding soundtrack that \"\n",
    "       \"truly immersed the audience in the story. The lead actors delivered powerful \"\n",
    "       \"performances, bringing depth and emotion to their characters. However, the pacing \"\n",
    "       \"was somewhat slow in the middle, and certain plot points felt predictable and underdeveloped. \"\n",
    "       \"Despite these shortcomings, the compelling narrative arc and the strong direction kept \"\n",
    "       \"the viewers engaged throughout. The film's ending was uplifting and satisfying, \"\n",
    "       \"leaving a lasting impression. Overall, it was a highly enjoyable experience that combined \"\n",
    "       \"artistic excellence with heartfelt storytelling.\")\n",
    "\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0, -0.2, 1.2]  # Includes invalid weights\n",
    "\n",
    "for alpha in weights:\n",
    "    try:\n",
    "        kw_model.alpha = alpha\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            doc, \n",
    "            top_n=5, \n",
    "            print_doc_polarity=True, \n",
    "            keyphrase_ngram_range=(1, 2)\n",
    "        )\n",
    "        print(f\"\\nalpha = {alpha}\\n\")\n",
    "        for kw, score in keywords:\n",
    "            print(f\"{kw:20s} \\t score: {score:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nalpha = {alpha} caused error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c969d36",
   "metadata": {},
   "source": [
    "## Test 5: Keyword Extraction on a Document with Mixed Sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166bfa0f",
   "metadata": {},
   "source": [
    "This test evaluates the performance of the `KeyBERTSentimentAware` model on a document containing both positive and negative sentiments. It helps observe how the model balances semantic relevance and sentiment alignment when the document expresses contrasting opinions.\n",
    "\n",
    "**Objectives:**\n",
    "- Verify that the model extracts keywords reflecting both the positive and negative aspects of the text.\n",
    "- Assess the impact of sentiment-aware filtering and scoring in realistic scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c89d3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 4.72 (Neutral) ===\n",
      "\n",
      "Keywords extracted from mixed sentiment document:\n",
      "actors performed adequately \t score: 0.8231\n",
      "storyline straightforward \t score: 0.7820\n",
      "adequately storyline straightforward \t score: 0.7623\n",
      "flaws soundtrack complemented \t score: 0.7451\n",
      "adequately storyline \t score: 0.7011\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.7\n",
    "kw_model = KeyBERTSentimentAware(model=embedding_model)\n",
    "\n",
    "doc_mixed = (\n",
    "    \"The movie had decent cinematography and the actors performed adequately. \"\n",
    "    \"The storyline was straightforward and predictable, with neither surprising twists nor major flaws. \"\n",
    "    \"The soundtrack complemented the scenes suitably, without standing out. \"\n",
    "    \"Overall, the film provided a passable entertainment experience—nothing exceptional,\" \n",
    "    \"but not disappointing either.\"\n",
    ")\n",
    "\n",
    "# Extract top 5 keywords with n-grams up to length 3\n",
    "keywords = kw_model.extract_keywords(doc_mixed, top_n=5, keyphrase_ngram_range=(1, 3), print_doc_polarity=True)\n",
    "\n",
    "print(\"Keywords extracted from mixed sentiment document:\")\n",
    "for kw, score in keywords:\n",
    "    print(f\"{kw:20s} \\t score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3a48",
   "metadata": {},
   "source": [
    "## Test 6:  Comparing Keyword Extraction Between Base KeyBERT and Sentiment-Aware KeyBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d719a1",
   "metadata": {},
   "source": [
    "This test compares the standard `KeyBERT` model with the `KeyBERTSentimentAware` extension on a small set of reviews with varying sentiment. The goal is to assess how sentiment integration influences the selection and ranking of keywords.\n",
    "\n",
    "**Objectives:**\n",
    "- Compute sentiment polarity for each review and observe the average emotional tone across the dataset.\n",
    "- Extract top keywords using both the standard KeyBERT and the sentiment-aware model.\n",
    "- Rank keywords based on their average score and frequency of occurrence across reviews.\n",
    "- Apply semantic diversity filtering to ensure that selected keywords are not too similar to each other.\n",
    "- Compare the final top 5 keywords from both models, and observe how sentiment-aware keywords differ in alignment and relevance.\n",
    "\n",
    "This test demonstrates how sentiment-aware filtering can shift the focus of keyword extraction, promoting emotionally consistent terms and reducing the prominence of keywords that conflict with the document’s tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6981df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity per review (0 = negative, 1 = positive):\n",
      " Review #1: Polarity = 0.957 (Positive)\n",
      " Review #2: Polarity = 0.983 (Positive)\n",
      " Review #3: Polarity = 0.877 (Positive)\n",
      " Review #4: Polarity = 0.980 (Positive)\n",
      " Review #5: Polarity = 0.993 (Positive)\n",
      " Review #6: Polarity = 0.025 (Negative)\n",
      "\n",
      "Average sentiment polarity across all reviews: 0.803 (Positive)\n",
      "\n",
      "\n",
      "Top 5 Diverse Keywords - Base KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "emotional depth screenplay                   0.6815\n",
      "uplifting cinematic journey                  0.6554\n",
      "absolute masterpiece                         0.6435\n",
      "narrative excellent acting                   0.6363\n",
      "beautifully crafted film                     0.6284\n",
      "\n",
      "Top 5 Diverse Keywords - Sentiment-Aware KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "absolute masterpiece                         0.8726          0.978\n",
      "breathtaking soundtrack perfectly            0.8676          0.945\n",
      "best films                                   0.8624          0.969\n",
      "truly inspiring movie                        0.8393          0.966\n",
      "excellent acting                             0.8270          0.943\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize base KeyBERT (semantic-only) and sentiment-aware KeyBERT\n",
    "kw_base = KB(model=embedding_model)\n",
    "kw_sentiment = KeyBERTSentimentAware(model=embedding_model, alpha=0.7)\n",
    "\n",
    "# Define reviews with mixed sentiments to highlight topic differences\n",
    "reviews = [\n",
    "    \"\"\"This film is a stunning achievement in storytelling, with unforgettable characters and a gripping plot.\n",
    "    The visuals are breathtaking, and the soundtrack perfectly complements every scene.\"\"\",\n",
    "\n",
    "    \"\"\"I was completely captivated from start to finish. The performances were heartfelt,\n",
    "    and the director’s vision shines through in every frame. A truly inspiring movie experience.\"\"\",\n",
    "\n",
    "    \"\"\"A beautifully crafted film with rich emotional depth. The screenplay is tight,\n",
    "    and the cinematography creates an immersive atmosphere that kept me hooked.\"\"\",\n",
    "\n",
    "    \"\"\"One of the best films I've seen in years. It combines a compelling narrative with excellent acting,\n",
    "    making it both entertaining and thought-provoking.\"\"\",\n",
    "\n",
    "    \"\"\"An absolute masterpiece! Every element, from the score to the visual effects,\n",
    "    contributes to a powerful and uplifting cinematic journey.\"\"\",\n",
    "\n",
    "    \"\"\"“An utter disaster of a film. The plot was incoherent, characters were completely flat, and the \n",
    "    pacing was excruciatingly slow. Dialogue felt forced and unnatural throughout. \n",
    "    I struggled to stay awake — a total waste of time.”\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Function to convert continuous polarity (0 to 1) into categorical sentiment label\n",
    "def polarity_label(p):\n",
    "    if p < 0.4:\n",
    "        return \"Negative\"\n",
    "    elif p > 0.6:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Compute and print the sentiment polarity for each review individually\n",
    "print(\"Sentiment polarity per review (0 = negative, 1 = positive):\")\n",
    "polarities = []\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    pol = kw_sentiment._get_doc_polarity_continuous(review)\n",
    "    polarities.append(pol)\n",
    "    print(f\" Review #{i}: Polarity = {pol:.3f} ({polarity_label(pol)})\")\n",
    "\n",
    "# Compute and print average polarity over all reviews with categorical label\n",
    "mean_polarity = np.mean(polarities)\n",
    "print(f\"\\nAverage sentiment polarity across all reviews: {mean_polarity:.3f} ({polarity_label(mean_polarity)})\\n\")\n",
    "\n",
    "# Function to accumulate scores and polarities per keyword for a given model\n",
    "def accumulate_keywords(model, reviews):\n",
    "    scores = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    polarities = defaultdict(list)\n",
    "\n",
    "    for review in reviews:\n",
    "        # Extract keywords with their scores\n",
    "        kws = model.extract_keywords(review, top_n=7, keyphrase_ngram_range=(1,3))\n",
    "        for kw, score in kws:\n",
    "            scores[kw] += score\n",
    "            counts[kw] += 1\n",
    "\n",
    "            # For sentiment-aware model: calculate polarity per keyword\n",
    "            if hasattr(model, '_get_candidate_polarities'):\n",
    "                cand_pol = kw_sentiment._get_candidate_polarities([kw])[0]\n",
    "                polarities[kw].append(cand_pol)\n",
    "    return scores, counts, polarities\n",
    "\n",
    "# Accumulate data for both models\n",
    "base_scores, base_counts, base_pols = accumulate_keywords(kw_base, reviews)\n",
    "sent_scores, sent_counts, sent_pols = accumulate_keywords(kw_sentiment, reviews)\n",
    "\n",
    "# Compute average score and average polarity per keyword\n",
    "def compute_averages(scores, counts, polarities):\n",
    "    avg_scores = {kw: scores[kw] / counts[kw] for kw in scores}\n",
    "    avg_pols = {kw: np.mean(polarities[kw]) for kw in polarities} if polarities else {}\n",
    "    return avg_scores, avg_pols\n",
    "\n",
    "base_avg_scores, base_avg_pols = compute_averages(base_scores, base_counts, base_pols)\n",
    "sent_avg_scores, sent_avg_pols = compute_averages(sent_scores, sent_counts, sent_pols)\n",
    "\n",
    "# Ranking function combining average score and normalized frequency to balance importance and consistency\n",
    "def rank_keywords(avg_scores, counts, alpha=0.7):\n",
    "    max_count = max(counts.values()) if counts else 1\n",
    "    ranked = []\n",
    "    for kw in avg_scores:\n",
    "        freq_norm = counts[kw] / max_count  # Normalize frequency to [0,1]\n",
    "        combined_score = alpha * avg_scores[kw] + (1 - alpha) * freq_norm\n",
    "        ranked.append((kw, combined_score))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)  # Sort descending by combined score\n",
    "    return ranked\n",
    "\n",
    "# Rank keywords for both models\n",
    "base_ranked = rank_keywords(base_avg_scores, base_counts)\n",
    "sent_ranked = rank_keywords(sent_avg_scores, sent_counts)\n",
    "\n",
    "# Function to embed keyword phrases and normalize embeddings for cosine similarity\n",
    "def embed_keywords(keywords):\n",
    "    return embedding_model.encode(keywords, convert_to_tensor=True, normalize_embeddings=True).cpu().numpy()\n",
    "\n",
    "# Select top N keywords ensuring semantic diversity by filtering out candidates too similar to already selected ones\n",
    "def select_diverse_keywords(ranked_list, avg_scores, avg_pols, top_n=5, similarity_threshold=0.7):\n",
    "    selected = []\n",
    "    selected_embs = []\n",
    "\n",
    "    all_keywords = [kw for kw, _ in ranked_list]\n",
    "    all_embs = embed_keywords(all_keywords)\n",
    "\n",
    "    for i, (kw, _) in enumerate(ranked_list):\n",
    "        if not selected:\n",
    "            selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "            selected_embs.append(all_embs[i])\n",
    "        else:\n",
    "            emb = all_embs[i]\n",
    "            # Compute max cosine similarity with selected keywords\n",
    "            sims = [np.dot(emb, se) for se in selected_embs]\n",
    "            if max(sims) < similarity_threshold:\n",
    "                selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "                selected_embs.append(emb)\n",
    "        if len(selected) >= top_n:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "# Select top 5 diverse keywords for each model\n",
    "top_base = select_diverse_keywords(base_ranked, base_avg_scores, base_avg_pols, top_n=5)\n",
    "top_sent = select_diverse_keywords(sent_ranked, sent_avg_scores, sent_avg_pols, top_n=5)\n",
    "\n",
    "# Nicely print the final selected keywords with average scores and polarities (only for sentiment-aware)\n",
    "def print_topics(title, topics, show_polarity=True, flop=False):\n",
    "    if flop:\n",
    "        print(f\"\\n{title} (Flop {len(topics)}):\")\n",
    "        # Reverse order for flop to show worst first\n",
    "        topics_to_print = reversed(topics)\n",
    "    else:\n",
    "        print(f\"\\n{title} (Top {len(topics)}):\")\n",
    "        topics_to_print = topics\n",
    "\n",
    "    header = f\"{'Keyword':40s} {'Avg Score':>10s}\"\n",
    "    if show_polarity:\n",
    "        header += f\" {'Avg Polarity':>14s}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for kw, score, pol in topics_to_print:\n",
    "        line = f\"{kw:40s} {score:10.4f}\"\n",
    "        if show_polarity:\n",
    "            line += f\" {pol:14.3f}\"\n",
    "        print(line)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print_topics(\"Top 5 Diverse Keywords - Base KeyBERT\", top_base, show_polarity=False)\n",
    "print_topics(\"Top 5 Diverse Keywords - Sentiment-Aware KeyBERT\", top_sent, show_polarity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "399c8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flop 5 Keywords - Base KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "combines compelling narrative                0.4711\n",
      "heartfelt director                           0.5045\n",
      "director vision shines                       0.5103\n",
      "best films ve                                0.5111\n",
      "dialogue felt                                0.5181\n",
      "\n",
      "Flop 5 Keywords - Sentiment-Aware KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "powerful uplifting                           0.7013          0.900\n",
      "unnatural struggled stay                     0.7101          0.095\n",
      "narrative excellent acting                   0.7114          0.851\n",
      "inspiring movie                              0.7116          0.878\n",
      "unnatural struggled                          0.7161          0.085\n"
     ]
    }
   ],
   "source": [
    "# Select flop 5 keywords (lowest ranked) for each model\n",
    "flop_base = base_ranked[-5:]\n",
    "flop_sent = sent_ranked[-5:]\n",
    "\n",
    "# Disabling diversity filtering for flop: simply take the flop keywords as is\n",
    "flop_base_diverse = [(kw, base_avg_scores[kw], base_avg_pols.get(kw, float('nan'))) for kw, _ in flop_base]\n",
    "flop_sent_diverse = [(kw, sent_avg_scores[kw], sent_avg_pols.get(kw, float('nan'))) for kw, _ in flop_sent]\n",
    "\n",
    "# Print flop keywords nicely\n",
    "print_topics(\"Flop 5 Keywords - Base KeyBERT\", flop_base_diverse, show_polarity=False, flop=True)\n",
    "print_topics(\"Flop 5 Keywords - Sentiment-Aware KeyBERT\", flop_sent_diverse, show_polarity=True, flop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308cd0f",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "The comparison highlights key differences between the base KeyBERT and the sentiment-aware extension:\n",
    "\n",
    "- **KeyBERT** selects keywords based purely on semantic similarity, often favoring neutral or generic terms that may not reflect the review's tone.\n",
    "- **Sentiment-aware KeyBERT** integrates sentiment polarity during both filtering and scoring, surfacing keywords that are emotionally aligned with each individual review.\n",
    "\n",
    "This results in:\n",
    "- More emotionally coherent keywords at both the single-review and aggregate level.\n",
    "- Penalization of sentimentally inconsistent keywords in globally positive (or negative) datasets.\n",
    "- Improved interpretability for applications where sentiment is important, such as opinion mining or customer feedback analysis.\n",
    "\n",
    "Incorporating sentiment enhances keyword relevance, emotional alignment, and the overall contextual quality of extracted topics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
