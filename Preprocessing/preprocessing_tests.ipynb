{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to **test and demonstrate the behavior of a lightweight text preprocessing pipeline** implemented in the `Preprocessor` class. The preprocessing steps are tailored to prepare movie reviews for transformer-based keyword extraction using models like **KeyBERT** with **`all-MiniLM-L6-v2`** embeddings. \n",
    "\n",
    "The main preprocessing operations include:\n",
    "- **Typo correction** using `autocorrect`\n",
    "- **Punctuation-spacing normalization**, ensuring readability for tokenizers\n",
    "- **Nonsense/empty review filtering**, removing unusable entries\n",
    "- **Lemmatization** using **spaCy** for efficient text normalization\n",
    "\n",
    "Each step is tested with controlled input examples to verify correctness and robustness before applying the pipeline to full datasets.\n",
    "\n",
    "> **Note**: Stop word removal is **not** handled at this stage, as it is delegated to the **custom KeyBERT extensions**, which apply stop word filtering directly during keyword selection. This ensures compatibility with downstream transformers and maximizes context-aware relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy is already installed.\n",
      "autocorrect is already installed.\n",
      "wordfreq is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",                         \n",
    "    \"spacy\",           \n",
    "    \"autocorrect\",\n",
    "    \"wordfreq\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from wordfreq import zipf_frequency # type: ignore\n",
    "\n",
    "# spaCy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy (download if not present)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Custom Preprocessor\n",
    "\n",
    "This cell imports the `Preprocessor` class from the custom `preprocessing.py` module.  \n",
    "The class encapsulates all the text cleaning operations required to prepare review texts before passing them to a Transformer-based model.  \n",
    "It provides methods for typo correction, punctuation normalization, lemmatization, and filtering of nonsensical content, and will be applied to each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 ‚Äì Typo Correction\n",
    "\n",
    "This test evaluates the typo correction capabilities of the `Preprocessor` class.\n",
    "\n",
    "The input consists of sentences with common spelling errors such as:\n",
    "- `\"amazng\"` ‚Üí `\"amazing\"`  \n",
    "- `\"dirction\"` ‚Üí `\"direction\"`  \n",
    "- `\"absolutly\"` ‚Üí `\"absolutely\"`\n",
    "\n",
    "Typo correction is a key step in improving the quality of keyword extraction and semantic embeddings.  \n",
    "The logic implemented combines several techniques:\n",
    "\n",
    "- **Whitespace normalization**: collapses multiple consecutive spaces into a single space.\n",
    "\n",
    "- **Proper noun preservation**: capitalized words that are not at the beginning of a sentence are excluded from correction to avoid altering named entities.\n",
    "\n",
    "- **Character repetition handling**:\n",
    "   - If a word contains 3 or more repeated alphabetic characters (e.g., \"loooong\"), they are first reduced to 2 (‚Üí \"loong\"), then to 1 (‚Üí \"long\"), checking validity at each step.\n",
    "   - If reducing the repetition results in a valid word, that version is kept.\n",
    "\n",
    "- **Autocorrect fallback**: if no valid form is found through the above steps, the word is passed to `autocorrect` for correction.\n",
    "\n",
    "This combined approach prevents overcorrection (e.g., `\"baad\"` becoming `\"band\"`) and enhances the robustness of the text preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Typo Correction Test ===\n",
      "\n",
      "Review 1:\n",
      "Original:  This movi was amazng\n",
      "Corrected: This move was amazing\n",
      "\n",
      "Review 2:\n",
      "Original:  The dirction of the film is goooood\n",
      "Corrected: The direction of the film is good\n",
      "\n",
      "Review 3:\n",
      "Original:  Charactrs were not believabl\n",
      "Corrected: Characters were not believable\n",
      "\n",
      "Review 4:\n",
      "Original:  Absolutly stunning performnce by the lead actr\n",
      "Corrected: Absolutely stunning performance by the lead actor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Sample reviews with typos\n",
    "typo_reviews = [\n",
    "    \"This movi was amazng\",\n",
    "    \"The dirction of the film is goooood\",\n",
    "    \"Charactrs were not believabl\",\n",
    "    \"Absolutly stunning performnce by the lead actr\",\n",
    "]\n",
    "\n",
    "# Apply typo correction only\n",
    "print(\"=== Typo Correction Test ===\\n\")\n",
    "for i, review in enumerate(typo_reviews, 1):\n",
    "    corrected = pre.correct_typos(review)\n",
    "    print(f\"Review {i}:\\nOriginal:  {review}\\nCorrected: {corrected}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 ‚Äì Punctuation Spacing Normalization\n",
    "\n",
    "This test evaluates the punctuation spacing normalization step of the `Preprocessor` class.\n",
    "\n",
    "The objective is to ensure that a **space is inserted after punctuation marks** (such as `.`, `,`, `!`, `?`, `;`, `:`) **only when appropriate**.  \n",
    "Specifically, a space is added **only if** the punctuation is **directly followed by a letter or underscore**, and **not** by a digit or another punctuation mark.\n",
    "\n",
    "This normalization improves **readability** and prevents the **merging of adjacent words**, which could negatively affect downstream tasks like tokenization or embedding.  \n",
    "At the same time, it preserves numeric formats and punctuation sequences such as:\n",
    "- `\"Hello.This\"` ‚Üí `\"Hello. This\"` \n",
    "- `\"Wow!!!Great\"` ‚Üí `\"Wow!!! Great\"` \n",
    "- `\"Price is $200,000.00\"` ‚Üí remains unchanged\n",
    "\n",
    "By applying this rule selectively, the model maintains clean sentence structure without corrupting numerical data or stylistic emphasis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 1: This movie is great!Amazing direction.\n",
      "Normalized 1: This movie is great! Amazing direction.\n",
      "\n",
      "Original 2: Wait...what?Really?\n",
      "Normalized 2: Wait... what? Really?\n",
      "\n",
      "Original 3: Incredible,unbelievable!Must watch.\n",
      "Normalized 3: Incredible, unbelievable! Must watch.\n",
      "\n",
      "Original 4: I loved it.The actors were amazing.\n",
      "Normalized 4: I loved it. The actors were amazing.\n",
      "\n",
      "Original 5: I paid 300,000$ this house!!!It was worth it.\n",
      "Normalized 5: I paid 300,000$ this house!!! It was worth it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Sample reviews with punctuation issues\n",
    "sample_texts = [\n",
    "    \"This movie is great!Amazing direction.\",\n",
    "    \"Wait...what?Really?\",\n",
    "    \"Incredible,unbelievable!Must watch.\",\n",
    "    \"I loved it.The actors were amazing.\",\n",
    "    \"I paid 300,000$ this house!!!It was worth it.\",\n",
    "]\n",
    "\n",
    "# Apply only punctuation spacing normalization\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    normalized = pre.normalize_spacing(text)\n",
    "    print(f\"Original {i}: {text}\")\n",
    "    print(f\"Normalized {i}: {normalized}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 ‚Äì Nonsense Detection\n",
    "\n",
    "This test evaluates the ability of the `Preprocessor` class to detect and flag **nonsensical or low-quality reviews**.\n",
    "\n",
    "The implemented logic marks a review as *nonsense* if it satisfies one of the following conditions:\n",
    "- The text is **too short** (e.g., fewer than 10 characters).\n",
    "- The **ratio of alphabetic characters** to total characters is very low (e.g., dominated by symbols or numbers).\n",
    "\n",
    "This filtering step is essential to discard meaningless entries that could negatively affect downstream tasks such as embedding generation or keyword extraction.\n",
    "\n",
    "We isolate and apply only the **nonsense detection** module in this test, checking how it handles various inputs including:\n",
    "- Empty strings  \n",
    "- Symbol-only content  \n",
    "- Short but meaningful phrases  \n",
    "- Number-dominated text  \n",
    "\n",
    "Each input is labeled as either `OK` (valid) or `NONSENSE` (to be discarded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: '!!!...??' ‚Üí NONSENSE\n",
      "\n",
      "Sample 2: '1234567890' ‚Üí NONSENSE\n",
      "\n",
      "Sample 3: 'Ok' ‚Üí NONSENSE\n",
      "\n",
      "Sample 4: 'This is fine.' ‚Üí OK\n",
      "\n",
      "Sample 5: '....' ‚Üí NONSENSE\n",
      "\n",
      "Sample 6: '!!!!????....' ‚Üí NONSENSE\n",
      "\n",
      "Sample 7: 'The movie was good.' ‚Üí OK\n",
      "\n",
      "Sample 8: 'üëçüèªüëçüèªüëçüèª' ‚Üí NONSENSE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Preprocessor\n",
    "from preprocessing import Preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Test cases for nonsense detection\n",
    "samples = [\n",
    "    \"!!!...??\",               # Only punctuation\n",
    "    \"1234567890\",             # Only numbers\n",
    "    \"Ok\",                     # Too short\n",
    "    \"This is fine.\",          # Valid sentence\n",
    "    \"....\",                   # Dots only\n",
    "    \"!!!!????....\",           # Random punctuation\n",
    "    \"The movie was good.\",    # Proper review\n",
    "    \"üëçüèªüëçüèªüëçüèª\"                   # Emoticons only\n",
    "]\n",
    "\n",
    "# Apply nonsense detection logic\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    result = pre.is_nonsense(sample)\n",
    "    status = \"NONSENSE\" if result else \"OK\"\n",
    "    print(f\"Sample {i}: '{sample}' ‚Üí {status}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4 ‚Äì Lemmatization\n",
    "\n",
    "In this test, we evaluate the **lemmatization** capability of the `Preprocessor` class, implemented using `spaCy`.\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form (lemma), which helps normalize textual data. For instance:\n",
    "- `\"running\"` ‚Üí `\"run\"`\n",
    "- `\"cars\"` ‚Üí `\"car\"`\n",
    "- `\"was\"` ‚Üí `\"be\"`\n",
    "\n",
    "In this test, we apply **only the lemmatization step**, using a variety of phrases containing inflected forms of verbs and nouns, and inspect whether the transformations are performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lemmatization Test ===\n",
      "Sample 1:\n",
      "Original    ‚Üí The cats are running in the gardens.\n",
      "Lemmatized  ‚Üí The cat be run in the garden.\n",
      "\n",
      "Sample 2:\n",
      "Original    ‚Üí She was eating apples.\n",
      "Lemmatized  ‚Üí She be eat apple.\n",
      "\n",
      "Sample 3:\n",
      "Original    ‚Üí They have been thinking about it.\n",
      "Lemmatized  ‚Üí They have be think about it.\n",
      "\n",
      "Sample 4:\n",
      "Original    ‚Üí He walks, talks, and smiles.\n",
      "Lemmatized  ‚Üí He walk, talk, and smile.\n",
      "\n",
      "Sample 5:\n",
      "Original    ‚Üí Children played with toys yesterday.\n",
      "Lemmatized  ‚Üí Child play with toy yesterday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences with inflected forms\n",
    "lemmatization_samples = [\n",
    "    \"The cats are running in the gardens.\",\n",
    "    \"She was eating apples.\",\n",
    "    \"They have been thinking about it.\",\n",
    "    \"He walks, talks, and smiles.\",\n",
    "    \"Children played with toys yesterday.\"\n",
    "]\n",
    "\n",
    "# Apply lemmatization\n",
    "print(\"=== Lemmatization Test ===\")\n",
    "for i, text in enumerate(lemmatization_samples, 1):\n",
    "    result = pre.lemmatize_text(text)\n",
    "    print(f\"Sample {i}:\\nOriginal    ‚Üí {text}\\nLemmatized  ‚Üí {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5 ‚Äì Full Preprocessing Pipeline\n",
    "\n",
    "This test evaluates the **entire preprocessing pipeline** implemented in the `Preprocessor` class.  \n",
    "The pipeline includes all previously tested steps, executed in sequence:\n",
    "\n",
    "1. **Typo Correction** ‚Üí Fixes common spelling mistakes.\n",
    "2. **Punctuation Spacing Normalization** ‚Üí Ensures correct spacing after punctuation marks, but only when followed by a letter.\n",
    "3. **Nonsense Detection** ‚Üí Removes reviews that are too short or composed mostly of symbols and digits.\n",
    "4. **Lemmatization** ‚Üí Converts words to their base form using `spaCy` while preserving punctuation formatting and casing.\n",
    "\n",
    "We apply this pipeline to a variety of noisy reviews to verify its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "Original:  This movie is absoltly      amazng!The charactrs were believabl(not all of theeeem).\n",
      "Processed: This movie be absolutely amazng! The character be believabl(not all of they).\n",
      "\n",
      "=== Sample 2 ===\n",
      "Original:  Whaat??Noo...thiiiiis is baad dirction!!!\n",
      "Processed: Whaat?? Noo... this be bad direction!!!\n",
      "\n",
      "=== Sample 3 ===\n",
      "Original:  1234 .... ü§ñü§ñü§ñ ???\n",
      "REMOVED (nonsense)\n",
      "\n",
      "=== Sample 4 ===\n",
      "Original:  I was stunned.The performnce was stunning.\n",
      "Processed: I be stun. The performance be stunning.\n",
      "\n",
      "=== Sample 5 ===\n",
      "Original:  LOOOOOVED the filmmmm!!!!!I see it on VHS and the end was...unexpected!\n",
      "Processed: LOVE the filmmmm!!!!! I see it on VHS and the end be... unexpected!\n",
      "\n",
      "=== Sample 6 ===\n",
      "Original:  He's very happy because he's found what he's been looking for. I've told him that he'll succeed if he's ready. I'm sure they'd agree if they're honest.\n",
      "Processed: He be very happy because he be find what he be be look for. I have tell he that he will succeed if he be ready. I be sure they would agree if they be honest.\n",
      "\n",
      "=== Sample 7 ===\n",
      "Original:  It is nots very          baaaaad,burt        not thatttt goad eithaer.\n",
      "Processed: It be not very bad, but not that good either.\n"
     ]
    }
   ],
   "source": [
    "# Define sample reviews for the full pipeline\n",
    "samples = [\n",
    "    \"This movie is absoltly      amazng!The charactrs were believabl(not all of theeeem).\",\n",
    "    \"Whaat??Noo...thiiiiis is baad dirction!!!\",\n",
    "    \"1234 .... ü§ñü§ñü§ñ ???\",  # nonsense\n",
    "    \"I was stunned.The performnce was stunning.\",\n",
    "    \"LOOOOOVED the filmmmm!!!!!I see it on VHS and the end was...unexpected!\",\n",
    "    \"He's very happy because he's found what he's been looking for. I've told him that he'll succeed if he's ready. I'm sure they'd agree if they're honest.\",\n",
    "    \"It is nots very          baaaaad,burt        not thatttt goad eithaer.\"\n",
    "]\n",
    "\n",
    "# Apply full preprocessing pipeline\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    result = pre.preprocess_review(sample)\n",
    "    status = \"REMOVED (nonsense)\" if result is None else f\"Processed: {result}\"\n",
    "    print(f\"\\n=== Sample {i} ===\\nOriginal:  {sample}\\n{status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
