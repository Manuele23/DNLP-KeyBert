{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to **test and demonstrate the behavior of a lightweight text preprocessing pipeline** implemented in the `Preprocessor` class. The preprocessing steps are tailored to prepare movie reviews for transformer-based keyword extraction using models like **KeyBERT** with **`all-MiniLM-L6-v2`** embeddings.\n",
    "\n",
    "The main preprocessing operations include:\n",
    "- **Typo correction** using `autocorrect`\n",
    "- **Punctuation-spacing normalization**, ensuring readability for tokenizers\n",
    "- **Nonsense/empty review filtering**, removing unusable entries\n",
    "- **Lemmatization** using **spaCy** for efficient text normalization\n",
    "\n",
    "Each step is tested with controlled input examples to verify correctness and robustness before applying the pipeline to full datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy is already installed.\n",
      "textblob is already installed.\n",
      "autocorrect is already installed.\n",
      "wordfreq is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",                         \n",
    "    \"spacy\",           \n",
    "    \"textblob\",\n",
    "    \"autocorrect\",\n",
    "    \"wordfreq\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from autocorrect import Speller\n",
    "from wordfreq import zipf_frequency # type: ignore\n",
    "\n",
    "# spaCy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy (download if not present)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Custom Preprocessor\n",
    "\n",
    "This cell imports the `Preprocessor` class from the custom `preprocessing.py` module.  \n",
    "The class encapsulates all the text cleaning operations required to prepare review texts before passing them to a Transformer-based model.  \n",
    "It provides methods for typo correction, punctuation normalization, lemmatization, and filtering of nonsensical content, and will be applied to each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 – Typo Correction\n",
    "\n",
    "This test evaluates the typo correction capabilities of the `Preprocessor` class.\n",
    "\n",
    "The input consists of sample sentences that contain common spelling mistakes, such as:\n",
    "- `\"amazng\"` instead of `\"amazing\"`\n",
    "- `\"dirction\"` instead of `\"direction\"`\n",
    "- `\"absolutly\"` instead of `\"absolutely\"`\n",
    "\n",
    "The goal is to determine whether the typo correction module can successfully recover the correct words. Accurate correction is essential for downstream tasks such as keyword extraction and semantic embedding, as many NLP models rely on semantically clean inputs.\n",
    "\n",
    "The implemented method uses a combination of heuristics and the `autocorrect` library:\n",
    "1. **Initial Check**: Each word is first checked against its **Zipf frequency** score. Words with high frequency (Zipf > 3.5) are assumed to be valid and skipped.\n",
    "\n",
    "2. **Character Repetition Handling**:\n",
    "   - If a word contains 3 or more repeated alphabetic characters (e.g., `\"loooong\"`), they are first reduced to 2 (→ `\"loong\"`), then to 1 (→ `\"long\"`), checking validity at each step.\n",
    "   - If reducing the repetition results in a valid word, that version is kept.\n",
    "   \n",
    "3. **Fallback**: If no valid form is found, the word is passed through `autocorrect` for a final correction attempt.\n",
    "\n",
    "This approach helps mitigate common issues where `autocorrect` alone may produce incorrect words (e.g., `\"baad\"` → `\"band\"` instead of `\"bad\"`). By combining frequency checks with controlled repetition reduction, the system achieves more reliable corrections without distorting valid words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Typo Correction Test ===\n",
      "\n",
      "Review 1:\n",
      "Original:  This movi was amazng!\n",
      "Corrected: This move was amazing!\n",
      "\n",
      "Review 2:\n",
      "Original:  The dirction of the film is goooood.\n",
      "Corrected: The direction of the film is good.\n",
      "\n",
      "Review 3:\n",
      "Original:  Charactrs were not believabl.\n",
      "Corrected: Characters were not believable.\n",
      "\n",
      "Review 4:\n",
      "Original:  Absolutly stunning performnce by the lead actr.\n",
      "Corrected: Absolutely stunning performance by the lead actor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Sample reviews with typos\n",
    "typo_reviews = [\n",
    "    \"This movi was amazng!\",\n",
    "    \"The dirction of the film is goooood.\",\n",
    "    \"Charactrs were not believabl.\",\n",
    "    \"Absolutly stunning performnce by the lead actr.\",\n",
    "]\n",
    "\n",
    "# Apply typo correction only\n",
    "print(\"=== Typo Correction Test ===\\n\")\n",
    "for i, review in enumerate(typo_reviews, 1):\n",
    "    corrected = pre.correct_typos(review)\n",
    "    print(f\"Review {i}:\\nOriginal:  {review}\\nCorrected: {corrected}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 – Punctuation Spacing Normalization\n",
    "\n",
    "This test focuses on evaluating the punctuation spacing normalization step implemented in the `Preprocessor` class.\n",
    "\n",
    "The goal is to ensure that a space is inserted after punctuation marks (e.g., `.`, `,`, `!`, `?`, `;`, `:`) **only if** they are directly followed by an alphanumeric character.  \n",
    "This is intended to improve sentence readability and avoid unintended word merging, which may negatively impact tokenization and embedding models.\n",
    "\n",
    "Examples of input include:\n",
    "- `\"This movie is great!Amazing direction.\"` → `\"This movie is great! Amazing direction.\"`\n",
    "- `\"Wait...what?Really?\"` → `\"Wait...what? Really?\"`\n",
    "\n",
    "Only the punctuation normalization function is applied in this test to evaluate its isolated behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 1: This movie is great!Amazing direction.\n",
      "Normalized 1: This movie is great! Amazing direction.\n",
      "\n",
      "Original 2: Wait...what?Really?\n",
      "Normalized 2: Wait... what? Really?\n",
      "\n",
      "Original 3: Incredible,unbelievable!Must watch.\n",
      "Normalized 3: Incredible, unbelievable! Must watch.\n",
      "\n",
      "Original 4: I loved it.The actors were amazing.\n",
      "Normalized 4: I loved it. The actors were amazing.\n",
      "\n",
      "Original 5: Terrible;no plot,no logic;just noise.\n",
      "Normalized 5: Terrible; no plot, no logic; just noise.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Sample reviews with punctuation issues\n",
    "sample_texts = [\n",
    "    \"This movie is great!Amazing direction.\",\n",
    "    \"Wait...what?Really?\",\n",
    "    \"Incredible,unbelievable!Must watch.\",\n",
    "    \"I loved it.The actors were amazing.\",\n",
    "    \"Terrible;no plot,no logic;just noise.\"\n",
    "]\n",
    "\n",
    "# Apply only punctuation spacing normalization\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    normalized = pre.normalize_spacing(text)\n",
    "    print(f\"Original {i}: {text}\")\n",
    "    print(f\"Normalized {i}: {normalized}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3 – Nonsense Detection\n",
    "\n",
    "This test evaluates the ability of the `Preprocessor` class to detect and flag **nonsensical or low-quality reviews**.\n",
    "\n",
    "The implemented logic marks a review as *nonsense* if it satisfies one of the following conditions:\n",
    "- The text is **too short** (e.g., fewer than 10 characters).\n",
    "- The **ratio of alphabetic characters** to total characters is very low (e.g., dominated by symbols or numbers).\n",
    "\n",
    "This filtering step is essential to discard meaningless entries that could negatively affect downstream tasks such as embedding generation or keyword extraction.\n",
    "\n",
    "We isolate and apply only the **nonsense detection** module in this test, checking how it handles various inputs including:\n",
    "- Empty strings  \n",
    "- Symbol-only content  \n",
    "- Short but meaningful phrases  \n",
    "- Number-dominated text  \n",
    "\n",
    "Each input is labeled as either `OK` (valid) or `NONSENSE` (to be discarded).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: '!!!...??' → NONSENSE\n",
      "\n",
      "Sample 2: '1234567890' → NONSENSE\n",
      "\n",
      "Sample 3: 'Ok' → NONSENSE\n",
      "\n",
      "Sample 4: 'This is fine.' → OK\n",
      "\n",
      "Sample 5: '....' → NONSENSE\n",
      "\n",
      "Sample 6: '!!!!????....' → NONSENSE\n",
      "\n",
      "Sample 7: 'The movie was good.' → OK\n",
      "\n",
      "Sample 8: '👍🏻👍🏻👍🏻' → NONSENSE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Preprocessor\n",
    "from preprocessing import Preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Test cases for nonsense detection\n",
    "samples = [\n",
    "    \"!!!...??\",               # Only punctuation\n",
    "    \"1234567890\",             # Only numbers\n",
    "    \"Ok\",                     # Too short\n",
    "    \"This is fine.\",          # Valid sentence\n",
    "    \"....\",                   # Dots only\n",
    "    \"!!!!????....\",           # Random punctuation\n",
    "    \"The movie was good.\",    # Proper review\n",
    "    \"👍🏻👍🏻👍🏻\"                   # Emoticons only\n",
    "]\n",
    "\n",
    "# Apply nonsense detection logic\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    result = pre.is_nonsense(sample)\n",
    "    status = \"NONSENSE\" if result else \"OK\"\n",
    "    print(f\"Sample {i}: '{sample}' → {status}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4 – Lemmatization\n",
    "\n",
    "In this test, we evaluate the **lemmatization** capability of the `Preprocessor` class, implemented using `spaCy`.\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form (lemma), which helps normalize textual data. For instance:\n",
    "- `\"running\"` → `\"run\"`\n",
    "- `\"cars\"` → `\"car\"`\n",
    "- `\"was\"` → `\"be\"`\n",
    "\n",
    "This normalization is critical for downstream NLP tasks such as:\n",
    "- Keyword extraction\n",
    "- Embedding generation\n",
    "- Clustering or classification\n",
    "\n",
    "In this test, we apply **only the lemmatization step**, using a variety of phrases containing inflected forms of verbs and nouns, and inspect whether the transformations are performed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lemmatization Test ===\n",
      "Sample 1:\n",
      "Original    → The cats are running in the gardens.\n",
      "Lemmatized  → The cat be run in the garden.\n",
      "\n",
      "Sample 2:\n",
      "Original    → She was eating apples.\n",
      "Lemmatized  → She be eat apple.\n",
      "\n",
      "Sample 3:\n",
      "Original    → They have been thinking about it.\n",
      "Lemmatized  → They have be think about it.\n",
      "\n",
      "Sample 4:\n",
      "Original    → He walks, talks, and smiles.\n",
      "Lemmatized  → He walk, talk, and smile.\n",
      "\n",
      "Sample 5:\n",
      "Original    → Children played with toys yesterday.\n",
      "Lemmatized  → Child play with toy yesterday.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences with inflected forms\n",
    "lemmatization_samples = [\n",
    "    \"The cats are running in the gardens.\",\n",
    "    \"She was eating apples.\",\n",
    "    \"They have been thinking about it.\",\n",
    "    \"He walks, talks, and smiles.\",\n",
    "    \"Children played with toys yesterday.\"\n",
    "]\n",
    "\n",
    "# Apply lemmatization\n",
    "print(\"=== Lemmatization Test ===\")\n",
    "for i, text in enumerate(lemmatization_samples, 1):\n",
    "    result = pre.lemmatize_text(text)\n",
    "    print(f\"Sample {i}:\\nOriginal    → {text}\\nLemmatized  → {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5 – Full Preprocessing Pipeline\n",
    "\n",
    "This test evaluates the **entire preprocessing pipeline** implemented in the `Preprocessor` class.  \n",
    "The pipeline includes all previously tested steps, executed in sequence:\n",
    "\n",
    "1. **Typo Correction** → Fixes common spelling mistakes using `autocorrect`.\n",
    "2. **Punctuation Spacing Normalization** → Ensures correct spacing after punctuation marks, but only when followed by a letter (e.g., `\"Hello.This\"` → `\"Hello. This\"`).\n",
    "3. **Nonsense Detection** → Removes reviews that are too short or composed mostly of symbols and digits.\n",
    "4. **Lemmatization** → Converts words to their base form using `spaCy` while preserving punctuation formatting and casing.\n",
    "\n",
    "We apply this pipeline to a variety of noisy reviews to verify its effectiveness on:\n",
    "- Misspelled words\n",
    "- Incorrect punctuation\n",
    "- Empty or meaningless reviews\n",
    "- Complex but valid inputs\n",
    "\n",
    "This final test validates the correctness and stability of the preprocessing logic before applying it at scale to our review datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 1 ===\n",
      "Original:  This movie is absoltly amazng!The charactrs were believabl.\n",
      "Processed: This movie be absolutely amazing! The character be believable.\n",
      "\n",
      "=== Sample 2 ===\n",
      "Original:  Whaat??Noo...thiiiiis is baad dirction!!!\n",
      "Processed: What?? No... this be bad direction!!!\n",
      "\n",
      "=== Sample 3 ===\n",
      "Original:  1234 .... 🤖🤖🤖 ???\n",
      "REMOVED (nonsense)\n",
      "\n",
      "=== Sample 4 ===\n",
      "Original:  I was stunned.The performnce was stunning.\n",
      "Processed: I be stun. The performance be stunning.\n",
      "\n",
      "=== Sample 5 ===\n",
      "Original:  LOOOOOVED the filmmmm!!!!!The end was...unexpected!\n",
      "Processed: Love the film!!!!! The end be... unexpected!\n",
      "\n",
      "=== Sample 6 ===\n",
      "Original:  bad.\n",
      "REMOVED (nonsense)\n",
      "\n",
      "=== Sample 7 ===\n",
      "Original:  It is not very bad,but not that good either.\n",
      "Processed: It be not very bad, but not that good either.\n"
     ]
    }
   ],
   "source": [
    "# Define sample reviews for the full pipeline\n",
    "samples = [\n",
    "    \"This movie is absoltly amazng!The charactrs were believabl.\",\n",
    "    \"Whaat??Noo...thiiiiis is baad dirction!!!\",\n",
    "    \"1234 .... 🤖🤖🤖 ???\",  # nonsense\n",
    "    \"I was stunned.The performnce was stunning.\",\n",
    "    \"LOOOOOVED the filmmmm!!!!!The end was...unexpected!\",\n",
    "    \"bad.\",  # likely to be nonsense\n",
    "    \"It is not very bad,but not that good either.\"\n",
    "]\n",
    "\n",
    "# Apply full preprocessing pipeline\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    result = pre.preprocess_review(sample)\n",
    "    status = \"REMOVED (nonsense)\" if result is None else f\"Processed: {result}\"\n",
    "    print(f\"\\n=== Sample {i} ===\\nOriginal:  {sample}\\n{status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
