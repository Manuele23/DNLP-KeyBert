{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import The Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "spacy is already installed.\n",
      "nltk is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\", \"spacy\", \"nltk\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Dataset: `other_reviews_df.pkl`\n",
    "\n",
    "In this section, we load and explore the `other_reviews_df.pkl` dataset.\n",
    "The goal is to understand the structure of the DataFrame, particularly identifying the column that contains the review text, in order to proceed with the preprocessing required for keyword extraction using KeyBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['Review_ID', 'Movie_ID', 'Movie_Title', 'Rating', 'Review_Date', 'Review_Title', 'Review_Text', 'Helpful_Votes', 'Total_Votes']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_ID</th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Movie_Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Helpful_Votes</th>\n",
       "      <th>Total_Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9637661</td>\n",
       "      <td>tt6751668</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23 February 2024</td>\n",
       "      <td>Solid Film Craftsmanship, Trash Story</td>\n",
       "      <td>I'm genuinely baffled this film won not only b...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5510542</td>\n",
       "      <td>tt6751668</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26 February 2020</td>\n",
       "      <td>MASTERPIECE</td>\n",
       "      <td>Just watch it. It has everything; entertainmen...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5182892</td>\n",
       "      <td>tt6751668</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12 October 2019</td>\n",
       "      <td>First Hit: I really enjoyed this story as it d...</td>\n",
       "      <td>First Hit: I really enjoyed this story as it d...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5499682</td>\n",
       "      <td>tt6751668</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21 February 2020</td>\n",
       "      <td>If you love cliché stories this movie is not f...</td>\n",
       "      <td>I was not expecting that much of this movie. N...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6094155</td>\n",
       "      <td>tt6751668</td>\n",
       "      <td>Parasite</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14 September 2020</td>\n",
       "      <td>Amazing.</td>\n",
       "      <td>Good acting, cinematography, twists and screen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Review_ID   Movie_ID Movie_Title  Rating        Review_Date  \\\n",
       "0   9637661  tt6751668    Parasite     5.0   23 February 2024   \n",
       "1   5510542  tt6751668    Parasite    10.0   26 February 2020   \n",
       "2   5182892  tt6751668    Parasite    10.0    12 October 2019   \n",
       "3   5499682  tt6751668    Parasite     9.0   21 February 2020   \n",
       "4   6094155  tt6751668    Parasite     8.0  14 September 2020   \n",
       "\n",
       "                                        Review_Title  \\\n",
       "0              Solid Film Craftsmanship, Trash Story   \n",
       "1                                        MASTERPIECE   \n",
       "2  First Hit: I really enjoyed this story as it d...   \n",
       "3  If you love cliché stories this movie is not f...   \n",
       "4                                           Amazing.   \n",
       "\n",
       "                                         Review_Text  Helpful_Votes  \\\n",
       "0  I'm genuinely baffled this film won not only b...            3.0   \n",
       "1  Just watch it. It has everything; entertainmen...            3.0   \n",
       "2  First Hit: I really enjoyed this story as it d...           24.0   \n",
       "3  I was not expecting that much of this movie. N...            2.0   \n",
       "4  Good acting, cinematography, twists and screen...            0.0   \n",
       "\n",
       "   Total_Votes  \n",
       "0          8.0  \n",
       "1          5.0  \n",
       "2         40.0  \n",
       "3          5.0  \n",
       "4          0.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_pickle('../Dataset/others_reviews_df.pkl')\n",
    "\n",
    "# Display available columns and the first rows\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of `Review_Text` for KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and Using `en_core_web_sm`\n",
    "\n",
    "`en_core_web_sm` is a lightweight English language model provided by **spaCy**. It includes essential NLP features such as:\n",
    "- **Tokenization**: Splits text into individual words.\n",
    "- **Part-of-Speech (POS) Tagging**: Assigns grammatical categories to words.\n",
    "- **Lemmatization**: Converts words to their base forms.\n",
    "- **Named Entity Recognition (NER)**: Identifies entities like names, dates, and locations.\n",
    "\n",
    "In our case, we use `en_core_web_sm` specifically for **lemmatization**, which helps standardize words by reducing them to their root form. This improves the quality of keyword extraction with **KeyBERT**, as it avoids redundant variations of the same word.\n",
    "\n",
    "Before using it, we need to install the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model 'en_core_web_sm' is already installed.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Check if en_core_web_sm is already installed\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model 'en_core_web_sm' is already installed.\")\n",
    "# Install the model if it's not already installed\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    print(\"Model 'en_core_web_sm' installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the reviews \n",
    "\n",
    "To prepare the review text for keyword extraction with KeyBERT, we apply several preprocessing steps:\n",
    "- Convert all text to **lowercase**.\n",
    "- Remove **punctuation** and **special characters**.\n",
    "- Remove **common stopwords** to focus on meaningful words.\n",
    "- Apply **lemmatization** or **stemming** to standardize word forms.\n",
    "- (**Optional**) Filter out very short reviews that may not provide useful keywords.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "Both Lemmization and Stemming reduce words to their base forms, but with key differences:\n",
    "- Lemmatization uses a linguistic dictionary to find the base form (lemma) of a word. It is more accurate but slower.\n",
    "    - Example: “running” → “run”, “better” → “good”.\n",
    "- Stemming removes suffixes without considering the meaning, sometimes producing incorrect word forms. It is faster but less precise.\n",
    "    - Example: “running” → “runn”, “better” → “better”.\n",
    "\n",
    "For KeyBERT, lemmatization is preferable as it preserves readable and correct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manuelemustari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load NLP tools\n",
    "nlp = spacy.load(\"en_core_web_sm\")                      # Load spaCy's English tokenizer to lemmatize text\n",
    "nltk.download('stopwords')                              # Download NLTK's stopwords to remove them from text\n",
    "stop_words = set(stopwords.words('english'))            # Get the list of stopwords in English\n",
    "stemmer = PorterStemmer()                               # Initialize Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'll\", 'she', 'all', 'against', 'while', 'both', 'each', 'o', 'yourself', 'can', 'our', 'very', 'those', 'will', 'shan', 'some', 'ain', 'below', 'doesn', 't', \"it'll\", 'how', 'or', 'which', 'are', 'ma', 'mightn', \"it'd\", 'we', \"we're\", 'during', 'themselves', 'he', 'ours', \"needn't\", \"haven't\", \"she's\", 'isn', 'am', 'aren', 'yourselves', 'myself', \"hasn't\", 'him', \"wouldn't\", 'through', 'hers', 'being', 'in', 'didn', 'don', \"won't\", 'theirs', 'then', \"aren't\", 'haven', \"she'll\", \"you've\", 'doing', 'needn', 'been', \"she'd\", 'won', 'by', 'should', 's', 'as', 'again', 'down', \"we've\", 'mustn', 'so', \"mightn't\", 'there', 'hadn', \"weren't\", 'because', 'at', 'their', 'who', 'with', 'few', \"they're\", 'shouldn', 'where', 'were', 'an', 'having', 'any', \"he's\", 'out', 'nor', 'other', 'here', 'of', 'does', 'hasn', 'before', \"isn't\", \"mustn't\", 'once', 'only', 'such', 'up', \"wasn't\", 'further', 'if', 'above', 'll', 'more', 'my', \"hadn't\", \"we'd\", 'a', 'wasn', 'you', 'than', \"i've\", 'most', 'not', \"i'm\", 'after', 'weren', 'couldn', \"they've\", 'yours', 'had', 'herself', \"couldn't\", 'into', \"don't\", \"he'll\", \"shan't\", \"shouldn't\", \"should've\", \"i'd\", 'be', \"i'll\", 'until', 'to', 'did', 'whom', 're', 'them', 'has', 'me', 'the', 'his', 'and', \"they'd\", \"they'll\", 'too', 'just', 'from', 'was', 'on', 'is', 'why', 'her', 'off', 'y', \"it's\", 'i', 'do', 'itself', 'm', 'himself', 'no', 've', \"doesn't\", 'when', 'it', 'about', \"you're\", \"didn't\", \"we'll\", 'under', \"you'd\", 'your', 'that', 'have', 'now', 'same', 'what', 'but', 'its', 'own', 'd', 'they', 'ourselves', 'this', 'for', \"that'll\", 'these', 'over', 'wouldn', \"he'd\", 'between'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text Function\n",
    "def clean_text(text):\n",
    "    \"\"\" Convert text to lowercase and remove special characters \"\"\"\n",
    "    if not isinstance(text, str):  # Handle non-string values\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and special characters\n",
    "    return text\n",
    "\n",
    "# Lemmization Function\n",
    "def lemmatize_text(text):\n",
    "    \"\"\" Apply lemmatization using spaCy \"\"\"\n",
    "    # Tokenize the text\n",
    "    doc = nlp(text)    \n",
    "\n",
    "    # Lemmatize each token and join             \n",
    "    return \" \".join([token.lemma_ for token in doc if token.text not in stop_words])\n",
    "\n",
    "# Stemming Function\n",
    "def stem_text(text):\n",
    "    \"\"\" Apply stemming using Porter Stemmer \"\"\"\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    # Stem each word and join\n",
    "    return \" \".join([stemmer.stem(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Preprocess Reviews Function\n",
    "def preprocess_text(text, method=\"lemma\"):\n",
    "    \"\"\"\n",
    "    Preprocess text by cleaning and applying either lemmatization or stemming.\n",
    "    - 'lemma' applies lemmatization using spaCy.\n",
    "    - 'stem' applies stemming using NLTK.\n",
    "    \"\"\"\n",
    "    cleaned = clean_text(text)\n",
    "\n",
    "    if method == \"lemma\":\n",
    "        return lemmatize_text(cleaned)\n",
    "    elif method == \"stem\":\n",
    "        return stem_text(cleaned)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'lemma' or 'stem'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15132/15132 [12:19<00:00, 20.47it/s]\n",
      "100%|██████████| 15132/15132 [01:10<00:00, 213.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Processed_Review_Title</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Processed_Review_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solid Film Craftsmanship, Trash Story</td>\n",
       "      <td>solid film craftsmanship trash story</td>\n",
       "      <td>I'm genuinely baffled this film won not only b...</td>\n",
       "      <td>genuinely baffle film good foreign film good d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MASTERPIECE</td>\n",
       "      <td>masterpiece</td>\n",
       "      <td>Just watch it. It has everything; entertainmen...</td>\n",
       "      <td>watch everything entertainment comedy thrill h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First Hit: I really enjoyed this story as it d...</td>\n",
       "      <td>first hit really enjoy story dive hilarious ab...</td>\n",
       "      <td>First Hit: I really enjoyed this story as it d...</td>\n",
       "      <td>first hit really enjoy story dive hilarious ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you love cliché stories this movie is not f...</td>\n",
       "      <td>love clich story movie</td>\n",
       "      <td>I was not expecting that much of this movie. N...</td>\n",
       "      <td>expect much movie normally film nominate oscar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazing.</td>\n",
       "      <td>amazing</td>\n",
       "      <td>Good acting, cinematography, twists and screen...</td>\n",
       "      <td>good act cinematography twist screenplay side ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Review_Title  \\\n",
       "0              Solid Film Craftsmanship, Trash Story   \n",
       "1                                        MASTERPIECE   \n",
       "2  First Hit: I really enjoyed this story as it d...   \n",
       "3  If you love cliché stories this movie is not f...   \n",
       "4                                           Amazing.   \n",
       "\n",
       "                              Processed_Review_Title  \\\n",
       "0               solid film craftsmanship trash story   \n",
       "1                                        masterpiece   \n",
       "2  first hit really enjoy story dive hilarious ab...   \n",
       "3                             love clich story movie   \n",
       "4                                            amazing   \n",
       "\n",
       "                                         Review_Text  \\\n",
       "0  I'm genuinely baffled this film won not only b...   \n",
       "1  Just watch it. It has everything; entertainmen...   \n",
       "2  First Hit: I really enjoyed this story as it d...   \n",
       "3  I was not expecting that much of this movie. N...   \n",
       "4  Good acting, cinematography, twists and screen...   \n",
       "\n",
       "                               Processed_Review_Text  \n",
       "0  genuinely baffle film good foreign film good d...  \n",
       "1  watch everything entertainment comedy thrill h...  \n",
       "2  first hit really enjoy story dive hilarious ab...  \n",
       "3  expect much movie normally film nominate oscar...  \n",
       "4  good act cinematography twist screenplay side ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Apply lemmatization to review body text\n",
    "df_processed['Processed_Review_Text'] = df_processed['Review_Text'].progress_apply(lambda x: preprocess_text(x, method=\"lemma\"))\n",
    "\n",
    "# Apply stemming to review titles\n",
    "df_processed['Processed_Review_Title'] = df_processed['Review_Title'].progress_apply(lambda x: preprocess_text(x, method=\"lemma\"))\n",
    "\n",
    "# Display a preview of the result\n",
    "df_processed[['Review_Title', 'Processed_Review_Title', 'Review_Text', 'Processed_Review_Text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the First Preprocessed Dataset\n",
    "\n",
    "After completing the text preprocessing steps, we save the resulting DataFrame as a `.pkl` file to ensure consistency with the original dataset format.  \n",
    "The output filename uses the same base name as the original file, prefixed with `preprocessed_`, allowing us to distinguish it while preserving traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved to: ../Dataset/preprocessed_others_reviews_df.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the processed DataFrame to a new pickle file\n",
    "output_path = '../Dataset/preprocessed_others_reviews_df.pkl'\n",
    "df_processed.to_pickle(output_path)\n",
    "print(f\"Preprocessed dataset saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Dataset; `sw_reviews.pkl`\n",
    "\n",
    "We now apply the same preprocessing pipeline to the second dataset, `sw_reviews.pkl`.  \n",
    "The objective is to clean and standardize both the review text and titles using the previously defined functions:\n",
    "- Lemmatization is applied to the main review text.\n",
    "- Stemming is applied to the review titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/sw_reviews.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the second dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_sw \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Dataset/sw_reviews.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Enable tqdm for pandas apply\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/sw_reviews.pkl'"
     ]
    }
   ],
   "source": [
    "# Load the second dataset\n",
    "df_sw = pd.read_pickle('../Dataset/sw_reviews_df.pkl')\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create a processed copy\n",
    "df_sw_processed = df_sw.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "df_sw_processed['Processed_Review_Text'] = df_sw_processed['Review_Text'].progress_apply(lambda x: preprocess_text(x, method=\"lemma\"))\n",
    "df_sw_processed['Processed_Review_Title'] = df_sw_processed['Review_Title'].progress_apply(lambda x: preprocess_text(x, method=\"lemma\"))\n",
    "\n",
    "# Display a preview of the result\n",
    "df_sw_processed[['Review_Title', 'Processed_Review_Title', 'Review_Text', 'Processed_Review_Text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Second Preprocessed Dataset\n",
    "\n",
    "The preprocessed version of `sw_reviews.pkl` is saved in `.pkl` format using the same naming convention as before.  \n",
    "The file is named `preprocessed_sw_reviews.pkl` to ensure clarity and consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to a new pickle file\n",
    "output_path = '../Dataset/preprocessed_sw_reviews_df.pkl'\n",
    "df_sw_processed.to_pickle(output_path)\n",
    "print(f\"Preprocessed dataset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
