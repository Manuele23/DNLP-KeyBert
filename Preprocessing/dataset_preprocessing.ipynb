{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c65feee",
   "metadata": {},
   "source": [
    "# Preprocessing of Review Texts for Transformer-Based Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e72c8",
   "metadata": {},
   "source": [
    "This section focuses on applying essential **text preprocessing** to a collection of movie review datasets located in the `Review_By_Movie` folder. Each file in the folder is a `.pkl` dataset containing raw user reviews for a specific movie, including:\n",
    "\n",
    "- The 9 *Star Wars* episodes: `SW_Episode1.pkl` to `SW_Episode9.pkl`  \n",
    "- Other films: `HarryPotter.pkl`, `IndianaJones.pkl`, `LaLaLand.pkl`, `Parasite.pkl`, `GoodBadUgly.pkl`, `Oppenheimer.pkl`  \n",
    "\n",
    "For each dataset, a new column called `Preprocessed_Review` will be created, containing the **cleaned and normalized version** of the original review text.\n",
    "\n",
    "Since these reviews will be later processed by a Transformer-based keyword extraction model (specifically **KeyBERT** with the `all-MiniLM-L6-v2` embedding backend), the preprocessing is deliberately **minimal but targeted**.\n",
    "\n",
    "Transformers internally handle many aspects like tokenization, lowercasing, and truncation. Therefore, the goal of this preprocessing is not to reshape the text dramatically, but to improve its quality and consistency, especially for keyword selection.\n",
    "\n",
    "The following operations are applied:\n",
    "\n",
    "- **Typo correction**  \n",
    "  Typo correction is handled using the `autocorrect` library, but with an **enhanced logic** to deal with common limitations of naive spell-checking. In particular, special care is taken with **letter repetitions**, which often occur in user-generated reviews (e.g., `\"loooong\"`, `\"baaad\"`, `\"amazzing\"`).  \n",
    "\n",
    "  The logic works as follows:\n",
    "  1. **Words already recognized as valid English** (based on frequency from the `wordfreq` lexicon) are left unchanged.\n",
    "\n",
    "  2. **Whitespace normalization** is also applied, collapsing multiple consecutive spaces into one (e.g., `\"What     a mess\"` → `\"What a mess\"`).\n",
    "\n",
    "  3. Words that begin with a **capital letter and are not at the start of the sentence** are assumed to be **proper nouns** and are not altered, to preserve named entities like `\"Harry\"` or `\"Oppenheimer\"`.\n",
    "\n",
    "  4. If a word has **three or more repeated letters** (e.g., `\"stunnnning\"`), these are first reduced to **two** repeated characters (`\"stunning\"`), and if still invalid, to **one** (`\"stuning\"`), checking validity at each step.\n",
    "\n",
    "  5. If a word has **two repeated letters** (e.g., `\"baad\"`), it is tentatively reduced to one (`\"bad\"`) **only if** the resulting word is frequent enough to be valid. If not, the original word is passed to `autocorrect` as a fallback.\n",
    "\n",
    "  This layered strategy helps avoid common mistakes, such as:\n",
    "  - `\"good\"` → being incorrectly reduced to `\"god\"`  \n",
    "  - `\"baad\"` → being incorrectly corrected to `\"band\"` instead of `\"bad\"`  \n",
    "  - `\"stunning\"` written as `\"stunnnning\"` → correctly restored by reducing repetitions first\n",
    "\n",
    "  The result is a **more accurate and robust correction process**, which avoids over-correcting valid words while still handling noisy user input effectively.\n",
    "\n",
    "- **Punctuation spacing normalization**  \n",
    "  Ensures that punctuation marks (e.g., `.`, `!`, `?`) are followed by a space **only if** the next character is a word character. This avoids token fusion issues (e.g., `hello.great` → `hello. great`), while preserving expressive punctuation (e.g., `!!!`) or numbers expressing values (e.g., `$300,000`)\n",
    "\n",
    "- **Nonsense and empty review removal**  \n",
    "  Short or unintelligible reviews (e.g., only numbers, symbols, or emojis) are discarded using a character ratio heuristic, to avoid noise in downstream tasks.\n",
    "\n",
    "- **Lemmatization**  \n",
    "  Words are reduced to their base (dictionary) form using **spaCy**, improving the semantic clarity and consistency of text (e.g., `running` → `run`, `actors` → `actor`).\n",
    "\n",
    "This lightweight pipeline ensures that the text is **clean, meaningful, and semantically compact**, while still preserving the structure expected by transformer-based models.\n",
    "\n",
    "> **Note**: Stop word removal is **not performed here**, as it is managed internally by the **KeyBERT extensions**, which apply stop word filtering during keyword scoring. This keeps the preprocessing stage model-agnostic and allows for richer semantic extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234e166",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603feec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy is already installed.\n",
      "autocorrect is already installed.\n",
      "wordfreq is already installed.\n",
      "tqdm is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",                         \n",
    "    \"spacy\",           \n",
    "    \"autocorrect\",\n",
    "    \"wordfreq\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from wordfreq import zipf_frequency # type: ignore\n",
    "\n",
    "# spaCy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy (download if not present)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os library for file operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5398d12",
   "metadata": {},
   "source": [
    "### Importing the Custom Preprocessor\n",
    "\n",
    "This cell imports the `Preprocessor` class from the custom `preprocessing.py` module.  \n",
    "The class encapsulates all the text cleaning operations required to prepare review texts before passing them to a Transformer-based model.  \n",
    "It provides methods for typo correction, punctuation normalization, lemmatization, and filtering of nonsensical content, and will be applied to each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104a6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor  # Custom preprocessor module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76c310",
   "metadata": {},
   "source": [
    "## Batch Preprocessing of Movie Review Datasets\n",
    "\n",
    "In this step, we apply the custom `Preprocessor` class to **all movie review datasets** stored in the `Review_By_Movie` folder.  \n",
    "Each `.pkl` file corresponds to a different movie and contains a column named `Review` with raw user reviews.\n",
    "\n",
    "For each dataset, the following operations are performed:\n",
    "\n",
    "- The reviews are preprocessed using the `Preprocessor.preprocess_review()` pipeline, which includes:\n",
    "  - **Typo correction**\n",
    "  - **Punctuation spacing normalization**\n",
    "  - **Nonsense or empty review filtering**\n",
    "  - **Lemmatization** (via spaCy)\n",
    "- The cleaned review is stored in a new column called `Preprocessed_Review`.\n",
    "- Any rows where preprocessing failed (e.g., meaningless reviews) are removed.\n",
    "- The updated dataset is **saved back to disk, overwriting the original file**.\n",
    "- A summary is printed showing the number of reviews before and after preprocessing.\n",
    "\n",
    "This batch step ensures that **all datasets are ready for transformer-based keyword extraction** using models like KeyBERT, with improved quality and consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcd9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GoodBadUgly.pkl: 100%|██████████| 1430/1430 [09:04<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoodBadUgly.pkl: 1430 → 1429 valid reviews after preprocessing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Parasite.pkl: 100%|██████████| 3702/3702 [10:41<00:00,  5.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parasite.pkl: 3702 → 3702 valid reviews after preprocessing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SW_Episode2.pkl:   6%|▌         | 221/3880 [01:55<31:53,  1.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/3k/39cgj5q54l52b9t9s6kj73kr0000gn/T/ipykernel_7269/4228999202.py\", line 18, in <module>\n",
      "    df[\"Preprocessed_Review\"] = df[\"Review_Text\"].progress_apply(pre.preprocess_review)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/std.py\", line 917, in inner\n",
      "    return getattr(df, df_function)(wrapper, **kwargs)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py\", line 4917, in apply\n",
      "    return SeriesApply(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py\", line 1427, in apply\n",
      "    return self.apply_standard()\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py\", line 1507, in apply_standard\n",
      "    mapped = obj._map_values(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/pandas/core/base.py\", line 921, in _map_values\n",
      "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
      "    return lib.map_infer(values, mapper, convert=convert)\n",
      "  File \"lib.pyx\", line 2972, in pandas._libs.lib.map_infer\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/std.py\", line 912, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/manuelemustari/Desktop/Università/Politecnico di Torino/2° year/1° period/Deep Natural Language Processing/DNLP-KeyBert/Preprocessing/preprocessing.py\", line 59, in preprocess_review\n",
      "  File \"/Users/manuelemustari/Desktop/Università/Politecnico di Torino/2° year/1° period/Deep Natural Language Processing/DNLP-KeyBert/Preprocessing/preprocessing.py\", line 27, in correct_typos\n",
      "    - Try reducing to 2 repeated characters first.\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 128, in autocorrect_sentence\n",
      "    return re.sub(\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py\", line 210, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 130, in <lambda>\n",
      "    lambda match: self.autocorrect_word(match.group(0)),\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 119, in autocorrect_word\n",
      "    candidates += self.get_candidates(decapitalized)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 104, in get_candidates\n",
      "    or self.existing(w.double_typos())\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 94, in existing\n",
      "    return {word for word in words if word in self.nlp_data}\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py\", line 94, in <setcomp>\n",
      "    return {word for word in words if word in self.nlp_data}\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/autocorrect/typos.py\", line 62, in _inserts\n",
      "    yield \"\".join((a, c, b))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Preprocessing.preprocessing import Preprocessor  # Assicurati che il path e nome siano corretti\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# Initialize preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Process each .pkl file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_pickle(file_path)\n",
    "        original_count = len(df)\n",
    "\n",
    "        # Apply preprocessing\n",
    "        tqdm.pandas(desc=f\"Processing {filename}\")\n",
    "        df[\"Preprocessed_Review\"] = df[\"Review_Text\"].progress_apply(pre.preprocess_review)\n",
    "\n",
    "        # Identify and print removed rows\n",
    "        removed = df[df[\"Preprocessed_Review\"].isna()]\n",
    "        if not removed.empty:\n",
    "            print(f\"\\n--- Removed reviews from {filename} ---\")\n",
    "            for idx, row in removed.iterrows():\n",
    "                print(f\"[Review ID: {idx}] {row['Review_Text']}\\n\")\n",
    "\n",
    "        # Drop rows where preprocessing returned None\n",
    "        df.dropna(subset=[\"Preprocessed_Review\"], inplace=True)\n",
    "        new_count = len(df)\n",
    "\n",
    "        # Save back to disk (overwrite)\n",
    "        df.to_pickle(file_path)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"{filename}: {original_count} → {new_count} valid reviews after preprocessing\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
