{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c65feee",
   "metadata": {},
   "source": [
    "# Preprocessing of Review Texts for Transformer-Based Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e72c8",
   "metadata": {},
   "source": [
    "In this section, the goal is to perform basic but essential preprocessing on a collection of movie review datasets stored in the `Review_By_Movie` folder.  \n",
    "The folder contains individual `.pkl` files, each corresponding to a specific movie. The datasets included are:\n",
    "\n",
    "- SW_Episode1.pkl \n",
    "- SW_Episode2.pkl  \n",
    "- SW_Episode3.pkl  \n",
    "- SW_Episode4.pkl  \n",
    "- SW_Episode5.pkl  \n",
    "- SW_Episode6.pkl  \n",
    "- SW_Episode7.pkl  \n",
    "- SW_Episode8.pkl  \n",
    "- SW_Episode9.pkl  \n",
    "- HarryPotter.pkl  \n",
    "- IndianaJones.pkl  \n",
    "- LaLaLand.pkl\n",
    "- Parasite.pkl  \n",
    "- GoodBadUgly.pkl  \n",
    "- Oppenheimer.pkl\n",
    "\n",
    "The preprocessing step will add a new column named `Preprocessed_Review` to each dataset, containing the cleaned version of the review text.  \n",
    "Since the processed reviews will later be passed to a Transformer model (specifically `KeyBERT` using `all-MiniLM-L6-v2` as the embedding model), only minimal preprocessing is needed.\n",
    "\n",
    "Transformers are generally robust to text noise and handle tokenization, lowercasing, and truncation internally. However, to improve the quality of the extracted keywords, the following custom preprocessing steps will be applied:\n",
    "\n",
    "- **Typo correction** for common misspellings.  \n",
    "- **Punctuation spacing normalization**: ensure a space follows punctuation marks **only** if followed by a word character, and **not** by other punctuation (e.g., `hello.could` â†’ `hello. could`, but `!!!` is left unchanged).  \n",
    "- **Removal of nonsensical or empty reviews**, such as strings with only symbols, numbers, or unintelligible text.  \n",
    "- **Lemmatization**, to reduce inflected words to their base form, improving the consistency of the text fed to the model.\n",
    "\n",
    "This light preprocessing aims to clean the text just enough to improve embedding quality, without interfering with the structure expected by the transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234e166",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603feec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "tqdm is already installed.\n",
      "nltk is already installed.\n",
      "spacy is already installed.\n",
      "textblob is already installed.\n",
      "autocorrect is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",          \n",
    "    \"tqdm\",            \n",
    "    \"nltk\",            \n",
    "    \"spacy\",           \n",
    "    \"textblob\",\n",
    "    \"autocorrect\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fd368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core Libraries ===\n",
    "import re                      # Regular expressions for text cleaning\n",
    "\n",
    "from autocorrect import Speller  # For spell checking\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5398d12",
   "metadata": {},
   "source": [
    "### Importing the Custom Preprocessor\n",
    "\n",
    "This cell imports the `Preprocessor` class from the custom `preprocessing.py` module.  \n",
    "The class encapsulates all the text cleaning operations required to prepare review texts before passing them to a Transformer-based model.  \n",
    "It provides methods for typo correction, punctuation normalization, lemmatization, and filtering of nonsensical content, and will be applied to each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104a6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor  # Custom preprocessor module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b093b21",
   "metadata": {},
   "source": [
    "## Testing the Preprocessor on a Sample Review\n",
    "\n",
    "In this cell, the `Preprocessor` class is instantiated and applied to a sample movie review.  \n",
    "This test allows us to verify that the preprocessing pipeline performs as expected, including punctuation normalization, typo correction, nonsense filtering, and lemmatization.  \n",
    "The output will help ensure that the resulting cleaned text is appropriate for Transformer-based keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a4ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'This movi was amaaazing!!!The direction is...well,not good.I think.'\n",
      "Processed: This move was amaaazing!!! The direction is... well,not good. I think.\n",
      "---\n",
      "Original: '....!!!???'\n",
      "Processed: None\n",
      "---\n",
      "Original: '!!!'\n",
      "Processed: None\n",
      "---\n",
      "Original: 'goood!!'\n",
      "Processed: None\n",
      "---\n",
      "Original: '           '\n",
      "Processed: None\n",
      "---\n",
      "Original: None\n",
      "Processed: None\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "pre = Preprocessor()\n",
    "\n",
    "samples = [\n",
    "    \"This movi was amaaazing!!!The direction is...well,not good.I think.\",\n",
    "    \"....!!!???\",\n",
    "    \"!!!\",\n",
    "    \"goood!!\",\n",
    "    \"           \",\n",
    "    None\n",
    "]\n",
    "\n",
    "for s in samples:\n",
    "    print(\"Original:\", repr(s))\n",
    "    print(\"Processed:\", pre.preprocess_review(s))\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcd9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
