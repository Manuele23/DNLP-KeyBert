{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c65feee",
   "metadata": {},
   "source": [
    "# Preprocessing of Review Texts for Transformer-Based Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e72c8",
   "metadata": {},
   "source": [
    "This section focuses on applying essential **text preprocessing** to a collection of movie review datasets located in the `Review_By_Movie` folder. Each file in the folder is a `.pkl` dataset containing raw user reviews for a specific movie, including:\n",
    "\n",
    "- The 9 *Star Wars* episodes: `SW_Episode1.pkl` to `SW_Episode9.pkl`  \n",
    "- Other films: `HarryPotter.pkl`, `IndianaJones.pkl`, `LaLaLand.pkl`, `Parasite.pkl`, `GoodBadUgly.pkl`, `Oppenheimer.pkl`  \n",
    "\n",
    "For each dataset, a new column called `Preprocessed_Review` will be created, containing the **cleaned and normalized version** of the original review text.\n",
    "\n",
    "### Context: Transformer-based Keyword Extraction\n",
    "\n",
    "Since these reviews will be later processed by a Transformer-based keyword extraction model (specifically **KeyBERT** with the `all-MiniLM-L6-v2` embedding backend), the preprocessing is deliberately **minimal but targeted**.\n",
    "\n",
    "Transformers internally handle many aspects like tokenization, subword modeling, casing, and truncation. Therefore, the goal of this preprocessing is not to reshape the text dramatically, but to improve its quality and consistency, especially for keyword selection.\n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "\n",
    "#### 1. **Punctuation Spacing Normalization**\n",
    "Ensures punctuation marks (e.g., `.`, `!`, `?`) are followed by a space **only if** the next character is a word character.\n",
    "\n",
    "- `\"hello.great\"` → `\"hello. great\"`\n",
    "- Preserves numbers: `$300,000.00` remains unchanged.\n",
    "\n",
    "#### 2. **Typo Correction**\n",
    "Typo correction is handled using the `autocorrect` library, with an enhanced strategy to deal with common limitations of naive spell-checking.  \n",
    "Special attention is paid to **letter repetitions**, which often occur in user-generated reviews (e.g., `\"loooong\"`, `\"baaad\"`, `\"amazzing\"`).\n",
    "\n",
    "**How it works:**\n",
    "- **Valid English words** (based on frequency in the `wordfreq` lexicon) are left unchanged.\n",
    "- **Whitespace is normalized** to collapse multiple spaces.\n",
    "- **Capitalized words** not at the beginning of a sentence are assumed to be **proper nouns** and left untouched.\n",
    "- Words with **3+ repeated letters** are reduced to 2, then to 1 if necessary, checking at each step for validity.\n",
    "- If still unrecognized, the word is passed to `autocorrect` as a last resort.\n",
    "\n",
    "This avoids common pitfalls such as:\n",
    "- `\"good\"` being miscorrected to `\"god\"`\n",
    "- `\"baad\"` corrected to `\"band\"` instead of `\"bad\"`\n",
    "- `\"stunnnning\"` → `\"stunning\"` (correctly fixed)\n",
    "\n",
    "#### 3. **Nonsense and Empty Review Removal**\n",
    "Short or unintelligible reviews (e.g., only numbers, emojis, or symbols) are removed using a character ratio heuristic.  \n",
    "This helps eliminate non-informative inputs from downstream analysis.\n",
    "\n",
    "### Why **Lemmatization Is Not Applied**\n",
    "\n",
    "Although lemmatization (e.g., `running` → `run`) is common in traditional NLP pipelines, it is **intentionally omitted** here.\n",
    "\n",
    "#### BERT does not need it:\n",
    "Transformer models like BERT are pre-trained on raw, natural text and already handle semantic normalization via **subword tokenization**. Lemmatization can **break expected token patterns**.  \n",
    "For instance:\n",
    "  - `\"was\"` → `\"be\"`  \n",
    "  - `\"actors\"` → `\"actor\"`  \n",
    "These changes may reduce model performance in embedding generation.\n",
    "\n",
    "#### KeyBERT prefers natural text:\n",
    "KeyBERT uses **BERT embeddings directly**, and performs best with text that reflects how humans write. Lemmatization can distort the input and degrade semantic richness.\n",
    "\n",
    "> **Note**: Stop word removal is **not performed here**, as it is handled by the **KeyBERT extensions** during keyword scoring.  \n",
    "> This keeps the preprocessing flexible and model-agnostic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234e166",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603feec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy is already installed.\n",
      "autocorrect is already installed.\n",
      "wordfreq is already installed.\n",
      "tqdm is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",                         \n",
    "    \"spacy\",           \n",
    "    \"autocorrect\",\n",
    "    \"wordfreq\",\n",
    "    \"tqdm\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from autocorrect import Speller\n",
    "from wordfreq import zipf_frequency # type: ignore\n",
    "\n",
    "# spaCy for NLP tasks\n",
    "import spacy\n",
    "\n",
    "# Load the English language model in spaCy (download if not present)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm' model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os library for file operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5398d12",
   "metadata": {},
   "source": [
    "### Importing the Custom Preprocessor\n",
    "\n",
    "This cell imports the `Preprocessor` class from the custom `preprocessing.py` module.  \n",
    "The class encapsulates all the text cleaning operations required to prepare review texts before passing them to a Transformer-based model.  \n",
    "It provides methods for typo correction, punctuation normalization and filtering of nonsensical content, and will be applied to each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104a6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor  # Custom preprocessor module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76c310",
   "metadata": {},
   "source": [
    "## Batch Preprocessing of Movie Review Datasets\n",
    "\n",
    "In this step, we apply the custom `Preprocessor` class to all movie review datasets stored in the `Review_By_Movie` folder.  \n",
    "Each `.pkl` file corresponds to a different movie and contains a column named `Review` with raw user reviews.\n",
    "\n",
    "For each dataset, the following operations are performed:\n",
    "\n",
    "- The reviews are preprocessed using the `Preprocessor.preprocess_review()` pipeline, which includes:\n",
    "  - Typo correction with repetition reduction and intelligent spell-checking\n",
    "  - Punctuation spacing normalization to ensure clean token boundaries\n",
    "  - Nonsense or empty review filtering, based on character composition\n",
    "\n",
    "- The cleaned review is stored in a new column called `Preprocessed_Review`.\n",
    "\n",
    "- Any rows where preprocessing failed (e.g., unintelligible or empty reviews) are removed.\n",
    "\n",
    "- The updated dataset is saved back to disk, overwriting the original file.\n",
    "\n",
    "- A summary is printed showing the number of reviews before and after preprocessing. Optionally, the discarded reviews can be inspected for transparency.\n",
    "\n",
    ">Note: Lemmatization is not applied, since the downstream model (KeyBERT) is based on Transformer embeddings, which already handle word normalization internally. Lemmatization may reduce semantic richness and interfere with subword tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fcd9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing GoodBadUgly.pkl: 100%|██████████| 1430/1430 [07:50<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Removed reviews from GoodBadUgly.pkl ---\n",
      "[Review ID: 40764] Exelent ........\n",
      "...\n",
      "....\n",
      "....\n",
      ".....\n",
      "......\n",
      "......\n",
      "\n",
      "GoodBadUgly.pkl: 1430 → 1429 valid reviews after preprocessing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Parasite.pkl:   4%|▍         | 140/3702 [00:51<21:55,  2.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m     22\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed_Review\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReview_Text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_review\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Identify and print removed rows\u001b[39;00m\n\u001b[1;32m     26\u001b[0m removed \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed_Review\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misna()]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Università/Politecnico di Torino/2° year/1° period/Deep Natural Language Processing/DNLP-KeyBert/Preprocessing/preprocessing.py:165\u001b[0m, in \u001b[0;36mPreprocessor.preprocess_review\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect_typos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# Fix typos and repeated letters\u001b[39;00m\n\u001b[1;32m    166\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_spacing(text)      \u001b[38;5;66;03m# Fix punctuation spacing\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_nonsense(text):\n",
      "File \u001b[0;32m~/Desktop/Università/Politecnico di Torino/2° year/1° period/Deep Natural Language Processing/DNLP-KeyBert/Preprocessing/preprocessing.py:84\u001b[0m, in \u001b[0;36mPreprocessor.correct_typos\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     81\u001b[0m         corrected\u001b[38;5;241m.\u001b[39mappend(reduced_1 \u001b[38;5;241m+\u001b[39m space)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m# Fallback: use autocorrect\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m         corrected\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_text\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m space)\n\u001b[1;32m     86\u001b[0m     prev_token \u001b[38;5;241m=\u001b[39m token_text\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:128\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocorrect_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mword_regexes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocorrect_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:130\u001b[0m, in \u001b[0;36mSpeller.autocorrect_sentence.<locals>.<lambda>\u001b[0;34m(match)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautocorrect_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    129\u001b[0m         word_regexes[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang],\n\u001b[0;32m--> 130\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m match: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautocorrect_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    131\u001b[0m         sentence,\n\u001b[1;32m    132\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:119\u001b[0m, in \u001b[0;36mSpeller.autocorrect_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misupper():\n\u001b[1;32m    118\u001b[0m     decapitalized \u001b[38;5;241m=\u001b[39m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m+\u001b[39m word[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 119\u001b[0m     candidates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecapitalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m best_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(candidates)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misupper():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:104\u001b[0m, in \u001b[0;36mSpeller.get_candidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     99\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting([word]) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting(w\u001b[38;5;241m.\u001b[39mtypos()) \u001b[38;5;129;01mor\u001b[39;00m [word]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting([word])\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexisting(w\u001b[38;5;241m.\u001b[39mtypos())\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexisting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_typos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m [word]\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:94\u001b[0m, in \u001b[0;36mSpeller.existing\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexisting\u001b[39m(\u001b[38;5;28mself\u001b[39m, words):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/__init__.py:94\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexisting\u001b[39m(\u001b[38;5;28mself\u001b[39m, words):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"{'the', 'teh'} => {'the'}\"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_data}\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/autocorrect/typos.py:56\u001b[0m, in \u001b[0;36mWord._replaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((a, c, b[\u001b[38;5;241m1\u001b[39m:]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from preprocessing import Preprocessor\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# Initialize preprocessor\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Process each .pkl file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_pickle(file_path)\n",
    "        original_count = len(df)\n",
    "\n",
    "        # Apply preprocessing\n",
    "        tqdm.pandas(desc=f\"Processing {filename}\")\n",
    "        df[\"Preprocessed_Review\"] = df[\"Review_Text\"].progress_apply(pre.preprocess_review)\n",
    "\n",
    "        # Identify and print removed rows\n",
    "        removed = df[df[\"Preprocessed_Review\"].isna()]\n",
    "        if not removed.empty:\n",
    "            print(f\"\\n--- Removed reviews from {filename} ---\")\n",
    "            for idx, row in removed.iterrows():\n",
    "                print(f\"[Review ID: {idx}] {row['Review_Text']}\\n\")\n",
    "\n",
    "        # Drop rows where preprocessing returned None\n",
    "        df.dropna(subset=[\"Preprocessed_Review\"], inplace=True)\n",
    "        new_count = len(df)\n",
    "\n",
    "        # Save back to disk (overwrite)\n",
    "        df.to_pickle(file_path)\n",
    "\n",
    "        # Summary\n",
    "        print(f\"{filename}: {original_count} → {new_count} valid reviews after preprocessing\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0694de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPROCESSING ===\n",
      "Original: A poor South Korean family lives in a ramshackle semi-basement apartment and gets by on hustles and cons. One day the son manages to get a job tutoring the child of a wealthy family. He sees an opportunity to get his parents and sister jobs in the household too. Soon all of them of are in and life is looking much rosier. Then fate throws them a curveball.Superb. Written and directed by Bong Joon Ho who gave us the superb crime-drama Memories of Murder plus the entertaining Snowpiercer, Parasite is a great mix of comedy and drama, pathos and social commentary.It starts off in very entertaining fashion as we meet the family that are experts in cons and manipulation. It is quite funny and initially they just seem like slackers. However, after a while it is quite awe-inspiring to see the work that goes into their deceptions.As the movie progresses it becomes darker and darker. The plot takes on a few surprising twists and turns and the two families involved become symbols of a class divide. The social commentary is thought-provoking without being preachy or seeming biased. Powerful ending.Solid performances round out an excellent plot and great direction.\n",
      "Processed: A poor South Korean family lives in a ramshackle semi-basement apartment and gets by on hustle and cons. One day the son manages to get a job tutoring the child of a wealthy family. He sees an opportunity to get his parents and sister jobs in the household too. Soon all of them of are in and life is looking much roster. Then fate throws them a curveball. Superb. Written and directed by Bong Joon Ho who gave us the superb crime-drama Memories of Murder plus the entertaining Snowpiercer, Parasite is a great mix of comedy and drama, paths and social commentary. It starts off in very entertaining fashion as we meet the family that are experts in cons and manipulation. It is quite funny and initially they just seem like glaciers. However, after a while it is quite awe-inspiring to see the work that goes into their receptions. As the movie progresses it becomes darker and darker. The plot takes on a few surprising twists and turns and the two families involved become symbols of a class divide. The social commentary is thought-provoking without being preach or seeming biased. Powerful ending. Solid performances round out an excellent plot and great direction.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocessing import Preprocessor  # Assicurati che il file sia importabile\n",
    "\n",
    "# Path al file\n",
    "file_path = \"../Dataset/Reviews_By_Movie/Parasite.pkl\"\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "# Istanzia il preprocessore\n",
    "pre = Preprocessor()\n",
    "\n",
    "# Applica la pipeline alla prima recensione\n",
    "first_review = df[\"Review_Text\"].iloc[456]\n",
    "processed = pre.preprocess_review(first_review)\n",
    "\n",
    "# Stampa risultato\n",
    "print(\"\\n=== PREPROCESSING ===\")\n",
    "print(\"Original:\", first_review)\n",
    "print(\"Processed:\", processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3963a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
