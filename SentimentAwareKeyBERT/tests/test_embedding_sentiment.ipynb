{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff641bd8",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook implements the class defined in `models/keywords_sentiment.py`, extending KeyBERT to incorporate sentiment information directly into the keyword selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662213f8",
   "metadata": {},
   "source": [
    "# KeyBERT with Integrated Sentiment-aware Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1106b08",
   "metadata": {},
   "source": [
    "Unlike post-hoc reranking strategies that adjust keyword scores after semantic filtering, this approach introduces sentiment alignment **early**, during candidate selection and scoring.\n",
    "\n",
    "*The goal is to extract keywords that are not only semantically relevant, but also emotionally aligned with the overall sentiment of the document.*\n",
    "\n",
    "### Theoretical Framework\n",
    "\n",
    "The original KeyBERT pipeline is modified to consider both semantic similarity and sentiment coherence during candidate filtering and ranking.\n",
    "\n",
    "Given:\n",
    "- $\\text{sim}_{sem}$: cosine similarity between document and candidate embeddings\n",
    "- $s_{doc} \\in [0, 1]$: sentiment polarity of the full document\n",
    "- $s_{cand} \\in [0, 1]$: sentiment polarity of each candidate keyword\n",
    "\n",
    "We compute a sentiment alignment score:\n",
    "\n",
    "$$\n",
    "\\text{align}(s_{doc}, s_{cand}) = 1 - |s_{doc} - s_{cand}|\n",
    "$$\n",
    "\n",
    "This is then combined with semantic similarity to form a final score:\n",
    "\n",
    "$$\n",
    "\\text{score}_{final} = (1 - \\alpha) \\cdot \\text{sim}_{sem} + \\alpha \\cdot (2 \\cdot \\text{align} - 1)\n",
    "$$\n",
    "\n",
    "where **α ∈ [0, 1]** controls the trade-off between semantic and sentiment-based scoring.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- **Early sentiment filtering**: Sentiment alignment is used to filter low-quality candidates before ranking.\n",
    "- **Continuous scoring**: Sentiment is computed as a weighted average of class probabilities, enabling smooth comparisons.\n",
    "- **Flexible control**: The parameter `alpha` lets users adjust the influence of sentiment.\n",
    "- **Embedding-compatible**: Works with any `SentenceTransformer` model and HuggingFace sentiment classifier.\n",
    "\n",
    "By using sentiment alignment to filter and score keywords **before ranking**, this approach allows sentiment to directly influence which candidates are considered at all — not just how they are ranked.\n",
    "\n",
    "### When to Use This\n",
    "\n",
    "This strategy is most effective when:\n",
    "- The text contains **strong sentiment signals** (e.g., reviews, social media, opinions).\n",
    "- You want to **prioritize emotional consistency** from the beginning.\n",
    "- Reducing **irrelevant or sentimentally inconsistent keywords early** is important for your use case.\n",
    "\n",
    "It offers more control over the candidate pool and generally produces **more focused, sentiment-aware results**, especially in emotionally polarized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2118e",
   "metadata": {},
   "source": [
    "### Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510ac332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "torch is already installed.\n",
      "Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "keybert is already installed.\n",
      "transformers is already installed.\n",
      "sentence_transformers is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"numpy\", \"torch\", \"scikit-learn\", \"keybert\", \"transformers\", \"sentence_transformers\"\n",
    "    \n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d59f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy: fundamental package for numerical operations and array handling\n",
    "import numpy as np\n",
    "\n",
    "# Typing module: used for function signature annotations (e.g., Tuple[int, int])\n",
    "from typing import Tuple\n",
    "\n",
    "# PyTorch: core deep learning framework for tensor operations and model inference\n",
    "import torch\n",
    "\n",
    "# PyTorch functional API: provides functions like softmax, relu, etc.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# scikit-learn utility to extract candidate phrases based on n-gram statistics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# scikit-learn function to compute cosine similarity between vector embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# KeyBERT: base class for keyword extraction using BERT-based embeddings\n",
    "from keybert import KeyBERT as KB\n",
    "\n",
    "# HuggingFace Transformers:\n",
    "# - AutoTokenizer: automatically loads the correct tokenizer for a given model\n",
    "# - AutoModelForSequenceClassification: loads a pretrained classification model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# SentenceTransformers: framework for encoding text into dense embeddings using pretrained transformer models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Library for regular expressions\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c140c4",
   "metadata": {},
   "source": [
    "# Classes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fd294",
   "metadata": {},
   "source": [
    "## SentimentModel: Flexible Transformer-based Sentiment Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f387a9e",
   "metadata": {},
   "source": [
    "The `SentimentModel` class provides a unified interface for performing sentiment analysis using pretrained HuggingFace transformer models.  \n",
    "It is designed to return **probability distributions** over sentiment classes and compute **continuous sentiment scores** for downstream tasks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This class supports both 3-class models (e.g., `cardiffnlp/twitter-roberta-base-sentiment`) and 5-class models (e.g., `nlptown/bert-base-multilingual-uncased-sentiment`) by dynamically adapting to the label schema of the specified model.\n",
    "\n",
    "It is used across both **reranking** and **candidate selection** pipelines to ensure consistent, interpretable sentiment scoring.\n",
    "\n",
    "### Functionality\n",
    "\n",
    "- **Model Loading:**  \n",
    "  Loads the tokenizer and sequence classification model specified via the `model_name`.  \n",
    "  All models must be compatible with HuggingFace’s `AutoModelForSequenceClassification`.\n",
    "\n",
    "- **Device Handling:**  \n",
    "  Automatically moves the model to the selected device (`cpu` or `cuda`) and validates compatibility.\n",
    "\n",
    "- **Token Limit Management:**  \n",
    "  For models with a maximum sequence length (typically **512 tokens**), the class **automatically splits input texts into smaller chunks**,  \n",
    "  runs inference on each chunk, and returns the **average probability distribution**.  \n",
    "  This prevents information loss and avoids runtime errors on long texts.\n",
    "\n",
    "- **Class Probability Prediction (`predict_proba`):**  \n",
    "  1. Tokenization and encoding of input text(s) with padding and truncation  \n",
    "  2. Automatic chunking if input exceeds token limit  \n",
    "  3. Inference using the transformer model (no gradient tracking)  \n",
    "  4. Softmax activation over logits to obtain class probabilities  \n",
    "  5. Returns a NumPy array of shape `(batch_size, num_classes)`\n",
    "\n",
    "- **Continuous Sentiment Scoring (`predict_score`):**  \n",
    "  Computes a **score in [0, 1]** as a probability-weighted average over mapped class values  \n",
    "  (e.g., `1 star → 0.0`, `5 stars → 1.0`, or `negative → 0.0`, `positive → 1.0`).\n",
    "\n",
    "### Why Use It\n",
    "\n",
    "- Compatible with **any HuggingFace sentiment model**\n",
    "- Automatically handles **long texts via chunk-aware processing**\n",
    "- Supports **batch processing** and **fine-grained scoring**\n",
    "- Integrates seamlessly with **sentiment-aware KeyBERT pipelines**\n",
    "- Outputs **interpretable, continuous sentiment scores**\n",
    "\n",
    "### Example Output\n",
    "\n",
    "→ **Text**: \"absolutely loved\"  \n",
    "→ **Probabilities:** `[0.01, 0.02, 0.07, 0.18, 0.72]`  \n",
    "→ **Score:** `0.89`\n",
    "\n",
    "→ **Text**: \"underdeveloped twist\"  \n",
    "→ **Probabilities:** `[0.55, 0.30, 0.10, 0.04, 0.01]`  \n",
    "→ **Score:** `0.22`\n",
    "\n",
    "→ **Text**: \"The film is visually stunning and emotionally profound, despite a few narrative missteps.\"  \n",
    "→ **Probabilities:** `[0.03, 0.07, 0.12, 0.25, 0.53]`  \n",
    "→ **Score:** `0.78`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca48c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Class definition\n",
    "class SentimentModel:\n",
    "    \"\"\"\n",
    "    A flexible sentiment analysis wrapper supporting multiple HuggingFace models.\n",
    "\n",
    "    This class dynamically adapts to the label schema of the specified model,\n",
    "    allowing for consistent polarity scoring across different sentiment models.\n",
    "    It also supports automatic truncation and chunking for long texts that exceed\n",
    "    the model's token limit, ensuring robust performance on lengthy inputs.\n",
    "\n",
    "    For models with a token limit (e.g., BERT-based with 512 tokens), the class\n",
    "    splits long texts into chunks, computes predictions for each, and returns\n",
    "    the averaged sentiment probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\", device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        model_name : str\n",
    "            HuggingFace model identifier.\n",
    "            Default is \"cardiffnlp/twitter-roberta-base-sentiment\".\n",
    "            Alternatively, you can use \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "        device : str\n",
    "            Computation device. Should be either 'cpu' or 'cuda'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate the selected device\n",
    "        if device not in [\"cpu\", \"cuda\"]:\n",
    "            raise ValueError(\"Device must be 'cpu' or 'cuda'.\")\n",
    "\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load tokenizer and model from HuggingFace Hub\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "        # Determine label mapping based on the model\n",
    "        self._set_label_mapping()\n",
    "\n",
    "    def _set_label_mapping(self):\n",
    "        \"\"\"\n",
    "        Set the label to score mapping based on the model's label schema.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve the model's configuration to get label mappings\n",
    "        id2label = self.model.config.id2label\n",
    "\n",
    "        # Sort labels by their IDs to maintain order\n",
    "        self.labels_ordered = [id2label[i] for i in range(len(id2label))]\n",
    "\n",
    "        # Define label to score mapping based on known models\n",
    "        if self.model_name == \"cardiffnlp/twitter-roberta-base-sentiment\":\n",
    "            # Labels: ['negative', 'neutral', 'positive']\n",
    "            self.label_to_score = {\n",
    "                'LABEL_0': 0.0, # negative\n",
    "                'LABEL_1': 0.5, # neutral\n",
    "                'LABEL_2': 1.0  # positive\n",
    "            }\n",
    "        elif self.model_name == \"nlptown/bert-base-multilingual-uncased-sentiment\":\n",
    "            # Labels: ['1 star', '2 stars', '3 stars', '4 stars', '5 stars']\n",
    "            self.label_to_score = {\n",
    "                '1 star': 0.0,\n",
    "                '2 stars': 0.25,\n",
    "                '3 stars': 0.5,\n",
    "                '4 stars': 0.75,\n",
    "                '5 stars': 1.0\n",
    "            }\n",
    "        else:\n",
    "            # For unknown models, assign scores evenly across labels\n",
    "            num_labels = len(self.labels_ordered)\n",
    "            self.label_to_score = {\n",
    "                label: idx / (num_labels - 1) for idx, label in enumerate(self.labels_ordered)\n",
    "            }\n",
    "\n",
    "    def _split_into_chunks(self, text, max_length=512):\n",
    "        \"\"\"\n",
    "        Split long texts into smaller chunks that do not exceed the token limit.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The input text to split.\n",
    "\n",
    "        max_length : int\n",
    "            Maximum token length allowed by the model.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        List[str]\n",
    "            A list of text chunks within the token limit.\n",
    "        \"\"\"\n",
    "\n",
    "        # Naively split on sentence delimiters\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "        chunks, current = [], \"\"\n",
    "\n",
    "        for sent in sentences:\n",
    "            # Check tokenized length with current buffer\n",
    "            if len(self.tokenizer.encode(current + \" \" + sent, add_special_tokens=True)) <= max_length:\n",
    "                current += \" \" + sent\n",
    "            else:\n",
    "                if current:\n",
    "                    chunks.append(current.strip())\n",
    "                current = sent\n",
    "        if current:\n",
    "            chunks.append(current.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def predict_proba(self, texts, max_length=512):\n",
    "        \"\"\"\n",
    "        Compute the probability distribution over sentiment classes for one or more input texts.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        texts : List[str]\n",
    "            List of text strings to analyze.\n",
    "\n",
    "        max_length : int\n",
    "            Maximum token length per chunk (default: 512).\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 2D array of shape (len(texts), num_classes), where each row represents\n",
    "            the predicted softmax probabilities for the corresponding input.\n",
    "        \"\"\"\n",
    "\n",
    "        all_probs = []\n",
    "\n",
    "        for text in texts:\n",
    "            # Always chunk to avoid loss of info on long inputs\n",
    "            chunks = self._split_into_chunks(text, max_length=max_length)\n",
    "            chunk_probs = []\n",
    "\n",
    "            for chunk in chunks:\n",
    "                # Tokenize and infer sentiment per chunk\n",
    "                inputs = self.tokenizer(chunk, return_tensors=\"pt\", truncation=True,\n",
    "                                        padding=True, max_length=max_length).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                    probs = F.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "                    chunk_probs.append(probs)\n",
    "\n",
    "            # Average the probabilities over all chunks\n",
    "            avg_probs = np.mean(chunk_probs, axis=0)\n",
    "            all_probs.append(avg_probs)\n",
    "\n",
    "        return np.array(all_probs)\n",
    "\n",
    "    def predict_score(self, text):\n",
    "        \"\"\"\n",
    "        Compute the continuous sentiment score for a single input text.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The input text to analyze.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        float\n",
    "            The sentiment score in the range [0, 1].\n",
    "        \"\"\"\n",
    "\n",
    "        probs = self.predict_proba([text])[0]\n",
    "        score = sum(\n",
    "            prob * self.label_to_score[label]\n",
    "            for prob, label in zip(probs, self.labels_ordered)\n",
    "        )\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e61d4",
   "metadata": {},
   "source": [
    "## KeyBERTSentimentAware Class: Sentiment-Integrated Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9c9ed",
   "metadata": {},
   "source": [
    "This class extends the base KeyBERT model by integrating sentiment analysis directly into the keyword extraction pipeline. It enhances the traditional semantic-only approach by incorporating continuous sentiment polarity scores for both the entire document and each candidate keyword.\n",
    "\n",
    "### Overview\n",
    "\n",
    "- **Candidate Extraction:**  \n",
    "  Uses `CountVectorizer` to extract a broad pool of candidate keywords (n-grams) from the document text.  \n",
    "  **Note:** see next paragraph for details.   \n",
    "\n",
    "- **Sentiment Analysis:**  \n",
    "  Leverages a transformer-based sentiment classification model (via HuggingFace) to compute **continuous sentiment polarity scores** in the range \\[0, 1\\] for both the document and each candidate keyword.\n",
    "\n",
    "- **Combined Scoring and Filtering:**  \n",
    "  Computes a weighted score that balances:\n",
    "  - **Semantic similarity** (cosine similarity between document and candidate embeddings).\n",
    "  - **Sentiment alignment** (1 minus the absolute difference between candidate and document sentiment).\n",
    "\n",
    "  The sentiment alignment score is then mapped to \\[-1, 1\\] to match the range of cosine similarity.  \n",
    "  Candidates with low combined scores are **filtered out before ranking**, allowing sentiment to guide selection from the earliest stages.\n",
    "\n",
    "  The trade-off is controlled by the `alpha` parameter:\n",
    "  - `alpha = 1.0` → pure sentiment-based filtering and ranking.\n",
    "  - `alpha = 0.0` → pure semantic similarity (equivalent to standard KeyBERT).\n",
    "\n",
    "### Candidate Selection in KeyBERT vs Sentiment-Aware Extension\n",
    "\n",
    "In standard KeyBERT, candidate keywords are extracted solely based on **surface-level frequency statistics** using `CountVectorizer`, without semantic or sentiment awareness. This means:\n",
    "\n",
    "- The candidate pool may include terms that are **frequent but sentimentally mismatched** with the document.\n",
    "- In strongly polarized reviews, irrelevant or misleading keywords may still be selected if they occur frequently.\n",
    "\n",
    "The sentiment-aware extension addresses this by introducing a **joint semantic-sentiment filter** immediately after candidate extraction:\n",
    "\n",
    "1. Extract a wide candidate pool using `CountVectorizer`.\n",
    "2. Compute continuous sentiment polarity scores for both the document and each candidate.\n",
    "3. Calculate a combined score per candidate, balancing semantic relevance and emotional alignment.\n",
    "4. Discard candidates whose combined score falls below a filtering threshold.\n",
    "\n",
    "This process ensures that only keywords that are both **topically and emotionally relevant** progress to the final ranking stage.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `model`: The semantic embedding model (e.g., a `SentenceTransformer` from `sentence-transformers`).\n",
    "- `sentiment_model_name`: HuggingFace identifier for the sentiment classifier (default: 5-class multilingual model).\n",
    "- `alpha`: Weight controlling the influence of sentiment (0 = only semantic, 1 = only sentiment).\n",
    "- `candidate_pool_size`: Maximum number of candidates extracted before filtering.\n",
    "- `device`: Device to run the model (`\"cpu\"` or `\"cuda\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyBERT extension for sentiment-aware keyword extraction\n",
    "# This class extends KeyBERT to include sentiment-aware keyword extraction\n",
    "# by defining a subclass of KeyBERT that modifies the scoring phase to incorporate\n",
    "# sentiment alignment in a post-processing step. The goal is to boost keywords\n",
    "# that are both semantically relevant and emotionally aligned with the overall\n",
    "# sentiment of the input review.\n",
    "class KeyBERTSentimentAware(KB):\n",
    "    \"\"\"\n",
    "    Extension of KeyBERT to integrate sentiment analysis in keyword extraction.\n",
    "\n",
    "    This class overrides and extends parts of KeyBERT's pipeline to:\n",
    "    - Extract a larger candidate pool using CountVectorizer.\n",
    "    - Calculate sentiment polarity scores for the document and candidates,\n",
    "      using a pretrained sentiment classification model with continuous outputs.\n",
    "    - Combine semantic similarity and sentiment alignment scores via a weighting factor alpha.\n",
    "    - Filter candidate keywords based on this combined score before final ranking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SentenceTransformer\n",
    "        Semantic embedding model used by KeyBERT.\n",
    "\n",
    "    sentiment_model_name : str, optional (default: \"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        Identifier of pretrained sentiment model on HuggingFace Hub.\n",
    "\n",
    "    alpha : float, optional (default: 0.5)\n",
    "        Weight to balance sentiment alignment vs semantic similarity.\n",
    "        alpha=1.0 means only sentiment alignment is considered.\n",
    "        alpha=0.0 means only semantic similarity is considered.\n",
    "\n",
    "    candidate_pool_size : int, optional (default: 100)\n",
    "        Maximum number of initial candidate keywords to extract.\n",
    "\n",
    "    device : str, optional (default: \"cpu\")\n",
    "        Device to run embedding and sentiment models on (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        sentiment_model_name: str =\"cardiffnlp/twitter-roberta-base-sentiment\", # or \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        alpha: float = 0.5,\n",
    "        candidate_pool_size: int = 100,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        # Validate that the specified device is either 'cpu' or 'cuda'\n",
    "        valid_devices = {\"cpu\", \"cuda\"}\n",
    "        if device not in valid_devices:\n",
    "            raise ValueError(f\"Device must be one of {valid_devices}.\")\n",
    "        \n",
    "        # Check CUDA availability if 'cuda' is requested\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise ValueError(\"CUDA is not available. Please use 'cpu' instead.\")\n",
    "\n",
    "        # Validate input types to ensure correct usage\n",
    "        if not isinstance(model, SentenceTransformer):\n",
    "            raise TypeError(\"model must be an instance of SentenceTransformer.\")\n",
    "        if not isinstance(sentiment_model_name, str):\n",
    "            raise TypeError(\"sentiment_model_name must be a string.\")\n",
    "        if not isinstance(alpha, float):\n",
    "            raise TypeError(\"alpha must be a float.\")\n",
    "        if not isinstance(candidate_pool_size, int):\n",
    "            raise TypeError(\"candidate_pool_size must be an integer.\")\n",
    "\n",
    "        # Validate value ranges to prevent logical errors\n",
    "        if not (0.0 <= alpha <= 1.0):\n",
    "            raise ValueError(\"alpha must be between 0 and 1 inclusive.\")\n",
    "        if candidate_pool_size <= 0:\n",
    "            raise ValueError(\"candidate_pool_size must be a positive integer.\")\n",
    "\n",
    "        # Initialize the superclass (KeyBERT) with the semantic embedding model\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Assign validated parameters to instance variables\n",
    "        self._alpha = None\n",
    "        self.alpha = alpha\n",
    "        self.candidate_pool_size = candidate_pool_size\n",
    "        self.device = device\n",
    "\n",
    "        # Store the semantic embedding model for embedding computation\n",
    "        self.embedder = model\n",
    "\n",
    "        # Initialize the sentiment model wrapper with the given model name and device\n",
    "        self.sentiment_model = SentimentModel(sentiment_model_name, device=device)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self._alpha\n",
    "\n",
    "    @alpha.setter\n",
    "    def alpha(self, value):\n",
    "        if not 0.0 <= value <= 1.0:\n",
    "            raise ValueError(f\"alpha must be in [0, 1]. Got {value}\")\n",
    "        self._alpha = value\n",
    "\n",
    "    def _get_doc_polarity_continuous(self, doc: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the document's continuous sentiment polarity score as the weighted sum of\n",
    "        predicted class probabilities multiplied by their numeric mappings.\n",
    "\n",
    "        This method overrides and replaces any default sentiment handling in the base class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            The document text.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Continuous sentiment polarity score between 0 (very negative) and 1 (very positive).\n",
    "        \"\"\"\n",
    "        # Get probability distribution over sentiment classes for the document\n",
    "        probs = self.sentiment_model.predict_proba([doc])[0]\n",
    "\n",
    "        # Compute continuous polarity as weighted average of class scores\n",
    "        polarity = sum(\n",
    "            p * self.sentiment_model.label_to_score[label]\n",
    "            for p, label in zip(probs, self.sentiment_model.labels_ordered)\n",
    "        )\n",
    "        return polarity\n",
    "\n",
    "    def _get_candidate_polarities(self, candidates) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute continuous sentiment polarity scores for each candidate keyword.\n",
    "\n",
    "        This method extends candidate scoring with sentiment, overriding base candidate processing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        candidates : iterable of str\n",
    "            List of candidate keywords.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Array of polarity scores for each candidate keyword.\n",
    "        \"\"\"\n",
    "        candidates = list(candidates)  # ensure correct input format for tokenizer\n",
    "        \n",
    "        # Batch predict probabilities for all candidates\n",
    "        probs_list = self.sentiment_model.predict_proba(candidates)\n",
    "        \n",
    "        polarities = []\n",
    "        for probs in probs_list:\n",
    "            # Weighted average as continuous polarity score\n",
    "            polarity = sum(\n",
    "                p * self.sentiment_model.label_to_score[label]\n",
    "                for p, label in zip(probs, self.sentiment_model.labels_ordered)\n",
    "            )\n",
    "            polarities.append(polarity)\n",
    "        return np.array(polarities)\n",
    "\n",
    "    def _select_candidates(\n",
    "        self, \n",
    "        doc: str, \n",
    "        ngram_range: Tuple[int, int] = (1, 3), \n",
    "        threshold: float = 0.4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract initial candidates with CountVectorizer and filter them based on combined\n",
    "        semantic similarity and sentiment alignment scores.\n",
    "\n",
    "        This method replaces the default candidate generation and filtering steps of KeyBERT,\n",
    "        incorporating sentiment filtering before final keyword ranking.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Document text.\n",
    "\n",
    "        ngram_range : tuple of int\n",
    "            N-gram size range for candidate extraction.\n",
    "\n",
    "        threshold : float\n",
    "            Minimum combined score for candidate retention.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of str\n",
    "            Filtered list of candidate keywords.\n",
    "        \"\"\"\n",
    "        # Extract candidates with CountVectorizer (statistical n-grams)\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english',\n",
    "            max_features=self.candidate_pool_size\n",
    "        )\n",
    "        candidates = vectorizer.fit([doc]).get_feature_names_out()\n",
    "\n",
    "        # Compute semantic embeddings for doc and candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity scores\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "        sentiment_scores_mapped = 2 * sentiment_scores - 1\n",
    "\n",
    "        # Combine semantic and sentiment scores with alpha weighting\n",
    "        combined_scores = self.alpha * sentiment_scores_mapped + (1 - self.alpha) * sim_scores\n",
    "\n",
    "        # Filter candidates that meet threshold on combined score\n",
    "        filtered_candidates = [c for c, s in zip(candidates, combined_scores) if s >= threshold]\n",
    "\n",
    "        return filtered_candidates\n",
    "\n",
    "    def extract_keywords(\n",
    "        self,\n",
    "        doc: str,\n",
    "        top_n: int = 5,\n",
    "        candidate_threshold: float = 0.4,\n",
    "        keyphrase_ngram_range: Tuple[int, int] = (1, 3),\n",
    "        print_doc_polarity: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract top keywords from a document by combining semantic similarity and sentiment alignment.\n",
    "\n",
    "        This method overrides the `extract_keywords` method from KeyBERT base class,\n",
    "        adding sentiment-aware candidate filtering and scoring.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        doc : str\n",
    "            Input document text.\n",
    "\n",
    "        top_n : int\n",
    "            Number of keywords to return.\n",
    "\n",
    "        candidate_threshold : float\n",
    "            Threshold score to filter candidate keywords.\n",
    "\n",
    "        keyphrase_ngram_range : tuple of int\n",
    "            N-gram range for candidate keyword extraction.\n",
    "\n",
    "        print_doc_polarity : bool\n",
    "            Whether to print the document's sentiment polarity score.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of tuples\n",
    "            List of (keyword, score, keyword_sentiment) tuples sorted by descending combined score.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Select candidates filtered by combined semantic+sentiment scoring\n",
    "        candidates = self._select_candidates(\n",
    "            doc,\n",
    "            ngram_range=keyphrase_ngram_range,\n",
    "            threshold=candidate_threshold\n",
    "        )\n",
    "        if not candidates:\n",
    "            print(\"No candidates passed the sentiment-semantic filter.\")\n",
    "            return []\n",
    "\n",
    "        # Compute semantic embeddings for document and filtered candidates\n",
    "        doc_emb = self.model.embed([doc])\n",
    "        cand_emb = self.model.embed(candidates)\n",
    "\n",
    "        # Compute continuous sentiment polarity for the document\n",
    "        doc_pol = self._get_doc_polarity_continuous(doc)\n",
    "\n",
    "        # Print document polarity if requested\n",
    "        if print_doc_polarity:\n",
    "            # Scale polarity from [0,1] to [0,10]\n",
    "            scaled_pol = doc_pol * 10\n",
    "\n",
    "            # Determine polarity label with neutral zone between 4 and 6 on 0-10 scale\n",
    "            if scaled_pol < 5.5:\n",
    "                polarity_label = \"Negative\"\n",
    "            elif scaled_pol > 6.5:\n",
    "                polarity_label = \"Positive\"\n",
    "            else:\n",
    "                polarity_label = \"Neutral\"\n",
    "\n",
    "            print(f\"\\n=== Document Polarity Score: {scaled_pol:.2f} ({polarity_label}) ===\\n\")\n",
    "\n",
    "        # Compute sentiment polarities for candidates\n",
    "        cand_pols = self._get_candidate_polarities(candidates)\n",
    "\n",
    "        # Calculate cosine semantic similarity scores (range [-1,1])\n",
    "        sim_scores = cosine_similarity(doc_emb, cand_emb)[0]\n",
    "\n",
    "        # Calculate sentiment alignment scores in [0,1] and map to [-1,1]\n",
    "        sentiment_scores = 1 - np.abs(cand_pols - doc_pol)\n",
    "        sentiment_scores_mapped = 2 * sentiment_scores - 1\n",
    "\n",
    "        # Final combined score with weighting factor alpha in [-1,1]\n",
    "        final_scores = self.alpha * sentiment_scores_mapped + (1 - self.alpha) * sim_scores\n",
    "\n",
    "        # Select top_n keywords sorted by combined score descending\n",
    "        top_indices = np.argsort(final_scores)[-top_n:][::-1]\n",
    "\n",
    "        return [(candidates[i], final_scores[i], cand_pols[i]) for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d991b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1533b",
   "metadata": {},
   "source": [
    "## Test 1: Basic Keyword Extraction with Sentiment-Aware KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebd768",
   "metadata": {},
   "source": [
    "This first test demonstrates the basic usage of the `KeyBERTSentimentAware` class on a simple document. We extract the top keywords combining semantic similarity and sentiment alignment with default parameter settings.\n",
    "\n",
    "**Objectives:**\n",
    "- Verify that the class instantiates correctly.\n",
    "- Check that keywords are extracted without errors.\n",
    "- Observe the impact of sentiment-aware ranking on keyword scores.\n",
    "\n",
    "We use a short, clearly positive sentence to observe how sentiment affects the keyword selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48528652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Document Polarity Score: 9.94 (Positive) ===\n",
      "\n",
      "Extracted Keywords, Scores, and Sentiment Polarity:\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "movie fantastic beautiful          0.8259        0.978\n",
      "movie fantastic                    0.7670        0.945\n",
      "great acting                       0.7512        0.914\n",
      "visuals great acting               0.7383        0.915\n",
      "beautiful visuals great            0.7343        0.989\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model \n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Sentiment-Aware KeyBERT\n",
    "kw_model = KeyBERTSentimentAware(\n",
    "                        model=embedding_model, \n",
    "                        alpha=0.5,\n",
    "                        sentiment_model_name=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Sample review\n",
    "doc = \"The movie was fantastic with beautiful visuals and great acting.\"\n",
    "\n",
    "# Extract top 5 keywords with polarity info\n",
    "keywords = kw_model.extract_keywords(doc, top_n=5, print_doc_polarity=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Extracted Keywords, Scores, and Sentiment Polarity:\\n\")\n",
    "print(f\"{'Keyword':30s} {'Score':>10s} {'Polarity':>12s}\")\n",
    "print(\"-\" * 55)\n",
    "for kw, score, pol in keywords:\n",
    "    print(f\"{kw:30s} {score:10.4f} {pol:12.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7016e7",
   "metadata": {},
   "source": [
    "## Test 2: Comparing Sentiment-Aware KeyBERT with Base KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e6b8e",
   "metadata": {},
   "source": [
    "In this test, we compare the keywords extracted by the sentiment-aware extension with those from the original KeyBERT model that relies solely on semantic similarity.\n",
    "\n",
    "**Objectives:**\n",
    "- Highlight differences in keyword selection between semantic-only and sentiment-aware approaches.\n",
    "- Understand the effect of integrating sentiment on keyword ranking.\n",
    "- Use the same document for a fair comparison.\n",
    "\n",
    "We use a document containing both positive and negative elements to see how sentiment influences the extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c80e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE KeyBERT Keywords:\n",
      "\n",
      "Keyword                             Score\n",
      "------------------------------------------\n",
      "soundtrack complements tone        0.6131\n",
      "impressive scenes dialogue         0.5666\n",
      "scenes dialogue feels              0.5444\n",
      "soundtrack complements             0.5318\n",
      "coherence soundtrack complements     0.5222\n",
      "\n",
      "=== Document Polarity Score: 8.51 (Positive) ===\n",
      "\n",
      "Sentiment-Aware KeyBERT Keywords:\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "----------------------------------------------------------\n",
      "impressive scenes dialogue         0.7602        0.874\n",
      "soundtrack complements tone        0.7347        0.779\n",
      "film offers solid                  0.7209        0.874\n",
      "cast visually impressive           0.7045        0.862\n",
      "visually impressive scenes         0.6739        0.914\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize models\n",
    "kw_base = KB(model=embedding_model)\n",
    "kw_sentiment = KeyBERTSentimentAware(\n",
    "                        model=embedding_model, \n",
    "                        alpha=0.5,\n",
    "                        sentiment_model_name=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Mixed sentiment document\n",
    "doc_mixed = (\n",
    "    \"The film offers a solid cast and visually impressive scenes, \"\n",
    "    \"although the dialogue often feels forced and some transitions lack coherence. \"\n",
    "    \"While the soundtrack complements the tone effectively, the overall narrative structure \"\n",
    "    \"is conventional and doesn't take many risks. It's a competent production with moments of brilliance, \"\n",
    "    \"but also several missed opportunities.\"\n",
    ")\n",
    "\n",
    "# Extract keywords — Base KeyBERT\n",
    "base_keywords = kw_base.extract_keywords(doc_mixed, top_n=5, keyphrase_ngram_range=(1, 3))\n",
    "\n",
    "print(\"BASE KeyBERT Keywords:\\n\")\n",
    "print(f\"{'Keyword':30s} {'Score':>10s}\")\n",
    "print(\"-\" * 42)\n",
    "for kw, score in base_keywords:\n",
    "    print(f\"{kw:30s} {score:10.4f}\")\n",
    "\n",
    "# Extract keywords — Sentiment-Aware KeyBERT\n",
    "sentiment_keywords = kw_sentiment.extract_keywords(\n",
    "    doc_mixed,\n",
    "    top_n=5,\n",
    "    keyphrase_ngram_range=(1, 3),\n",
    "    print_doc_polarity=True\n",
    ")\n",
    "\n",
    "print(\"Sentiment-Aware KeyBERT Keywords:\\n\")\n",
    "print(f\"{'Keyword':30s} {'Score':>10s} {'Polarity':>12s}\")\n",
    "print(\"-\" * 58)\n",
    "for kw, score, polarity in sentiment_keywords:\n",
    "    print(f\"{kw:30s} {score:10.4f} {polarity:12.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d686562",
   "metadata": {},
   "source": [
    "## Test 3: Candidate Filtering with Different Thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da344ff",
   "metadata": {},
   "source": [
    "\n",
    "In this test, we explore how varying the candidate filtering threshold affects the pool of candidate keywords before the final ranking.\n",
    "\n",
    "**Objectives:**\n",
    "- Understand the impact of the `candidate_threshold` parameter on candidate selection.\n",
    "- Observe how stricter thresholds reduce candidate pool size and potentially increase keyword relevance.\n",
    "- Use a moderately complex document with mixed sentiment.\n",
    "\n",
    "This test highlights the balance between recall and precision in candidate filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db445b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Candidate Threshold: 0.0\n",
      "Number of candidates after filtering: 26\n",
      "Candidates: ['beautiful cinematography', 'beautiful cinematography strong', 'cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong', 'strong performances', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.2\n",
      "Number of candidates after filtering: 21\n",
      "Candidates: ['cinematography', 'cinematography strong', 'cinematography strong performances', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'despite beautiful cinematography', 'difficult', 'difficult follow', 'difficult follow times', 'follow', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'strong performances storyline', 'times']\n",
      "\n",
      "Candidate Threshold: 0.4\n",
      "Number of candidates after filtering: 17\n",
      "Candidates: ['cinematography', 'cinematography strong', 'convoluted', 'convoluted difficult', 'convoluted difficult follow', 'despite', 'difficult', 'difficult follow', 'difficult follow times', 'follow times', 'performances', 'performances storyline', 'performances storyline convoluted', 'storyline', 'storyline convoluted', 'storyline convoluted difficult', 'times']\n",
      "\n",
      "Candidate Threshold: 0.6\n",
      "Number of candidates after filtering: 10\n",
      "Candidates: ['cinematography', 'convoluted difficult', 'convoluted difficult follow', 'difficult', 'difficult follow', 'difficult follow times', 'performances storyline', 'performances storyline convoluted', 'storyline convoluted', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 0.8\n",
      "Number of candidates after filtering: 2\n",
      "Candidates: ['performances storyline convoluted', 'storyline convoluted difficult']\n",
      "\n",
      "Candidate Threshold: 1.0\n",
      "Number of candidates after filtering: 0\n",
      "Candidates: []\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the semantic embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize our sentiment-aware KeyBERT with default weight_sentiment=0.5\n",
    "kw_model = KeyBERTSentimentAware(\n",
    "                        model=embedding_model, \n",
    "                        alpha=0.5,\n",
    "                        sentiment_model_name=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "\n",
    "doc = (\n",
    "    \"Despite the beautiful cinematography and strong performances, \"\n",
    "    \"the storyline was convoluted and difficult to follow at times.\"\n",
    ")\n",
    "\n",
    "thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"\\nCandidate Threshold: {thresh}\")\n",
    "    candidates = kw_model._select_candidates(doc, threshold=thresh)\n",
    "    print(f\"Number of candidates after filtering: {len(candidates)}\")\n",
    "    print(\"Candidates:\", candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10aa95",
   "metadata": {},
   "source": [
    "## Test 4: Impact of Sentiment Weighting (`alpha`) on Keyword Ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90d8fd",
   "metadata": {},
   "source": [
    "\n",
    "This test examines how changing the `alpha`, the weighting parameter influences the balance between semantic similarity and sentiment alignment in keyword scoring.\n",
    "\n",
    "**Objectives:**\n",
    "- Observe differences in extracted keywords when prioritizing sentiment vs. semantic relevance.\n",
    "- Understand the flexibility of the model in adapting to different use cases by tuning `alpha`.\n",
    "- Use a document with both positive and negative sentiments to highlight effect.\n",
    "\n",
    "We test three values of `alpha`: 0.0 (semantic only), 0.5 (balanced), and 1.0 (sentiment only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32000df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpha = 0.0\n",
      "\n",
      "=== Document Polarity Score: 9.90 (Positive) ===\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "film extraordinary                 0.6329        0.807\n",
      "scene felt                         0.5675        0.483\n",
      "masterful scene                    0.5253        0.825\n",
      "emotion performances               0.5084        0.517\n",
      "performances deeply                0.5039        0.684\n",
      "Testing alpha = 0.25\n",
      "\n",
      "=== Document Polarity Score: 9.90 (Positive) ===\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "film extraordinary                 0.6332        0.807\n",
      "visuals stunning                   0.5881        0.961\n",
      "masterful scene                    0.5616        0.825\n",
      "stunning direction                 0.5161        0.888\n",
      "story resonated                    0.5120        0.837\n",
      "Testing alpha = 0.5\n",
      "\n",
      "=== Document Polarity Score: 9.90 (Positive) ===\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "visuals stunning                   0.7063        0.961\n",
      "film extraordinary                 0.6335        0.807\n",
      "beautifully crafted                0.6131        0.936\n",
      "stunning direction                 0.6093        0.888\n",
      "masterful scene                    0.5979        0.825\n",
      "Testing alpha = 0.75\n",
      "\n",
      "=== Document Polarity Score: 9.90 (Positive) ===\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "visuals stunning                   0.8245        0.961\n",
      "beautifully crafted                0.7522        0.936\n",
      "stunning direction                 0.7025        0.888\n",
      "stunning                           0.6648        0.883\n",
      "purposeful beautifully             0.6646        0.863\n",
      "Testing alpha = 1.0\n",
      "\n",
      "=== Document Polarity Score: 9.90 (Positive) ===\n",
      "\n",
      "Keyword                             Score     Polarity\n",
      "-------------------------------------------------------\n",
      "visuals stunning                   0.9427        0.961\n",
      "beautifully crafted                0.8913        0.936\n",
      "stunning direction                 0.7956        0.888\n",
      "stunning                           0.7860        0.883\n",
      "beautifully                        0.7534        0.867\n",
      "Testing alpha = -0.2\n",
      "Error for alpha = -0.2: alpha must be in [0, 1]. Got -0.2\n",
      "Testing alpha = 1.2\n",
      "Error for alpha = 1.2: alpha must be in [0, 1]. Got 1.2\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 🔤 Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Sentiment-Aware KeyBERT\n",
    "kw_model = KeyBERTSentimentAware(\n",
    "                        model=embedding_model, \n",
    "                        alpha=0.5,\n",
    "                        sentiment_model_name=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "\n",
    "# Sample reviews\n",
    "neutral_rev = (\n",
    "    \"The film features competent acting and decent production values, \"\n",
    "    \"but lacks any standout elements. While the visuals are polished and the plot flows logically, \"\n",
    "    \"the characters remain underdeveloped and the emotional impact is minimal. \"\n",
    "    \"It’s a watchable experience, neither particularly memorable nor offensive.\"\n",
    ")\n",
    "\n",
    "positive_rev = (\n",
    "    \"This film was an extraordinary blend of artistry and emotion. \"\n",
    "    \"The performances were deeply moving, the visuals were stunning, and the direction masterful. \"\n",
    "    \"Every scene felt purposeful and beautifully crafted. The story resonated on a profound level, \"\n",
    "    \"leaving the audience inspired and emotionally fulfilled.\"\n",
    ")\n",
    "\n",
    "negative_rev = (\n",
    "    \"The movie was a frustrating mess of clichés and poor execution. \"\n",
    "    \"The acting felt robotic, the plot was incoherent, and the pacing dragged endlessly. \"\n",
    "    \"Even the soundtrack, which could have been a redeeming factor, was repetitive and uninspired. \"\n",
    "    \"By the end, it felt like a complete waste of time.\"\n",
    ")\n",
    "\n",
    "# Choose review for testing\n",
    "chosen_rev = positive_rev  # change to neutral_rev or negative_rev to test other tones\n",
    "\n",
    "# Test different alpha values (including invalid ones)\n",
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0, -0.2, 1.2]\n",
    "\n",
    "# Run test\n",
    "for alpha in weights:\n",
    "    try:\n",
    "        print(f\"Testing alpha = {alpha}\")\n",
    "        kw_model.alpha = alpha  # will raise ValueError if invalid\n",
    "\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            chosen_rev,\n",
    "            top_n=5,\n",
    "            keyphrase_ngram_range=(1, 2),\n",
    "            print_doc_polarity=True\n",
    "        )\n",
    "\n",
    "        print(f\"{'Keyword':30s} {'Score':>10s} {'Polarity':>12s}\")\n",
    "        print(\"-\" * 55)\n",
    "        for kw, score, pol in keywords:\n",
    "            print(f\"{kw:30s} {score:10.4f} {pol:12.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error for alpha = {alpha}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3a48",
   "metadata": {},
   "source": [
    "## Test 5:  Comparing Keyword Extraction Between Base KeyBERT and Sentiment-Aware KeyBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d719a1",
   "metadata": {},
   "source": [
    "This test compares the standard `KeyBERT` model with the `KeyBERTSentimentAware` extension on a small set of reviews with varying sentiment. The goal is to assess how sentiment integration influences the selection and ranking of keywords.\n",
    "\n",
    "**Objectives:**\n",
    "- Compute sentiment polarity for each review and observe the average emotional tone across the dataset.\n",
    "- Extract top keywords using both the standard KeyBERT and the sentiment-aware model.\n",
    "- Rank keywords based on their average score and frequency of occurrence across reviews.\n",
    "- Apply semantic diversity filtering to ensure that selected keywords are not too similar to each other.\n",
    "- Compare the final top 5 keywords from both models, and observe how sentiment-aware keywords differ in alignment and relevance.\n",
    "\n",
    "This test demonstrates how sentiment-aware filtering can shift the focus of keyword extraction, promoting emotionally consistent terms and reducing the prominence of keywords that conflict with the document’s tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6981df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity per review (0 = negative, 1 = positive):\n",
      "\n",
      " Review #1: Polarity = 0.957 (Positive)\n",
      " Review #2: Polarity = 0.983 (Positive)\n",
      " Review #3: Polarity = 0.877 (Positive)\n",
      " Review #4: Polarity = 0.980 (Positive)\n",
      " Review #5: Polarity = 0.993 (Positive)\n",
      " Review #6: Polarity = 0.022 (Negative)\n",
      "\n",
      "Average sentiment polarity across all reviews: 0.802 (Positive)\n",
      "\n",
      "Top 5 Diverse Keywords - Base KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "emotional depth screenplay                   0.6815\n",
      "uplifting cinematic journey                  0.6554\n",
      "absolute masterpiece                         0.6435\n",
      "narrative excellent acting                   0.6363\n",
      "beautifully crafted film                     0.6284\n",
      "\n",
      "Top 5 Diverse Keywords - Sentiment-Aware KeyBERT (Top 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "absolute masterpiece                         0.8071          0.978\n",
      "breathtaking soundtrack perfectly            0.7955          0.945\n",
      "best films                                   0.7851          0.969\n",
      "excellent acting                             0.7607          0.943\n",
      "beautifully crafted film                     0.7601          0.931\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT as KB\n",
    "import numpy as np\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize models: base and sentiment-aware\n",
    "kw_base = KB(model=embedding_model)\n",
    "kw_sentiment = KeyBERTSentimentAware(\n",
    "    model=embedding_model,\n",
    "    alpha=0.5,\n",
    "    sentiment_model_name=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# Define sample reviews\n",
    "reviews = [\n",
    "    \"\"\"This film is a stunning achievement in storytelling, with unforgettable characters and a gripping plot.\n",
    "    The visuals are breathtaking, and the soundtrack perfectly complements every scene.\"\"\",\n",
    "\n",
    "    \"\"\"I was completely captivated from start to finish. The performances were heartfelt,\n",
    "    and the director’s vision shines through in every frame. A truly inspiring movie experience.\"\"\",\n",
    "\n",
    "    \"\"\"A beautifully crafted film with rich emotional depth. The screenplay is tight,\n",
    "    and the cinematography creates an immersive atmosphere that kept me hooked.\"\"\",\n",
    "\n",
    "    \"\"\"One of the best films I've seen in years. It combines a compelling narrative with excellent acting,\n",
    "    making it both entertaining and thought-provoking.\"\"\",\n",
    "\n",
    "    \"\"\"An absolute masterpiece! Every element, from the score to the visual effects,\n",
    "    contributes to a powerful and uplifting cinematic journey.\"\"\",\n",
    "\n",
    "    \"\"\"An utter disaster of a film. The plot was incoherent, characters were completely flat, and the \n",
    "    pacing was excruciatingly slow. Dialogue felt forced and unnatural throughout. \n",
    "    I struggled to stay awake — a total waste of time. I hate this film in all its aspects\"\"\"\n",
    "]\n",
    "\n",
    "# Helper: Convert polarity score into a label\n",
    "def polarity_label(p):\n",
    "    if p < 0.4:\n",
    "        return \"Negative\"\n",
    "    elif p > 0.6:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Compute and print sentiment polarity for each review\n",
    "print(\"Sentiment polarity per review (0 = negative, 1 = positive):\\n\")\n",
    "polarities = []\n",
    "for i, review in enumerate(reviews, 1):\n",
    "    pol = kw_sentiment._get_doc_polarity_continuous(review)\n",
    "    polarities.append(pol)\n",
    "    print(f\" Review #{i}: Polarity = {pol:.3f} ({polarity_label(pol)})\")\n",
    "\n",
    "mean_polarity = np.mean(polarities)\n",
    "print(f\"\\nAverage sentiment polarity across all reviews: {mean_polarity:.3f} ({polarity_label(mean_polarity)})\")\n",
    "\n",
    "# Accumulate scores and polarities per keyword across reviews\n",
    "def accumulate_keywords(model, reviews):\n",
    "    scores = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    polarities = defaultdict(list)\n",
    "\n",
    "    for review in reviews:\n",
    "        kws = model.extract_keywords(review, top_n=7, keyphrase_ngram_range=(1, 3))\n",
    "\n",
    "        for result in kws:\n",
    "            if isinstance(result, tuple) and len(result) == 3:\n",
    "                kw, score, polarity = result\n",
    "                polarities[kw].append(polarity)\n",
    "            elif isinstance(result, tuple) and len(result) == 2:\n",
    "                kw, score = result\n",
    "                polarity = float('nan')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            scores[kw] += score\n",
    "            counts[kw] += 1\n",
    "\n",
    "    return scores, counts, polarities\n",
    "\n",
    "# Extract data from both models\n",
    "base_scores, base_counts, base_pols = accumulate_keywords(kw_base, reviews)\n",
    "sent_scores, sent_counts, sent_pols = accumulate_keywords(kw_sentiment, reviews)\n",
    "\n",
    "# Compute average score and average polarity per keyword\n",
    "def compute_averages(scores, counts, polarities):\n",
    "    avg_scores = {kw: scores[kw] / counts[kw] for kw in scores}\n",
    "    avg_pols = {kw: np.mean(polarities[kw]) for kw in polarities} if polarities else {}\n",
    "    return avg_scores, avg_pols\n",
    "\n",
    "base_avg_scores, base_avg_pols = compute_averages(base_scores, base_counts, base_pols)\n",
    "sent_avg_scores, sent_avg_pols = compute_averages(sent_scores, sent_counts, sent_pols)\n",
    "\n",
    "# Rank keywords using score + normalized frequency\n",
    "def rank_keywords(avg_scores, counts, alpha=0.5):\n",
    "    max_count = max(counts.values()) if counts else 1\n",
    "    ranked = []\n",
    "    for kw in avg_scores:\n",
    "        freq_norm = counts[kw] / max_count\n",
    "        combined_score = alpha * avg_scores[kw] + (1 - alpha) * freq_norm\n",
    "        ranked.append((kw, combined_score))\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    return ranked\n",
    "\n",
    "base_ranked = rank_keywords(base_avg_scores, base_counts)\n",
    "sent_ranked = rank_keywords(sent_avg_scores, sent_counts)\n",
    "\n",
    "# Embed keyword phrases and normalize embeddings\n",
    "def embed_keywords(keywords):\n",
    "    return embedding_model.encode(keywords, convert_to_tensor=True, normalize_embeddings=True).cpu().numpy()\n",
    "\n",
    "# Select top N semantically diverse keywords\n",
    "def select_diverse_keywords(ranked_list, avg_scores, avg_pols, top_n=5, similarity_threshold=0.7):\n",
    "    selected = []\n",
    "    selected_embs = []\n",
    "\n",
    "    all_keywords = [kw for kw, _ in ranked_list]\n",
    "    all_embs = embed_keywords(all_keywords)\n",
    "\n",
    "    for i, (kw, _) in enumerate(ranked_list):\n",
    "        if not selected:\n",
    "            selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "            selected_embs.append(all_embs[i])\n",
    "        else:\n",
    "            emb = all_embs[i]\n",
    "            sims = [np.dot(emb, se) for se in selected_embs]\n",
    "            if max(sims) < similarity_threshold:\n",
    "                selected.append((kw, avg_scores[kw], avg_pols.get(kw, float('nan'))))\n",
    "                selected_embs.append(emb)\n",
    "        if len(selected) >= top_n:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "# Final selection\n",
    "top_base = select_diverse_keywords(base_ranked, base_avg_scores, base_avg_pols, top_n=5)\n",
    "top_sent = select_diverse_keywords(sent_ranked, sent_avg_scores, sent_avg_pols, top_n=5)\n",
    "\n",
    "# Print final results\n",
    "def print_topics(title, topics, show_polarity=True, flop=False):\n",
    "    if flop:\n",
    "        print(f\"\\n{title} (Flop {len(topics)}):\")\n",
    "        topics_to_print = reversed(topics)\n",
    "    else:\n",
    "        print(f\"\\n{title} (Top {len(topics)}):\")\n",
    "        topics_to_print = topics\n",
    "\n",
    "    header = f\"{'Keyword':40s} {'Avg Score':>10s}\"\n",
    "    if show_polarity:\n",
    "        header += f\" {'Avg Polarity':>14s}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for kw, score, pol in topics_to_print:\n",
    "        line = f\"{kw:40s} {score:10.4f}\"\n",
    "        if show_polarity:\n",
    "            line += f\" {pol:14.3f}\"\n",
    "        print(line)\n",
    "\n",
    "# Display both models’ top keywords\n",
    "print_topics(\"Top 5 Diverse Keywords - Base KeyBERT\", top_base, show_polarity=False)\n",
    "print_topics(\"Top 5 Diverse Keywords - Sentiment-Aware KeyBERT\", top_sent, show_polarity=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "399c8605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flop 5 Keywords - Base KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score\n",
      "---------------------------------------------------\n",
      "combines compelling narrative                0.4711\n",
      "dialogue felt forced                         0.4803\n",
      "slow dialogue felt                           0.5029\n",
      "heartfelt director                           0.5045\n",
      "director vision shines                       0.5103\n",
      "\n",
      "Flop 5 Keywords - Sentiment-Aware KeyBERT (Flop 5):\n",
      "Keyword                                   Avg Score   Avg Polarity\n",
      "------------------------------------------------------------------\n",
      "unnatural struggled stay                     0.6130          0.095\n",
      "truly inspiring                              0.6159          0.972\n",
      "unnatural struggled                          0.6184          0.085\n",
      "narrative excellent                          0.6246          0.875\n",
      "utter disaster                               0.6258          0.066\n"
     ]
    }
   ],
   "source": [
    "# Select flop 5 keywords (lowest ranked) for each model\n",
    "flop_base = base_ranked[-5:]\n",
    "flop_sent = sent_ranked[-5:]\n",
    "\n",
    "# Disabling diversity filtering for flop: simply take the flop keywords as is\n",
    "flop_base_diverse = [(kw, base_avg_scores[kw], base_avg_pols.get(kw, float('nan'))) for kw, _ in flop_base]\n",
    "flop_sent_diverse = [(kw, sent_avg_scores[kw], sent_avg_pols.get(kw, float('nan'))) for kw, _ in flop_sent]\n",
    "\n",
    "# Print flop keywords\n",
    "print_topics(\"Flop 5 Keywords - Base KeyBERT\", flop_base_diverse, show_polarity=False, flop=True)\n",
    "print_topics(\"Flop 5 Keywords - Sentiment-Aware KeyBERT\", flop_sent_diverse, show_polarity=True, flop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f60256",
   "metadata": {},
   "source": [
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308cd0f",
   "metadata": {},
   "source": [
    "The comparison highlights key differences between the base KeyBERT and the sentiment-aware extension:\n",
    "\n",
    "- **KeyBERT** selects keywords based purely on semantic similarity, often favoring neutral or generic terms that may not reflect the review's tone.\n",
    "- **Sentiment-aware KeyBERT** integrates sentiment polarity during both filtering and scoring, surfacing keywords that are emotionally aligned with each individual review.\n",
    "\n",
    "This results in:\n",
    "- More emotionally coherent keywords at both the single-review and aggregate level.\n",
    "- Penalization of sentimentally inconsistent keywords in globally positive (or negative) datasets.\n",
    "- Improved interpretability for applications where sentiment is important, such as opinion mining or customer feedback analysis.\n",
    "\n",
    "Incorporating sentiment enhances keyword relevance, emotional alignment, and the overall contextual quality of extracted topics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
