{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a77813",
   "metadata": {},
   "source": [
    "This notebook evaluates and compares different keyword extraction models applied to movie reviews. The focus is on understanding how well each model can extract meaningful keywords that align with the ground truth.\n",
    "\n",
    "We aim to assess the performance of two models:\n",
    "- **Base** KeyBERT model\n",
    "- **Metadata-enhanced** version that integrates additional information: KeyBERTMetadata\n",
    "\n",
    "The evaluation is performed on a set of reviews for a selected movie, where each model predicts a ranked list of top-5 keywords per review.\n",
    "\n",
    "The notebook uses:\n",
    "- A **ground truth dataset** of annotated keywords per movie retrieved from IMDB\n",
    "- **Model outputs**: lists of predicted keywords with associated confidence scores for each review\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Three types of evaluation are conducted:\n",
    "\n",
    "**1. Basic (Unweighted) Metrics**\n",
    "\n",
    "- **Precision**, **Recall**, and **F1-score** based on approximate binary matching\n",
    "- Each review is evaluated independently, and metrics are averaged\n",
    "\n",
    "**2. Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision/Recall/F1**: matches are weighted by the model’s confidence scores\n",
    "- **nDCG@5**: evaluates ranking quality of the predicted top-5 keywords\n",
    "\n",
    "**3. Semantic Evaluation (Embedding-Based)**\n",
    "\n",
    "- Instead of using strict text matching, this evaluation uses **sentence-transformer embeddings** to compute **cosine similarity** between predicted and ground truth keywords.\n",
    "- A predicted keyword is considered correct if its similarity to any ground truth keyword exceeds a given threshold (e.g., 0.75).\n",
    "- We then compute **semantic precision, recall, and F1-score** based on these soft matches.\n",
    "\n",
    "### Why Not BERTScore?\n",
    "\n",
    "Although **BERTScore** is a powerful metric for evaluating textual similarity, it is designed for **long-form text comparisons** (e.g., full sentences or summaries). In our case:\n",
    "\n",
    "- Each review contains only **5 short keywords**, making token-level matching less informative.\n",
    "\n",
    "- BERTScore requires **pairwise comparisons** with equal-sized candidate and reference sets, which does not align with our top-*k* ranking setup.\n",
    "\n",
    "- It is also computationally expensive and not optimized for many short sequences.\n",
    "\n",
    "Therefore, we adopt a **lightweight and more interpretable approach** using sentence embeddings and cosine similarity, tailored specifically to the **semantic similarity of keyword-level predictions**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers is already installed.\n",
      "tqdm is already installed.\n",
      "pandas is already installed.\n",
      "torch is already installed.\n",
      "numpy is already installed.\n",
      "Installing scikit-learn...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"scikit-learn\", \"tqdm\", \"transformers\", \"torch\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973bef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "import math    # Mathematical functions (e.g., logarithms for nDCG calculation)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Transformers and PyTorch for embeddings and models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9ee47",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c179",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55796a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602be9",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"SW_Episode6\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a94a",
   "metadata": {},
   "source": [
    "This block defines the baseline utility functions used to evaluate predicted keywords against the ground truth. These functions do **not** take into account keyword confidence scores or ranking—they perform **binary, unweighted evaluation**.\n",
    "\n",
    "Specifically, this implementation includes:\n",
    "\n",
    "- **Normalization**: keywords are converted to lowercase, stripped of punctuation, and cleaned of extra whitespace to ensure consistent matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule that considers two keywords as matching if they are identical or if one is a substring of the other (e.g., *\"social satire\"* ≈ *\"satire\"*).\n",
    "\n",
    "- **Evaluation**: standard metrics — **precision**, **recall**, and **F1-score** — are calculated based on the number of approximate matches between predicted and ground truth keywords.\n",
    "\n",
    "This provides a basic but interpretable way to assess keyword extraction quality without considering the ranking or confidence scores assigned by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches the ground truth exactly\n",
    "# or if either keyword contains the other as a substring\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Evaluation function for a single prediction instance:\n",
    "# - Normalizes both predicted and ground truth keywords\n",
    "# - Computes how many predicted keywords approximately match the ground truth\n",
    "# - Calculates precision, recall, and F1-score\n",
    "def evaluate_keywords(pred_keywords, gt_keywords):\n",
    "    pred_keywords = [normalize_kw(k) for k in pred_keywords]\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    \n",
    "    # Count how many predicted keywords match approximately with any ground truth keyword\n",
    "    match_count = sum([is_approx_match(k, gt_keywords) for k in pred_keywords])\n",
    "    \n",
    "    # Precision: percentage of predicted keywords that are correct\n",
    "    precision = match_count / len(pred_keywords) if pred_keywords else 0\n",
    "\n",
    "    # Recall: percentage of ground truth keywords that were correctly predicted\n",
    "    recall = match_count / len(gt_keywords) if gt_keywords else 0\n",
    "\n",
    "    # F1-score: harmonic mean of precision and recall\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0199",
   "metadata": {},
   "source": [
    "This section evaluates two keyword extraction models — **base** and **metadata-enhanced** — using the ground truth.\n",
    "\n",
    "For each **review**, basic precision, recall, and F1-score are computed based on binary keyword matching. These metrics are then **averaged across all reviews** to provide an overall performance comparison between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d333544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_198ee\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_198ee_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_198ee_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_198ee_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_198ee_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_198ee_row0_col0\" class=\"data row0 col0\" >0.5488</td>\n",
       "      <td id=\"T_198ee_row0_col1\" class=\"data row0 col1\" >0.0101</td>\n",
       "      <td id=\"T_198ee_row0_col2\" class=\"data row0 col2\" >0.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_198ee_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_198ee_row1_col0\" class=\"data row1 col0\" >0.4671</td>\n",
       "      <td id=\"T_198ee_row1_col1\" class=\"data row1 col1\" >0.0086</td>\n",
       "      <td id=\"T_198ee_row1_col2\" class=\"data row1 col2\" >0.0169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2bd558f80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Create a dictionary to store evaluation results (precision, recall, F1) for each model\n",
    "results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie (same for all reviews)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        \n",
    "        # Column name with the predicted keywords for this model\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Proceed only if the column exists and contains a list of predicted keywords\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "\n",
    "            # Extract only the keyword strings from (keyword, score) tuples\n",
    "            predicted_keywords = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            \n",
    "            # Evaluate the prediction using precision, recall, and F1-score\n",
    "            precision, recall, f1 = evaluate_keywords(predicted_keywords, ground_truth_keywords)\n",
    "            \n",
    "            # Store the result for this specific review\n",
    "            results[model].append({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "# Aggregate the results to compute average metrics for each model\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precisions = [r[\"precision\"] for r in results[model]]\n",
    "    recalls = [r[\"recall\"] for r in results[model]]\n",
    "    f1s = [r[\"f1\"] for r in results[model]]\n",
    "    \n",
    "    # Calculate the average of each metric and round to 4 decimal places\n",
    "    summary[model] = {\n",
    "        \"avg_precision\": round(np.mean(precisions), 4),\n",
    "        \"avg_recall\": round(np.mean(recalls), 4),\n",
    "        \"avg_f1\": round(np.mean(f1s), 4)\n",
    "    }\n",
    "\n",
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1-score\",\n",
    "]\n",
    "\n",
    "# Display the summary table nicely\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics and nDCG@k with Graded Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055848",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the model’s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage.\n",
    "\n",
    "- **nDCG@k (Normalized Discounted Cumulative Gain)**: A ranking metric that rewards placing relevant keywords near the top of the prediction list. It uses **graded relevance**, which accounts for the importance of ground truth keywords based on their position.\n",
    "\n",
    "#### How nDCG@k with Graded Relevance is Computed\n",
    "\n",
    "1. **Assign graded relevance to ground truth keywords** based on their position $pos_{GT}$ (starting from 0). The relevance for a ground truth keyword at position $pos_{GT}$ is:\n",
    "\n",
    "   $$\n",
    "   rel_{GT} = \\frac{1}{\\log_2(pos_{GT} + 2)}\n",
    "   $$\n",
    "\n",
    "   Higher ranked keywords (lower $pos_{GT}$) have higher relevance scores.\n",
    "\n",
    "2. **For each predicted keyword at position $i$ (starting from 0)**, find the best matching ground truth keyword (using approximate matching). Assign the relevance of the predicted keyword $rel_i$ as the graded relevance of its matched ground truth keyword:\n",
    "\n",
    "   $$\n",
    "   rel_i = \\begin{cases}\n",
    "   \\frac{1}{\\log_2(pos_{GT} + 2)} & \\text{if predicted keyword matches GT keyword at } pos_{GT} \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "3. **Compute Discounted Cumulative Gain (DCG) for the predicted keywords up to rank $k$**:\n",
    "\n",
    "   $$\n",
    "   DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i + 1)}\n",
    "   $$\n",
    "\n",
    "   This discounts the relevance by the predicted keyword’s position, rewarding relevant keywords ranked higher.\n",
    "\n",
    "4. **Compute Ideal DCG (IDCG)** as the maximum possible DCG using the top-$k$ ground truth keywords ranked by their graded relevance:\n",
    "\n",
    "   $$\n",
    "   IDCG@k = \\sum_{i=1}^{k} \\frac{rel^{*}_i}{\\log_2(i + 1)}\n",
    "   $$\n",
    "\n",
    "   where $rel^{*}_i$ are the graded relevance scores of the top-$k$ ground truth keywords sorted by importance.\n",
    "\n",
    "5. **Calculate normalized DCG (nDCG)** by dividing DCG by IDCG:\n",
    "\n",
    "   $$\n",
    "   nDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "   $$\n",
    "\n",
    "### Example ($k=5$)\n",
    "\n",
    "Ground truth keywords ranked by importance:  \n",
    "`[\"fraud\", \"poverty\", \"scam\"]`\n",
    "\n",
    "Their graded relevance:  \n",
    "- \"fraud\" at position 0 → $rel_{GT} = \\frac{1}{\\log_2(0 + 2)} = 1.0$  \n",
    "- \"poverty\" at position 1 → $rel_{GT} = \\frac{1}{\\log_2(1 + 2)} = 0.63$  \n",
    "- \"scam\" at position 2 → $rel_{GT} = \\frac{1}{\\log_2(2 + 2)} = 0.5$\n",
    "\n",
    "**Predicted keywords in order:**  \n",
    "`[\"scam\", \"family\", \"poverty\", \"cinematography\", \"fraud\"]`\n",
    "\n",
    "Matching relevances assigned to predicted keywords:  \n",
    "- \"scam\" matches GT at pos 2 → $rel_0 = 0.5$  \n",
    "- \"family\" no match → $rel_1 = 0$  \n",
    "- \"poverty\" matches GT at pos 1 → $rel_2 = 0.63$  \n",
    "- \"cinematography\" no match → $rel_3 = 0$  \n",
    "- \"fraud\" matches GT at pos 0 → $rel_4 = 1.0$\n",
    "\n",
    "Compute DCG:\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0.5}{\\log_2(1 + 1)} + \\frac{0}{\\log_2(2 + 1)} + \\frac{0.63}{\\log_2(3 + 1)} + \\frac{0}{\\log_2(4 + 1)} + \\frac{1.0}{\\log_2(5 + 1)} \\approx 0.5 + 0 + 0.315 + 0 + 0.387 = 1.202\n",
    "$$\n",
    "\n",
    "Compute IDCG (ideal predicted order: \"fraud\", \"poverty\", \"scam\"):\n",
    "\n",
    "$$\n",
    "IDCG = \\frac{1.0}{\\log_2(1 + 1)} + \\frac{0.63}{\\log_2(2 + 1)} + \\frac{0.5}{\\log_2(3 + 1)} = 1.0 + 0.397 + 0.25 = 1.647\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.202}{1.647} \\approx 0.73\n",
    "$$\n",
    "\n",
    "**Change predicted order to:**  \n",
    "`[\"fraud\", \"poverty\", \"scam\", \"family\", \"cinematography\"]`\n",
    "\n",
    "Relevances for predicted keywords:\n",
    "\n",
    "- \"fraud\" matches GT at pos 0 → $rel_0 = 1.0$  \n",
    "- \"poverty\" matches GT at pos 1 → $rel_1 = 0.63$  \n",
    "- \"scam\" matches GT at pos 2 → $rel_2 = 0.5$  \n",
    "- \"family\" no match → $rel_3 = 0$  \n",
    "- \"cinematography\" no match → $rel_4 = 0$\n",
    "\n",
    "Compute DCG:\n",
    "\n",
    "$$\n",
    "DCG = \\frac{1.0}{\\log_2(1 + 1)} + \\frac{0.63}{\\log_2(2 + 1)} + \\frac{0.5}{\\log_2(3 + 1)} + 0 + 0 = 1.0 + 0.397 + 0.25 = 1.647\n",
    "$$\n",
    "\n",
    "IDCG is the same as before.\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.647}{1.647} = 1.0\n",
    "$$\n",
    "\n",
    "Changing the order of predicted keywords **does affect** the nDCG score: placing highly relevant keywords earlier leads to higher nDCG, reflecting better ranking quality.\n",
    "\n",
    "- When relevant keywords appear early in the predicted list, the score increases due to less discounting.\n",
    "- Conversely, when relevant keywords are placed lower, the score decreases because of higher discounting.\n",
    "- This metric thus rewards **both correct prediction and the quality of their ranking**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches any ground truth keyword\n",
    "# using a relaxed comparison: exact match or substring containment\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Weighted evaluation function:\n",
    "# Calculates precision, recall, and F1-score using the confidence scores of predicted keywords\n",
    "# - High-confidence correct predictions contribute more\n",
    "# - Precision is score-weighted; recall divides by total ground truth\n",
    "def evaluate_keywords_weighted(predicted_kw_score, gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate predicted keywords with confidence scores using weighted precision, recall, and F1.\n",
    "    \n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with associated confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords (annotated)\n",
    "    \n",
    "    Returns:\n",
    "        (precision, recall, f1): all metrics computed using score-weighted matching\n",
    "    \"\"\"\n",
    "    # Normalize both predicted and ground truth keywords\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords = [(normalize_kw(kw), score) for kw, score in predicted_kw_score if isinstance(kw, str)]\n",
    "    \n",
    "    total_score = sum(score for _, score in pred_keywords)\n",
    "    if total_score == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # Compute total score of predicted keywords that approximately match the ground truth\n",
    "    match_score = sum(score for kw, score in pred_keywords if is_approx_match(kw, gt_keywords))\n",
    "    \n",
    "    # Weighted precision and recall\n",
    "    precision = match_score / total_score\n",
    "    recall = match_score / len(gt_keywords) if gt_keywords else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Ranking-based evaluation function:\n",
    "# Computes the normalized Discounted Cumulative Gain (nDCG@k) for predicted keywords\n",
    "# Gives more credit when correct keywords appear earlier in the ranking\n",
    "def compute_ndcg(predicted_kw_score, gt_keywords, k=5):\n",
    "    \"\"\"\n",
    "    Compute nDCG@k between predicted keywords (with scores) and ground truth keywords,\n",
    "    using graded relevance based on ground truth ranking and approximate matching.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords ordered by importance\n",
    "        k (int): top-k keywords to consider\n",
    "\n",
    "    Returns:\n",
    "        float: normalized DCG score\n",
    "    \"\"\"\n",
    "    # Normalize ground truth and predicted keywords\n",
    "    gt_keywords_norm = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords_norm = [normalize_kw(kw) for kw, _ in predicted_kw_score[:k]]\n",
    "\n",
    "    relevance = []\n",
    "    for pk in pred_keywords_norm:\n",
    "        # Find ranks of all GT keywords matching predicted keyword approx.\n",
    "        match_ranks = [i for i, gk in enumerate(gt_keywords_norm) if is_approx_match(pk, [gk])]\n",
    "        if match_ranks:\n",
    "            # Assign relevance inversely proportional to rank (log discount)\n",
    "            best_rank = min(match_ranks)\n",
    "            rel = 1 / math.log2(best_rank + 2)  # +2 since ranks start at 0\n",
    "        else:\n",
    "            rel = 0\n",
    "        relevance.append(rel)\n",
    "\n",
    "    # Compute DCG with graded relevance\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # Compute ideal DCG (IDCG) assuming best possible ordering (top k GT keywords)\n",
    "    ideal_relevance = [1 / math.log2(i + 2) for i in range(min(k, len(gt_keywords_norm)))]\n",
    "    idcg = sum(ideal_relevance)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80dd7",
   "metadata": {},
   "source": [
    "In this section, we apply the score-aware evaluation metrics to each review for both models:\n",
    "\n",
    "- **Weighted Precision, Recall, F1**: accounts for the confidence scores of each predicted keyword.\n",
    "- **nDCG@5**: evaluates the ranking quality of the top-5 keywords based on their alignment with the ground truth.\n",
    "\n",
    "Each review is evaluated individually, and the metrics are then averaged across all reviews to summarize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ac273\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ac273_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_ac273_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_ac273_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "      <th id=\"T_ac273_level0_col3\" class=\"col_heading level0 col3\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ac273_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_ac273_row0_col0\" class=\"data row0 col0\" >0.5539</td>\n",
       "      <td id=\"T_ac273_row0_col1\" class=\"data row0 col1\" >0.0052</td>\n",
       "      <td id=\"T_ac273_row0_col2\" class=\"data row0 col2\" >0.0102</td>\n",
       "      <td id=\"T_ac273_row0_col3\" class=\"data row0 col3\" >0.2272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ac273_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_ac273_row1_col0\" class=\"data row1 col0\" >0.4702</td>\n",
       "      <td id=\"T_ac273_row1_col1\" class=\"data row1 col1\" >0.0051</td>\n",
       "      <td id=\"T_ac273_row1_col2\" class=\"data row1 col2\" >0.0100</td>\n",
       "      <td id=\"T_ac273_row1_col3\" class=\"data row1 col3\" >0.1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2cf3215e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Initialize results dictionary for weighted metrics and nDCG\n",
    "weighted_results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Loop through each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = row[pred_col]  # list of (kw, score)\n",
    "\n",
    "            # Compute weighted metrics\n",
    "            w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, ground_truth_keywords)\n",
    "\n",
    "            # Compute nDCG@5\n",
    "            ndcg = compute_ndcg(predicted_kw_score, ground_truth_keywords, k=5)\n",
    "\n",
    "            # Save results\n",
    "            weighted_results[model].append({\n",
    "                \"weighted_precision\": w_precision,\n",
    "                \"weighted_recall\": w_recall,\n",
    "                \"weighted_f1\": w_f1,\n",
    "                \"ndcg@5\": ndcg\n",
    "            })\n",
    "\n",
    "# Compute average metrics across all reviews\n",
    "weighted_summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    metrics = weighted_results[model]\n",
    "    weighted_summary[model] = {\n",
    "        \"avg_weighted_precision\": round(np.mean([m[\"weighted_precision\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_recall\": round(np.mean([m[\"weighted_recall\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_f1\": round(np.mean([m[\"weighted_f1\"] for m in metrics]), 4),\n",
    "        \"avg_ndcg@5\": round(np.mean([m[\"ndcg@5\"] for m in metrics]), 4)\n",
    "    }\n",
    "\n",
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "    \"nDCG@5\"\n",
    "]\n",
    "\n",
    "# Display the summary table\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabef0de",
   "metadata": {},
   "source": [
    "## Semantic Evaluation (Base vs Metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16c67f",
   "metadata": {},
   "source": [
    "In this section, we evaluate and compare the **Base** and **Metadata-enhanced** keyword extraction models using a **semantic similarity approach** based on contextual embeddings.\n",
    "\n",
    "Traditional evaluation metrics typically check for exact or approximate string matches between predicted and ground truth keywords. However, this can miss semantically related terms that are lexically different but convey the same meaning — for example, *\"scam\"* and *\"fraud\"*.\n",
    "\n",
    "To overcome this limitation, we leverage **sentence embeddings** generated by a pre-trained transformer model (such as a sentence-transformer). Each keyword — both predicted and ground truth — is converted into a dense vector representation that captures its semantic content.\n",
    "\n",
    "How the semantic evaluation works in detail:\n",
    "\n",
    "1. **Embedding keywords**:  \n",
    "    Both predicted keywords and ground truth keywords are embedded into high-dimensional vectors using the same model. The ground truth keywords are embedded **once beforehand** to avoid redundant computations. These embeddings are normalized to ensure cosine similarity is a valid similarity measure.\n",
    "\n",
    "2. **Computing similarity scores**:  \n",
    "   We calculate the **cosine similarity** between every predicted keyword embedding and every ground truth keyword embedding, resulting in a similarity matrix.\n",
    "\n",
    "3. **Determining matches using a threshold**:  \n",
    "   A predicted keyword is considered a **semantic match** if its cosine similarity with at least one ground truth keyword exceeds a set threshold (e.g., 0.75). This threshold balances between strictness and flexibility in matching semantic content.\n",
    "\n",
    "4. **Calculating semantic precision**:  \n",
    "   This is the fraction of predicted keywords that have a semantic match in the ground truth. It reflects how many of the model’s predictions are meaningful and relevant.\n",
    "\n",
    "5. **Calculating semantic recall**:  \n",
    "   This is the fraction of ground truth keywords that are captured by semantically similar predicted keywords. It indicates how well the model covers the essential concepts of the ground truth.\n",
    "\n",
    "6. **Calculating semantic F1-score**:  \n",
    "   The harmonic mean of semantic precision and recall, providing a single measure that balances both aspects.\n",
    "\n",
    "By evaluating keyword extraction in this semantic space, the metric is more robust to lexical variation and better reflects the true relevance of the predictions. This provides a deeper understanding of how well each model captures the **meaning** behind the ground truth keywords, beyond surface-level text matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f779851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model from the SentenceTransformers family\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model to generate contextual embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "def embed_keywords(keywords, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute sentence embeddings for a list of keyword strings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    keywords : List[str]\n",
    "        A list of keyword strings to encode.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Normalized embeddings tensor of shape (num_keywords, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Return empty tensor if input list is empty\n",
    "    if not keywords:\n",
    "        return torch.empty(0, encoder.config.hidden_size).to(device)\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the encoder to get hidden states\n",
    "        outputs = encoder(**inputs)\n",
    "\n",
    "        # Use mean pooling on the last hidden state to get fixed-size embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity computations\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_semantic_metrics(pred_keywords, gt_embeddings, threshold=0.75, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute semantic precision, recall, and F1 score between predicted keywords and\n",
    "    ground truth embeddings based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    pred_keywords : List[str]\n",
    "        List of predicted keywords for a single review.\n",
    "    gt_embeddings : torch.Tensor\n",
    "        Pre-computed normalized embeddings of ground truth keywords.\n",
    "    threshold : float\n",
    "        Cosine similarity threshold above which a predicted keyword is considered\n",
    "        semantically matching a ground truth keyword.\n",
    "    device : str\n",
    "        Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    precision : float\n",
    "        Fraction of predicted keywords that match any ground truth keyword semantically.\n",
    "    recall : float\n",
    "        Fraction of ground truth keywords that are matched by any predicted keyword.\n",
    "    f1 : float\n",
    "        Harmonic mean of precision and recall.\n",
    "    \"\"\"\n",
    "    # Handle empty predictions or empty ground truth embeddings edge cases\n",
    "    if len(pred_keywords) == 0 or gt_embeddings.shape[0] == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute embeddings for the predicted keywords only\n",
    "    pred_emb = embed_keywords(pred_keywords, device=device)\n",
    "\n",
    "    # Compute cosine similarity matrix between predicted and ground truth embeddings\n",
    "    # Shape: (num_predicted_keywords, num_ground_truth_keywords)\n",
    "    sims = torch.matmul(pred_emb, gt_embeddings.T)\n",
    "\n",
    "    # A predicted keyword is counted as a match if it has cosine similarity above\n",
    "    # the threshold with at least one ground truth keyword\n",
    "    pred_matches = (sims > threshold).any(dim=1).float().sum().item()\n",
    "\n",
    "    # Similarly, a ground truth keyword is matched if any predicted keyword exceeds threshold\n",
    "    gt_matches = (sims > threshold).any(dim=0).float().sum().item()\n",
    "\n",
    "    # Calculate precision: matched predictions / total predictions\n",
    "    precision = pred_matches / len(pred_keywords)\n",
    "\n",
    "    # Calculate recall: matched ground truths / total ground truth keywords\n",
    "    recall = gt_matches / gt_embeddings.shape[0]\n",
    "\n",
    "    # Compute harmonic mean for F1 score, handling zero division\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce9bda",
   "metadata": {},
   "source": [
    "### Semantic Evaluation of Base and Metadata Models Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa566c",
   "metadata": {},
   "source": [
    "In this step, we evaluate the semantic similarity between the predicted keywords of two models — **Base** and **Metadata-enhanced** — and the ground truth keywords using **sentence embeddings**.\n",
    "\n",
    "Unlike previous evaluations based on exact or approximate matching, this method leverages contextual embeddings from a pre-trained transformer to measure how semantically close the predicted keywords are to the reference keywords.\n",
    "\n",
    "For each review:\n",
    "- We extract only the **text of the predicted keywords**, ignoring their confidence scores.\n",
    "- We compute **semantic precision, recall, and F1** based on cosine similarity between embeddings.\n",
    "- We then average these metrics across all reviews for each model, providing an overall **semantic performance assessment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbd11b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic metrics - base: 100%|██████████| 1016/1016 [00:15<00:00, 63.70it/s]\n",
      "Semantic metrics - metadata: 100%|██████████| 1016/1016 [00:16<00:00, 62.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6da77\">\n",
       "  <caption>Semantic-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6da77_level0_col0\" class=\"col_heading level0 col0\" >Semantic_Precision</th>\n",
       "      <th id=\"T_6da77_level0_col1\" class=\"col_heading level0 col1\" >Semantic_Recall</th>\n",
       "      <th id=\"T_6da77_level0_col2\" class=\"col_heading level0 col2\" >Semantic_F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6da77_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_6da77_row0_col0\" class=\"data row0 col0\" >0.3537</td>\n",
       "      <td id=\"T_6da77_row0_col1\" class=\"data row0 col1\" >0.0103</td>\n",
       "      <td id=\"T_6da77_row0_col2\" class=\"data row0 col2\" >0.0199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6da77_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_6da77_row1_col0\" class=\"data row1 col0\" >0.2748</td>\n",
       "      <td id=\"T_6da77_row1_col1\" class=\"data row1 col1\" >0.0078</td>\n",
       "      <td id=\"T_6da77_row1_col2\" class=\"data row1 col2\" >0.0151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2d136e420>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute embeddings for the ground truth keywords once per selected movie\n",
    "# This avoids redundant computation when comparing against multiple predicted keywords\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "gt_emb = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# List to collect semantic evaluation results for each model\n",
    "semantic_scores = []\n",
    "\n",
    "# Loop over each model to evaluate semantic metrics separately\n",
    "for model in models_to_evaluate:\n",
    "    # Lists to accumulate precision, recall, and F1 scores for each review\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "\n",
    "    # Iterate over each review (row) in the selected movie's predictions, with a progress bar\n",
    "    for _, row in tqdm(selected_film.iterrows(), total=len(selected_film), desc=f\"Semantic metrics - {model}\"):\n",
    "        pred_col = f\"keywords_{model}\"  # Column name for predicted keywords of the current model\n",
    "\n",
    "        # Check if the predicted keywords column exists and contains a list\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            # Extract only the keyword strings (ignore confidence scores)\n",
    "            pred_kw = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "\n",
    "            # Compute semantic precision, recall, and F1 between predicted keywords and precomputed GT embeddings\n",
    "            precision, recall, f1 = compute_semantic_metrics(pred_kw, gt_emb, device=device)\n",
    "\n",
    "            # Append the scores for aggregation later\n",
    "            all_precisions.append(precision)\n",
    "            all_recalls.append(recall)\n",
    "            all_f1s.append(f1)\n",
    "\n",
    "    # After processing all reviews, calculate average semantic scores for the current model\n",
    "    if all_f1s:\n",
    "        semantic_scores.append({\n",
    "            \"Model\": model,\n",
    "            \"Semantic_Precision\": round(sum(all_precisions) / len(all_precisions), 4),\n",
    "            \"Semantic_Recall\": round(sum(all_recalls) / len(all_recalls), 4),\n",
    "            \"Semantic_F1\": round(sum(all_f1s) / len(all_f1s), 4)\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame and set 'Model' as the index\n",
    "summary_df = pd.DataFrame(semantic_scores).set_index(\"Model\")\n",
    "\n",
    "# Display the semantic evaluation summary as a nicely formatted table with 4 decimal places\n",
    "summary_df.style.format(precision=4).set_caption(\"Semantic-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187183",
   "metadata": {},
   "source": [
    "This section automatically processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains predicted keywords generated by different models.\n",
    "\n",
    "For each movie:\n",
    "- The corresponding ground truth keywords are loaded.\n",
    "- Predicted keywords from both models — **Base** and **Metadata-enhanced** — are evaluated.\n",
    "- For each review, the following metrics are computed:\n",
    "\n",
    "  - **Unweighted Metrics**: Precision, Recall, and F1-score based on approximate matching.\n",
    "\n",
    "  - **Score-aware Metrics**: Weighted Precision, Weighted Recall, Weighted F1, and nDCG@5 to evaluate prediction confidence and ranking quality.\n",
    "  \n",
    "  - **Semantic Metrics**: Semantic Precision, Semantic Recall, and Semantic F1 computed using cosine similarity between sentence embeddings.\n",
    "\n",
    "Finally, the metrics are averaged per movie and per model, and compiled into a comprehensive summary table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc08dd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7e5e5\">\n",
       "  <caption>Full Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7e5e5_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_7e5e5_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_7e5e5_level0_col2\" class=\"col_heading level0 col2\" >Avg_Precision</th>\n",
       "      <th id=\"T_7e5e5_level0_col3\" class=\"col_heading level0 col3\" >Avg_Recall</th>\n",
       "      <th id=\"T_7e5e5_level0_col4\" class=\"col_heading level0 col4\" >Avg_F1</th>\n",
       "      <th id=\"T_7e5e5_level0_col5\" class=\"col_heading level0 col5\" >Avg_Weighted_Precision</th>\n",
       "      <th id=\"T_7e5e5_level0_col6\" class=\"col_heading level0 col6\" >Avg_Weighted_Recall</th>\n",
       "      <th id=\"T_7e5e5_level0_col7\" class=\"col_heading level0 col7\" >Avg_Weighted_F1</th>\n",
       "      <th id=\"T_7e5e5_level0_col8\" class=\"col_heading level0 col8\" >Avg_nDCG@5</th>\n",
       "      <th id=\"T_7e5e5_level0_col9\" class=\"col_heading level0 col9\" >Avg_Semantic_Precision</th>\n",
       "      <th id=\"T_7e5e5_level0_col10\" class=\"col_heading level0 col10\" >Avg_Semantic_Recall</th>\n",
       "      <th id=\"T_7e5e5_level0_col11\" class=\"col_heading level0 col11\" >Avg_Semantic_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7e5e5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7e5e5_row0_col0\" class=\"data row0 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_7e5e5_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_7e5e5_row0_col2\" class=\"data row0 col2\" >0.5488</td>\n",
       "      <td id=\"T_7e5e5_row0_col3\" class=\"data row0 col3\" >0.0101</td>\n",
       "      <td id=\"T_7e5e5_row0_col4\" class=\"data row0 col4\" >0.0198</td>\n",
       "      <td id=\"T_7e5e5_row0_col5\" class=\"data row0 col5\" >0.5539</td>\n",
       "      <td id=\"T_7e5e5_row0_col6\" class=\"data row0 col6\" >0.0052</td>\n",
       "      <td id=\"T_7e5e5_row0_col7\" class=\"data row0 col7\" >0.0102</td>\n",
       "      <td id=\"T_7e5e5_row0_col8\" class=\"data row0 col8\" >0.2272</td>\n",
       "      <td id=\"T_7e5e5_row0_col9\" class=\"data row0 col9\" >0.9116</td>\n",
       "      <td id=\"T_7e5e5_row0_col10\" class=\"data row0 col10\" >0.1918</td>\n",
       "      <td id=\"T_7e5e5_row0_col11\" class=\"data row0 col11\" >0.2882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7e5e5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7e5e5_row1_col0\" class=\"data row1 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_7e5e5_row1_col1\" class=\"data row1 col1\" >metadata</td>\n",
       "      <td id=\"T_7e5e5_row1_col2\" class=\"data row1 col2\" >0.4671</td>\n",
       "      <td id=\"T_7e5e5_row1_col3\" class=\"data row1 col3\" >0.0086</td>\n",
       "      <td id=\"T_7e5e5_row1_col4\" class=\"data row1 col4\" >0.0169</td>\n",
       "      <td id=\"T_7e5e5_row1_col5\" class=\"data row1 col5\" >0.4702</td>\n",
       "      <td id=\"T_7e5e5_row1_col6\" class=\"data row1 col6\" >0.0051</td>\n",
       "      <td id=\"T_7e5e5_row1_col7\" class=\"data row1 col7\" >0.0100</td>\n",
       "      <td id=\"T_7e5e5_row1_col8\" class=\"data row1 col8\" >0.1800</td>\n",
       "      <td id=\"T_7e5e5_row1_col9\" class=\"data row1 col9\" >0.8823</td>\n",
       "      <td id=\"T_7e5e5_row1_col10\" class=\"data row1 col10\" >0.1614</td>\n",
       "      <td id=\"T_7e5e5_row1_col11\" class=\"data row1 col11\" >0.2449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e2d12de030>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load the ground truth once for all movies\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# List of models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Store results across all movies\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all keyword prediction files\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords for the current movie\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Retrieve ground truth keywords for the selected movie\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Precompute GT embeddings once per movie for efficiency\n",
    "            gt_embeddings = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "            # Initialize metrics for each model\n",
    "            results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            # Evaluate each review in the dataset\n",
    "            for _, row in selected_film.iterrows():\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "\n",
    "                    # Skip if no predictions or wrong format\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "                        predicted_kw_score = row[pred_col]\n",
    "\n",
    "                        # Extract only keyword strings for unweighted evaluation\n",
    "                        pred_kw_only = [kw for kw, _ in predicted_kw_score if isinstance(kw, str)]\n",
    "\n",
    "                        # Compute basic (unweighted) metrics\n",
    "                        precision, recall, f1 = evaluate_keywords(pred_kw_only, gt_keywords)\n",
    "\n",
    "                        # Compute score-weighted metrics\n",
    "                        w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, gt_keywords)\n",
    "\n",
    "                        # Compute ranking-based metric (nDCG@5)\n",
    "                        ndcg = compute_ndcg(predicted_kw_score, gt_keywords, k=5)\n",
    "\n",
    "                        # Compute semantic metrics using embeddings\n",
    "                        semantic_precision, semantic_recall, semantic_f1 = compute_semantic_metrics(\n",
    "                            pred_kw_only, gt_embeddings, threshold=0.5, device=device\n",
    "                        )\n",
    "\n",
    "                        # Store all metrics for this review\n",
    "                        results[model].append({\n",
    "                            \"precision\": precision,\n",
    "                            \"recall\": recall,\n",
    "                            \"f1\": f1,\n",
    "                            \"w_precision\": w_precision,\n",
    "                            \"w_recall\": w_recall,\n",
    "                            \"w_f1\": w_f1,\n",
    "                            \"ndcg@5\": ndcg,\n",
    "                            \"semantic_precision\": semantic_precision,\n",
    "                            \"semantic_recall\": semantic_recall,\n",
    "                            \"semantic_f1\": semantic_f1\n",
    "                        })\n",
    "\n",
    "            # Compute average metrics per model for the current movie\n",
    "            for model in models_to_evaluate:\n",
    "                if results[model]:\n",
    "                    metrics_df = pd.DataFrame(results[model])\n",
    "                    all_results.append({\n",
    "                        \"Movie\": movie_name,\n",
    "                        \"Model\": model,\n",
    "                        \"Avg_Precision\": round(metrics_df[\"precision\"].mean(), 4),\n",
    "                        \"Avg_Recall\": round(metrics_df[\"recall\"].mean(), 4),\n",
    "                        \"Avg_F1\": round(metrics_df[\"f1\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Precision\": round(metrics_df[\"w_precision\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Recall\": round(metrics_df[\"w_recall\"].mean(), 4),\n",
    "                        \"Avg_Weighted_F1\": round(metrics_df[\"w_f1\"].mean(), 4),\n",
    "                        \"Avg_nDCG@5\": round(metrics_df[\"ndcg@5\"].mean(), 4),\n",
    "                        \"Avg_Semantic_Precision\": round(metrics_df[\"semantic_precision\"].mean(), 4),\n",
    "                        \"Avg_Semantic_Recall\": round(metrics_df[\"semantic_recall\"].mean(), 4),\n",
    "                        \"Avg_Semantic_F1\": round(metrics_df[\"semantic_f1\"].mean(), 4),\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create the final summary DataFrame\n",
    "final_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display table with clean index and sorted values\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Full Evaluation Summary per Movie and Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
