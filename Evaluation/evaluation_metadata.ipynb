{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a77813",
   "metadata": {},
   "source": [
    "This notebook evaluates and compares two different models:\n",
    "\n",
    "- **Base**: the standard KeyBERT model\n",
    "- **Metadata-enhanced**: an extended version, called *KeyBERTMetadata*, which integrates contextual information from the review metadata.\n",
    "\n",
    "Each model predicts a ranked list of top-5 keywords for each review. However, all evaluations in this notebook are **performed globally**, by aggregating predictions across all reviews of the same movie.\n",
    "\n",
    "The notebook uses:\n",
    "- A **ground truth dataset** containing annotated keywords per movie, derived from IMDB.\n",
    "- **Model predictions**: for each movie, a set of predicted keywords with associated confidence scores is extracted across all reviews.\n",
    "\n",
    "We conduct three types of global evaluation:\n",
    "\n",
    "#### **1. Basic (Unweighted) Metrics**\n",
    "\n",
    "- **Precision**, **Recall**, and **F1-score** are computed based on exact (normalized) string matching between predicted and ground truth keywords.\n",
    "- Predictions are aggregated across all reviews, and metrics are calculated on the global set of unique keywords per model.\n",
    "\n",
    "#### **2. Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**: keywords are matched against ground truth, weighting each match by the model’s confidence score.\n",
    "- **nDCG@5** (Normalized Discounted Cumulative Gain): evaluates how well the model ranks relevant keywords within its top-5 predictions.\n",
    "\n",
    "#### **3. Semantic Evaluation (Embedding-Based)**\n",
    "\n",
    "- Predicted and ground truth keywords are encoded using **sentence-transformer embeddings**.\n",
    "- **Cosine similarity** is used to identify soft matches between keywords.\n",
    "- A predicted keyword is considered correct if its similarity with any ground truth keyword exceeds a threshold (e.g., 0.75).\n",
    "- Based on these matches, we compute **semantic precision, recall, and F1-score**.\n",
    "\n",
    "**Why Not Use BERTScore?**\n",
    "\n",
    "Although **BERTScore** is a powerful metric for evaluating textual similarity, it is designed for **long-form text comparisons** (e.g., sentences or summaries). It is **not appropriate for evaluating keyword-level predictions**, for several reasons:\n",
    "\n",
    "- Each model generates **short keyword lists**, where token-level similarity is not meaningful.\n",
    "- BERTScore expects **equal-length** candidate and reference sequences, which is incompatible with the top-*k* keyword setting.\n",
    "- It is computationally intensive and less interpretable for sparse keyword evaluation.\n",
    "\n",
    "Instead, we adopt a **faster and more interpretable approach** using sentence embeddings and cosine similarity, tailored specifically to **keyword-level semantic evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy is already installed.\n",
      "tqdm is already installed.\n",
      "torch is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers is already installed.\n",
      "pandas is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"tqdm\", \"transformers\", \"torch\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973bef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "import math    # Mathematical functions (e.g., logarithms for nDCG calculation)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops\n",
    "\n",
    "# Transformers and PyTorch for embeddings and models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9ee47",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c179",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55796a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602be9",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"SW_Episode6\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a94a",
   "metadata": {},
   "source": [
    "This block defines the core utility functions used to evaluate predicted keywords against the ground truth. These functions perform a **binary, unweighted evaluation**, ignoring confidence scores and ranking information.\n",
    "\n",
    "The evaluation pipeline includes the following steps:\n",
    "\n",
    "- **Normalization**: all keywords are lowercased, stripped of punctuation, and cleaned of extra whitespace to ensure consistent text matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule considers two keywords as a match if:\n",
    "  - They are exactly equal (after normalization), or\n",
    "  - One is a substring of the other (e.g., *\"social satire\"* is considered a match with *\"satire\"*).\n",
    "\n",
    "- **Global Evaluation**: for each model, all keywords predicted across the reviews of a given movie are aggregated, and then compared to the global set of ground truth keywords for that movie.\n",
    "\n",
    "- **Metrics**: we compute **Precision**, **Recall**, and **F1-score** based on the number of approximate matches between the predicted and ground truth keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_keywords(all_pred_keywords, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global precision, recall, and F1-score across a dataset using approximate matching.\n",
    "\n",
    "    This function compares predicted keywords to ground truth keywords for each review.\n",
    "    Matching is performed using approximate string comparison, and each ground truth keyword\n",
    "    can be matched only once to ensure fairness. The metrics are aggregated globally,\n",
    "    not per-review.\n",
    "\n",
    "    Args:\n",
    "        all_pred_keywords (List[List[str]]): \n",
    "            A list where each element is a list of predicted keywords for a single review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list where each element is a list of ground truth keywords for the corresponding review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Global precision, recall, and F1-score based on approximate matching.\n",
    "    \"\"\"\n",
    "    global_match_count = 0     # Total number of matched keywords across all reviews\n",
    "    global_pred_count = 0      # Total number of predicted keywords\n",
    "    global_gt_count = 0        # Total number of ground truth keywords\n",
    "\n",
    "    # Iterate through each review's predictions and ground truths\n",
    "    for pred_keywords, gt_keywords in zip(all_pred_keywords, all_gt_keywords):\n",
    "        # Normalize and sort keywords to ensure consistent behavior\n",
    "        pred_keywords = sorted([normalize_kw(k) for k in pred_keywords])\n",
    "        gt_keywords = sorted([normalize_kw(k) for k in gt_keywords])\n",
    "\n",
    "        global_pred_count += len(pred_keywords)\n",
    "        global_gt_count += len(gt_keywords)\n",
    "\n",
    "        matched_gts = set()  # Track which ground truth keywords have already been matched\n",
    "\n",
    "        for pred in pred_keywords:\n",
    "            for gt in gt_keywords:\n",
    "                if gt not in matched_gts and is_approx_match(pred, [gt]):\n",
    "                    global_match_count += 1\n",
    "                    matched_gts.add(gt)  # Avoid matching the same GT keyword multiple times\n",
    "                    break\n",
    "\n",
    "    # Compute global metrics\n",
    "    precision = global_match_count / global_pred_count if global_pred_count else 0\n",
    "    recall = global_match_count / global_gt_count if global_gt_count else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0199",
   "metadata": {},
   "source": [
    "This section evaluates two keyword extraction models — **base** and **metadata-enhanced** — against the ground truth annotations.\n",
    "\n",
    "For each model, we collect all predicted keywords across all reviews in the selected movie and compare them to the ground truth keywords using **binary approximate matching**.\n",
    "\n",
    "The evaluation computes **global precision, recall, and F1-score**, considering the entire set of predictions and ground truth keywords as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d333544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c95fa\">\n",
       "  <caption>Global Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c95fa_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_c95fa_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_c95fa_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c95fa_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_c95fa_row0_col0\" class=\"data row0 col0\" >0.9199</td>\n",
       "      <td id=\"T_c95fa_row0_col1\" class=\"data row0 col1\" >0.3577</td>\n",
       "      <td id=\"T_c95fa_row0_col2\" class=\"data row0 col2\" >0.5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c95fa_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_c95fa_row1_col0\" class=\"data row1 col0\" >0.9206</td>\n",
       "      <td id=\"T_c95fa_row1_col1\" class=\"data row1 col1\" >0.3580</td>\n",
       "      <td id=\"T_c95fa_row1_col2\" class=\"data row1 col2\" >0.5155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14d781370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie\n",
    "ground_truth_keywords = [normalize_kw(kw) for kw in kw_ground_truth[\"Keyword\"].tolist()]\n",
    "\n",
    "# Dictionary to store all predicted keywords per model (across all reviews)\n",
    "all_predictions = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_keywords = [\n",
    "                normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)\n",
    "            ]\n",
    "            \n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_kw = [kw for kw in predicted_keywords if kw not in seen and not seen.add(kw)]\n",
    "\n",
    "            all_predictions[model].append(unique_kw)\n",
    "\n",
    "# Evaluate each model globally\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precision, recall, f1 = evaluate_keywords(\n",
    "        all_predictions[model],  # List of lists\n",
    "        ground_truth_keywords\n",
    "    )\n",
    "\n",
    "    summary[model] = {\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Convert and display\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "summary_df.columns = [\"Precision\", \"Recall\", \"F1-score\"]\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics and nDCG@k with Graded Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055848",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### **Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the model’s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage.\n",
    "- **nDCG@k (Normalized Discounted Cumulative Gain)**: A ranking metric that rewards placing relevant keywords near the top of the prediction list. It uses **graded relevance**, which accounts for the importance of ground truth keywords based on their position.\n",
    "\n",
    "#### **How nDCG@k with Graded Relevance is Computed**\n",
    "\n",
    "1. **Assign graded relevance to ground truth keywords** based on their position $pos_{GT}$ (starting from 0):\n",
    "\n",
    "$$\n",
    "rel_{GT} = \\frac{1}{\\log_2(pos_{GT} + 2)}\n",
    "$$\n",
    "\n",
    "2. **Assign relevance to each predicted keyword at position $i$** (starting from 0), using approximate matching:\n",
    "\n",
    "$$\n",
    "rel_i = \\begin{cases}\n",
    "\\frac{1}{\\log_2(pos_{GT} + 2)} & \\text{if predicted keyword matches GT keyword at } pos_{GT} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. **Compute DCG@k** (Discounted Cumulative Gain):\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=0}^{k-1} \\frac{rel_i}{\\log_2(i + 2)}\n",
    "$$\n",
    "\n",
    "4. **Compute IDCG@k** (Ideal DCG using the best ranking):\n",
    "\n",
    "$$\n",
    "IDCG@k = \\sum_{i=0}^{k-1} \\frac{rel^*_i}{\\log_2(i + 2)}\n",
    "$$\n",
    "\n",
    "5. **Compute normalized nDCG**:\n",
    "\n",
    "$$\n",
    "nDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "#### **Example ($k=5$)**\n",
    "\n",
    "**Ground truth keywords (ranked):**  \n",
    "`[\"fraud\", \"poverty\", \"scam\"]`\n",
    "\n",
    "**Their graded relevance (using $rel_{GT} = 1/\\log_2(pos_{GT}+2)$):**\n",
    "\n",
    "- fraud (position 0): $1 / \\log_2(0+2) = 1.0$\n",
    "- poverty (position 1): $1 / \\log_2(1+2) \\approx 0.6309$\n",
    "- scam (position 2): $1 / \\log_2(2+2) = 0.5$\n",
    "\n",
    "#### **First predicted list**:\n",
    "`[\"scam\", \"family\", \"poverty\", \"cinematography\", \"fraud\"]`\n",
    "\n",
    "**Matches and assigned relevances:**\n",
    "\n",
    "| Predicted keyword | Match         | Relevance |\n",
    "|-------------------|---------------|-----------|\n",
    "| scam              | yes (pos 2)   | 0.5       |\n",
    "| family            | no            | 0         |\n",
    "| poverty           | yes (pos 1)   | 0.6309    |\n",
    "| cinematography    | no            | 0         |\n",
    "| fraud             | yes (pos 0)   | 1.0       |\n",
    "\n",
    "**Compute DCG:**\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0.5}{\\log_2(0 + 2)} + \\frac{0}{\\log_2(1 + 2)} + \\frac{0.6309}{\\log_2(2 + 2)} + \\frac{0}{\\log_2(3 + 2)} + \\frac{1.0}{\\log_2(4 + 2)} \\\\\n",
    "= \\frac{0.5}{1.0} + 0 + \\frac{0.6309}{2.0} + 0 + \\frac{1.0}{2.58496} \\approx 0.5 + 0 + 0.31545 + 0 + 0.38685 = \\mathbf{1.2023}\n",
    "$$\n",
    "\n",
    "**Compute IDCG:**\n",
    "\n",
    "Best possible ranking: `[\"fraud\", \"poverty\", \"scam\"]`  \n",
    "Relevance list: $[1.0, 0.6309, 0.5]$\n",
    "\n",
    "$$\n",
    "IDCG = \\frac{1.0}{\\log_2(0 + 2)} + \\frac{0.6309}{\\log_2(1 + 2)} + \\frac{0.5}{\\log_2(2 + 2)} \\\\\n",
    "= \\frac{1.0}{1.0} + \\frac{0.6309}{1.58496} + \\frac{0.5}{2.0} \\approx 1.0 + 0.3979 + 0.25 = \\mathbf{1.6479}\n",
    "$$\n",
    "\n",
    "**nDCG@5:**\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.2023}{1.6479} \\approx \\mathbf{0.7294}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Second predicted list**:\n",
    "`[\"fraud\", \"poverty\", \"scam\", \"family\", \"cinematography\"]`\n",
    "\n",
    "**All matches in top-3, correct order:**\n",
    "\n",
    "| Predicted keyword | Match         | Relevance |\n",
    "|-------------------|---------------|-----------|\n",
    "| fraud             | yes (pos 0)   | 1.0       |\n",
    "| poverty           | yes (pos 1)   | 0.6309    |\n",
    "| scam              | yes (pos 2)   | 0.5       |\n",
    "| family            | no            | 0         |\n",
    "| cinematography    | no            | 0         |\n",
    "\n",
    "**Compute DCG:**\n",
    "\n",
    "$$\n",
    "DCG = \\frac{1.0}{\\log_2(0 + 2)} + \\frac{0.6309}{\\log_2(1 + 2)} + \\frac{0.5}{\\log_2(2 + 2)} + 0 + 0 \\\\\n",
    "= 1.0 + 0.3979 + 0.25 = \\mathbf{1.6479}\n",
    "$$\n",
    "\n",
    "**nDCG@5:**\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.6479}{1.6479} = \\mathbf{1.0}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Interpretation**\n",
    "\n",
    "- When relevant keywords appear early in the predicted list, the score increases due to less discounting.\n",
    "- When relevant keywords are ranked lower, the score decreases due to higher discounting.\n",
    "- **nDCG@k rewards both correct predictions and their correct ranking**, making it suitable for evaluating keyword extractors that produce ranked lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def evaluate_keywords_weighted(all_predicted_kw_score, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global weighted precision, recall, and F1-score across multiple reviews.\n",
    "\n",
    "    This function accounts for confidence scores assigned to predicted keywords.\n",
    "    Matching is performed using approximate matching. Each keyword score contributes\n",
    "    to the precision and recall based on whether it matches a ground truth keyword.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Weighted precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    total_score = 0.0         # Sum of all predicted keyword scores\n",
    "    match_score = 0.0         # Sum of scores of correctly predicted keywords\n",
    "    total_gt = 0              # Total number of ground truth keywords across all reviews\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize keywords\n",
    "        gt_kw = [normalize_kw(k) for k in gt_kw]\n",
    "        pred_kw_score = [\n",
    "            (normalize_kw(kw), score) for kw, score in pred_kw_score if isinstance(kw, str)\n",
    "        ]\n",
    "\n",
    "        total_score += sum(score for _, score in pred_kw_score)\n",
    "        total_gt += len(gt_kw)\n",
    "\n",
    "        matched_gts = set()  # Track ground truth keywords already matched\n",
    "\n",
    "        for kw, score in pred_kw_score:\n",
    "            for gt in gt_kw:\n",
    "                if gt not in matched_gts and is_approx_match(kw, [gt]):\n",
    "                    match_score += score\n",
    "                    matched_gts.add(gt)\n",
    "                    break\n",
    "\n",
    "    precision = match_score / total_score if total_score > 0 else 0\n",
    "    recall = match_score / total_gt if total_gt > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def compute_global_ndcg(all_predicted_kw_score, all_gt_keywords, k=5):\n",
    "    \"\"\"\n",
    "    Compute global average nDCG@k (Normalized Discounted Cumulative Gain) over multiple reviews.\n",
    "\n",
    "    The relevance of each predicted keyword is based on the position of its best matching\n",
    "    ground truth keyword. Matching is done via approximate matching. The ideal DCG assumes\n",
    "    the best possible ranking of ground truth keywords.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review (ranked list).\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "        k (int): The number of top predicted keywords to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average nDCG@k across all reviews.\n",
    "    \"\"\"\n",
    "    total_ndcg = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize predicted and ground truth keywords\n",
    "        gt_keywords_norm = [normalize_kw(k) for k in gt_kw]\n",
    "        pred_keywords_norm = [normalize_kw(kw) for kw, _ in pred_kw_score[:k]]\n",
    "\n",
    "        relevance = []  # Relevance scores assigned to predicted keywords\n",
    "\n",
    "        for pk in pred_keywords_norm:\n",
    "            # Find the best (earliest) match position in the GT list\n",
    "            match_ranks = [\n",
    "                i for i, gk in enumerate(gt_keywords_norm) if is_approx_match(pk, [gk])\n",
    "            ]\n",
    "            if match_ranks:\n",
    "                best_rank = min(match_ranks)\n",
    "                rel = 1 / math.log2(best_rank + 2)  # Graded relevance\n",
    "            else:\n",
    "                rel = 0\n",
    "            relevance.append(rel)\n",
    "\n",
    "        # Compute DCG for predicted keywords\n",
    "        dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "        # Compute IDCG based on ideal ordering of GT keywords\n",
    "        ideal_relevance = [1 / math.log2(i + 2) for i in range(min(k, len(gt_keywords_norm)))]\n",
    "        idcg = sum(ideal_relevance)\n",
    "\n",
    "        if idcg > 0:\n",
    "            total_ndcg += dcg / idcg\n",
    "            count += 1\n",
    "\n",
    "    return total_ndcg / count if count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80dd7",
   "metadata": {},
   "source": [
    "In this section, we evaluate the overall performance of each model using **score-aware metrics** computed **globally across all reviews**:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**: These metrics incorporate the **confidence scores** assigned to each predicted keyword, reflecting how much of the model’s confidence is placed on correct predictions.\n",
    "- **nDCG@5 (Normalized Discounted Cumulative Gain)**: Assesses the overall **ranking quality** of the top-5 predicted keywords, rewarding correct keywords that are ranked higher.\n",
    "\n",
    "This global evaluation provides a holistic view of each model’s effectiveness in ranking and selecting relevant keywords across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_32de2\">\n",
       "  <caption>Global Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_32de2_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_32de2_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_32de2_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "      <th id=\"T_32de2_level0_col3\" class=\"col_heading level0 col3\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_32de2_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_32de2_row0_col0\" class=\"data row0 col0\" >0.9155</td>\n",
       "      <td id=\"T_32de2_row0_col1\" class=\"data row0 col1\" >0.1789</td>\n",
       "      <td id=\"T_32de2_row0_col2\" class=\"data row0 col2\" >0.2993</td>\n",
       "      <td id=\"T_32de2_row0_col3\" class=\"data row0 col3\" >0.6973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_32de2_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_32de2_row1_col0\" class=\"data row1 col0\" >0.9181</td>\n",
       "      <td id=\"T_32de2_row1_col1\" class=\"data row1 col1\" >0.2085</td>\n",
       "      <td id=\"T_32de2_row1_col2\" class=\"data row1 col2\" >0.3398</td>\n",
       "      <td id=\"T_32de2_row1_col3\" class=\"data row1 col3\" >0.7169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x103397c10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Prepare data structures to hold predictions for each model\n",
    "all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Collect predictions and GT for each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = [(kw, score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "            all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "\n",
    "# Dictionary to store global evaluation results\n",
    "weighted_summary = {}\n",
    "\n",
    "# Evaluate each model globally\n",
    "for model in models_to_evaluate:\n",
    "    preds = all_predicted_kw_score[model]\n",
    "\n",
    "    # Global weighted metrics\n",
    "    w_precision, w_recall, w_f1 = evaluate_keywords_weighted(preds, ground_truth_keywords)\n",
    "\n",
    "    # Global nDCG@5\n",
    "    ndcg = compute_global_ndcg(preds, ground_truth_keywords, k=5)\n",
    "\n",
    "    # Store results\n",
    "    weighted_summary[model] = {\n",
    "        \"weighted_precision\": round(w_precision, 4),\n",
    "        \"weighted_recall\": round(w_recall, 4),\n",
    "        \"weighted_f1\": round(w_f1, 4),\n",
    "        \"ndcg@5\": round(ndcg, 4)\n",
    "    }\n",
    "\n",
    "# Convert summary to DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Models as rows\n",
    "\n",
    "# Rename columns\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "    \"nDCG@5\"\n",
    "]\n",
    "\n",
    "# Display final table\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabef0de",
   "metadata": {},
   "source": [
    "## Semantic Evaluation (Base vs Metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16c67f",
   "metadata": {},
   "source": [
    "In this section, we evaluate and compare the **Base** and **Metadata-enhanced** keyword extraction models using a **semantic similarity approach** based on contextual embeddings.\n",
    "\n",
    "Traditional evaluation metrics rely on exact or approximate string matching between predicted and ground truth keywords. However, this approach may miss semantically related terms that differ lexically but convey the same meaning — such as *\"scam\"* and *\"fraud\"*.\n",
    "\n",
    "To address this limitation, we adopt a **global semantic evaluation**, where all predicted and ground truth keywords across the dataset are compared using **dense sentence embeddings** generated by a pre-trained transformer (e.g., Sentence-BERT).\n",
    "\n",
    "#### **Semantic Evaluation Procedure**\n",
    "\n",
    "1. **Embedding Keywords Globally**  \n",
    "   All predicted and ground truth keywords across all reviews are embedded into high-dimensional vectors using the same transformer model. Ground truth keywords are embedded **once**, and all vectors are normalized to allow cosine similarity comparisons.\n",
    "\n",
    "2. **Computing Similarity Matrix**  \n",
    "   For each model, we compute a cosine similarity matrix between **all predicted keywords** and **all ground truth keywords**.\n",
    "\n",
    "3. **Matching Threshold**  \n",
    "   A predicted keyword is considered a **semantic match** if its cosine similarity with at least one ground truth keyword exceeds a fixed threshold (e.g., **0.75**). This allows for flexible yet meaningful semantic alignment.\n",
    "\n",
    "4. **Global Semantic Precision**  \n",
    "   The proportion of predicted keywords that have at least one semantic match in the ground truth. This reflects how many of the model's predictions are semantically relevant.\n",
    "\n",
    "5. **Global Semantic Recall**  \n",
    "   The proportion of ground truth keywords that are captured by semantically similar predictions. This indicates how well the model covers the key concepts.\n",
    "\n",
    "6. **Global Semantic F1-score**  \n",
    "   The harmonic mean of semantic precision and recall, summarizing both relevance and coverage into a single score.\n",
    "\n",
    "This evaluation:\n",
    "\n",
    "- Is **more robust** than string-based metrics.\n",
    "- **Captures meaning**, not just surface forms.\n",
    "- Helps evaluate models that paraphrase or generalize beyond exact matches.\n",
    "\n",
    "This evaluation complements previous metrics and provides a more **realistic estimate** of how well the models capture the essence of user-annotated keywords in a global and context-aware manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f779851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model from the SentenceTransformers family\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model to generate contextual embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "def embed_keywords(keywords, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute sentence embeddings for a list of keyword strings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    keywords : List[str]\n",
    "        A list of keyword strings to encode.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Normalized embeddings tensor of shape (num_keywords, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Return empty tensor if input list is empty\n",
    "    if not keywords:\n",
    "        return torch.empty(0, encoder.config.hidden_size).to(device)\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the encoder to get hidden states\n",
    "        outputs = encoder(**inputs)\n",
    "\n",
    "        # Use mean pooling on the last hidden state to get fixed-size embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity computations\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def evaluate_semantic_keywords_global(all_pred_keywords, gt_keywords, threshold=0.75, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute global semantic precision, recall, and F1 score between all predicted keywords\n",
    "    and ground truth keywords using cosine similarity over embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    all_pred_keywords : List[List[str]]\n",
    "        List of predicted keywords for each review.\n",
    "    gt_keywords : List[str]\n",
    "        Global list of ground truth keywords for the movie.\n",
    "    threshold : float\n",
    "        Cosine similarity threshold for considering a match.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    precision : float\n",
    "    recall : float\n",
    "    f1 : float\n",
    "    \"\"\"\n",
    "    # Early return if either set is empty\n",
    "    if len(all_pred_keywords) == 0 or len(gt_keywords) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute embeddings\n",
    "    pred_emb = embed_keywords(all_pred_keywords, device=device)\n",
    "    gt_emb = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    sims = torch.matmul(pred_emb, gt_emb.T)\n",
    "\n",
    "    # Match counting based on threshold\n",
    "    pred_matches = (sims > threshold).any(dim=1).float().sum().item()\n",
    "    gt_matches = (sims > threshold).any(dim=0).float().sum().item()\n",
    "\n",
    "    precision = pred_matches / len(all_pred_keywords)\n",
    "    recall = gt_matches / len(gt_keywords)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce9bda",
   "metadata": {},
   "source": [
    "### Semantic Evaluation of Base and Metadata Models Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa566c",
   "metadata": {},
   "source": [
    "In this step, we evaluate the **semantic similarity** between the predicted keywords of two models — **Base** and **Metadata-enhanced** — and the ground truth keywords using **sentence embeddings**.\n",
    "\n",
    "Unlike exact or approximate string matching, this method leverages **contextual embeddings** from a pre-trained transformer to assess how semantically close the predicted keywords are to the reference keywords.\n",
    "\n",
    "The evaluation procedure is as follows:\n",
    "\n",
    "- We extract only the **text** of the predicted keywords for each model, discarding their confidence scores.\n",
    "- We embed all **predicted** and **ground truth** keywords using the same sentence transformer model.\n",
    "- Embeddings are **normalized** to ensure cosine similarity is a valid similarity measure.\n",
    "- For each predicted keyword, we compute the **cosine similarity** with all ground truth keywords.\n",
    "- A predicted keyword is considered a **semantic match** if its similarity with any ground truth keyword exceeds a fixed threshold (e.g., **0.75**).\n",
    "\n",
    "Once all matches are determined across all reviews of the selected movie, we compute:\n",
    "\n",
    "- **Semantic Precision**: Fraction of all predicted keywords (global) that have a semantic match.\n",
    "- **Semantic Recall**: Fraction of all ground truth keywords that are matched by at least one semantically similar predicted keyword.\n",
    "- **Semantic F1-score**: Harmonic mean of semantic precision and recall.\n",
    "\n",
    "This global semantic evaluation better reflects the models’ ability to capture **meaningful and relevant keywords**, even when the wording differs from the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbbd11b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_11fb8\">\n",
       "  <caption>Global Semantic-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_11fb8_level0_col0\" class=\"col_heading level0 col0\" >Semantic_Precision</th>\n",
       "      <th id=\"T_11fb8_level0_col1\" class=\"col_heading level0 col1\" >Semantic_Recall</th>\n",
       "      <th id=\"T_11fb8_level0_col2\" class=\"col_heading level0 col2\" >Semantic_F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_11fb8_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_11fb8_row0_col0\" class=\"data row0 col0\" >0.6736</td>\n",
       "      <td id=\"T_11fb8_row0_col1\" class=\"data row0 col1\" >0.4890</td>\n",
       "      <td id=\"T_11fb8_row0_col2\" class=\"data row0 col2\" >0.5666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11fb8_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_11fb8_row1_col0\" class=\"data row1 col0\" >0.6434</td>\n",
       "      <td id=\"T_11fb8_row1_col1\" class=\"data row1 col1\" >0.5074</td>\n",
       "      <td id=\"T_11fb8_row1_col2\" class=\"data row1 col2\" >0.5673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13f5f0730>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute embeddings for the ground truth keywords once per selected movie\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Dictionary to collect all predicted keywords per model (without duplicates)\n",
    "all_predictions = {model: set() for model in models_to_evaluate}\n",
    "\n",
    "# Collect predicted keywords across all reviews (as a set for uniqueness)\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            # Extract keyword strings and normalize\n",
    "            pred_kw = [normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            all_predictions[model].update(pred_kw)  # Add to set (no duplicates)\n",
    "\n",
    "# Compute semantic evaluation globally for each model\n",
    "semantic_scores = []\n",
    "for model in models_to_evaluate:\n",
    "    pred_kw = list(all_predictions[model])  # Convert back to list\n",
    "    precision, recall, f1 = evaluate_semantic_keywords_global(pred_kw, gt_keywords, device=device)\n",
    "\n",
    "    semantic_scores.append({\n",
    "        \"Model\": model,\n",
    "        \"Semantic_Precision\": round(precision, 4),\n",
    "        \"Semantic_Recall\": round(recall, 4),\n",
    "        \"Semantic_F1\": round(f1, 4)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and format\n",
    "summary_df = pd.DataFrame(semantic_scores).set_index(\"Model\")\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Semantic-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187183",
   "metadata": {},
   "source": [
    "This section automatically processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains predicted keywords generated by different models.\n",
    "\n",
    "For each movie:\n",
    "- The corresponding **ground truth keywords** are loaded.\n",
    "- Predicted keywords from both models — **Base** and **Metadata-enhanced** — are evaluated.\n",
    "- The evaluation is performed globally across all reviews in the movie, without computing metrics per review.\n",
    "\n",
    "For each model, the following **global metrics** are computed:\n",
    "\n",
    "- **Unweighted Metrics**:  \n",
    "  Precision, Recall, and F1-score using approximate string matching.\n",
    "\n",
    "- **Score-aware Metrics**:  \n",
    "  - **Weighted Precision**: proportion of total confidence assigned to correct predictions.  \n",
    "  - **Weighted Recall**: coverage of ground truth weighted by prediction confidence.  \n",
    "  - **Weighted F1-score**: harmonic mean of weighted precision and recall.  \n",
    "  - **nDCG@5**: evaluates the quality of keyword ranking using graded relevance and position-based discounting.\n",
    "\n",
    "- **Semantic Metrics**:  \n",
    "  Semantic Precision, Recall, and F1-score using **cosine similarity** between **sentence embeddings** of predicted and ground truth keywords.\n",
    "\n",
    "All metrics are computed globally for each movie and then compiled into a summary table to compare the overall performance of the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4faf7251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3c091\">\n",
       "  <caption>Global Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3c091_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_3c091_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_3c091_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_3c091_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_3c091_level0_col4\" class=\"col_heading level0 col4\" >F1-score</th>\n",
       "      <th id=\"T_3c091_level0_col5\" class=\"col_heading level0 col5\" >Weighted Precision</th>\n",
       "      <th id=\"T_3c091_level0_col6\" class=\"col_heading level0 col6\" >Weighted Recall</th>\n",
       "      <th id=\"T_3c091_level0_col7\" class=\"col_heading level0 col7\" >Weighted F1-score</th>\n",
       "      <th id=\"T_3c091_level0_col8\" class=\"col_heading level0 col8\" >nDCG@5</th>\n",
       "      <th id=\"T_3c091_level0_col9\" class=\"col_heading level0 col9\" >Semantic Precision</th>\n",
       "      <th id=\"T_3c091_level0_col10\" class=\"col_heading level0 col10\" >Semantic Recall</th>\n",
       "      <th id=\"T_3c091_level0_col11\" class=\"col_heading level0 col11\" >Semantic F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c091_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3c091_row0_col0\" class=\"data row0 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_3c091_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_3c091_row0_col2\" class=\"data row0 col2\" >0.8662</td>\n",
       "      <td id=\"T_3c091_row0_col3\" class=\"data row0 col3\" >0.3369</td>\n",
       "      <td id=\"T_3c091_row0_col4\" class=\"data row0 col4\" >0.4851</td>\n",
       "      <td id=\"T_3c091_row0_col5\" class=\"data row0 col5\" >0.9155</td>\n",
       "      <td id=\"T_3c091_row0_col6\" class=\"data row0 col6\" >0.1789</td>\n",
       "      <td id=\"T_3c091_row0_col7\" class=\"data row0 col7\" >0.2993</td>\n",
       "      <td id=\"T_3c091_row0_col8\" class=\"data row0 col8\" >0.6973</td>\n",
       "      <td id=\"T_3c091_row0_col9\" class=\"data row0 col9\" >0.6736</td>\n",
       "      <td id=\"T_3c091_row0_col10\" class=\"data row0 col10\" >0.4890</td>\n",
       "      <td id=\"T_3c091_row0_col11\" class=\"data row0 col11\" >0.5666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c091_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3c091_row1_col0\" class=\"data row1 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_3c091_row1_col1\" class=\"data row1 col1\" >metadata</td>\n",
       "      <td id=\"T_3c091_row1_col2\" class=\"data row1 col2\" >0.8890</td>\n",
       "      <td id=\"T_3c091_row1_col3\" class=\"data row1 col3\" >0.3457</td>\n",
       "      <td id=\"T_3c091_row1_col4\" class=\"data row1 col4\" >0.4978</td>\n",
       "      <td id=\"T_3c091_row1_col5\" class=\"data row1 col5\" >0.9181</td>\n",
       "      <td id=\"T_3c091_row1_col6\" class=\"data row1 col6\" >0.2085</td>\n",
       "      <td id=\"T_3c091_row1_col7\" class=\"data row1 col7\" >0.3398</td>\n",
       "      <td id=\"T_3c091_row1_col8\" class=\"data row1 col8\" >0.7169</td>\n",
       "      <td id=\"T_3c091_row1_col9\" class=\"data row1 col9\" >0.6434</td>\n",
       "      <td id=\"T_3c091_row1_col10\" class=\"data row1 col10\" >0.5074</td>\n",
       "      <td id=\"T_3c091_row1_col11\" class=\"data row1 col11\" >0.5673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16c120af0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Load ground truth\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over movie keyword predictions\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Ground truth per quel film\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Init: predicted keyword lists (per review, no duplicates)\n",
    "            all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            for _, row in selected_film.iterrows():\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "                        predicted_kw_score = [(normalize_kw(kw), score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "                        seen = set()\n",
    "                        unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "                        all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "            for model in models_to_evaluate:\n",
    "                # Flatten keywords (global unique list)\n",
    "                flat_kw = [kw for review in all_predicted_kw_score[model] for kw, _ in review]\n",
    "                unique_pred_keywords = list(set(flat_kw))\n",
    "\n",
    "                # Max score for each keyword\n",
    "                kw_score_max = {}\n",
    "                for review in all_predicted_kw_score[model]:\n",
    "                    for kw, score in review:\n",
    "                        if kw not in kw_score_max or score > kw_score_max[kw]:\n",
    "                            kw_score_max[kw] = score\n",
    "                sorted_kw_score = sorted(kw_score_max.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Score-aware format\n",
    "                pred_kw_score_list = all_predicted_kw_score[model]\n",
    "                keyword_only_list = [[kw for kw, _ in review] for review in pred_kw_score_list]\n",
    "\n",
    "                # Compute all metrics\n",
    "                p, r, f1 = evaluate_keywords(keyword_only_list, gt_keywords)\n",
    "                wp, wr, wf = evaluate_keywords_weighted(pred_kw_score_list, gt_keywords)\n",
    "                ndcg = compute_global_ndcg(pred_kw_score_list, gt_keywords, k=5)\n",
    "                sp, sr, sf1 = evaluate_semantic_keywords_global(unique_pred_keywords, gt_keywords, device=device)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Movie\": movie_name,\n",
    "                    \"Model\": model,\n",
    "                    \"Precision\": round(p, 4),\n",
    "                    \"Recall\": round(r, 4),\n",
    "                    \"F1-score\": round(f1, 4),\n",
    "                    \"Weighted Precision\": round(wp, 4),\n",
    "                    \"Weighted Recall\": round(wr, 4),\n",
    "                    \"Weighted F1-score\": round(wf, 4),\n",
    "                    \"nDCG@5\": round(ndcg, 4),\n",
    "                    \"Semantic Precision\": round(sp, 4),\n",
    "                    \"Semantic Recall\": round(sr, 4),\n",
    "                    \"Semantic F1-score\": round(sf1, 4),\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Final summary\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Global Evaluation Summary per Movie and Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
