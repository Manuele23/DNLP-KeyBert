{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTMetadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a77813",
   "metadata": {},
   "source": [
    "This notebook evaluates and compares different keyword extraction models applied to movie reviews. The focus is on understanding how well each model can extract meaningful keywords that align with the ground truth.\n",
    "\n",
    "We aim to assess the performance of two models:\n",
    "- **Base** KeyBERT model\n",
    "- **Metadata-enhanced** version that integrates additional information: KeyBERTMetadata\n",
    "\n",
    "The evaluation is performed on a set of reviews for a selected movie, where each model predicts a ranked list of top-5 keywords per review.\n",
    "\n",
    "The notebook uses:\n",
    "- A **ground truth dataset** of annotated keywords per movie retrieved from IMDB\n",
    "- **Model outputs**: lists of predicted keywords with associated confidence scores for each review\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Two types of evaluation are conducted:\n",
    "\n",
    "**1. Basic (Unweighted) Metrics**\n",
    "- **Precision**, **Recall**, and **F1-score** based on approximate binary matching\n",
    "- Each review is evaluated independently, and metrics are averaged\n",
    "\n",
    "**2. Score-Aware Metrics**\n",
    "- **Weighted Precision/Recall/F1**: matches are weighted by the model’s confidence scores\n",
    "- **nDCG@5**: evaluates ranking quality of the predicted top-5 keywords\n",
    "\n",
    "Together, these evaluations provide both a general and a fine-grained view of model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "numpy is already installed.\n",
      "Installing scikit-learn...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\", \"numpy\", \"scikit-learn\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "973bef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for structured data manipulation using DataFrames (tables)\n",
    "import pandas as pd\n",
    "\n",
    "# Import scikit-learn metrics to evaluate prediction performance\n",
    "# - precision_score: proportion of correctly predicted positive samples\n",
    "# - recall_score: proportion of actual positive samples correctly identified\n",
    "# - f1_score: harmonic mean of precision and recall, balances precision/recall trade-off\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the 're' module for text preprocessing using regular expressions\n",
    "# (e.g., removing punctuation or matching patterns)\n",
    "import re\n",
    "\n",
    "# Import numpy for efficient numerical computations, array operations, and statistics\n",
    "import numpy as np\n",
    "\n",
    "# Import math for mathematical functions like logarithms used in metrics such as nDCG\n",
    "import math\n",
    "\n",
    "# Import os to handle file system operations (e.g., listing files in directories)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords\n",
    "\n",
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"Parasite\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic – Unweighted)\n",
    "\n",
    "This block defines the baseline utility functions used to evaluate predicted keywords against the ground truth. These functions do **not** take into account keyword confidence scores or ranking—they perform **binary, unweighted evaluation**.\n",
    "\n",
    "Specifically, this implementation includes:\n",
    "\n",
    "- **Normalization**: keywords are converted to lowercase, stripped of punctuation, and cleaned of extra whitespace to ensure consistent matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule that considers two keywords as matching if they are identical or if one is a substring of the other (e.g., *\"social satire\"* ≈ *\"satire\"*).\n",
    "\n",
    "- **Evaluation**: standard metrics — **precision**, **recall**, and **F1-score** — are calculated based on the number of approximate matches between predicted and ground truth keywords.\n",
    "\n",
    "This provides a basic but interpretable way to assess keyword extraction quality without considering the ranking or confidence scores assigned by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches the ground truth exactly\n",
    "# or if either keyword contains the other as a substring\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Evaluation function for a single prediction instance:\n",
    "# - Normalizes both predicted and ground truth keywords\n",
    "# - Computes how many predicted keywords approximately match the ground truth\n",
    "# - Calculates precision, recall, and F1-score\n",
    "def evaluate_keywords(pred_keywords, gt_keywords):\n",
    "    pred_keywords = [normalize_kw(k) for k in pred_keywords]\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    \n",
    "    # Count how many predicted keywords match approximately with any ground truth keyword\n",
    "    match_count = sum([is_approx_match(k, gt_keywords) for k in pred_keywords])\n",
    "    \n",
    "    # Precision: percentage of predicted keywords that are correct\n",
    "    precision = match_count / len(pred_keywords) if pred_keywords else 0\n",
    "\n",
    "    # Recall: percentage of ground truth keywords that were correctly predicted\n",
    "    recall = match_count / len(gt_keywords) if gt_keywords else 0\n",
    "\n",
    "    # F1-score: harmonic mean of precision and recall\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic – Unweighted)\n",
    "\n",
    "This section evaluates two keyword extraction models — **base** and **metadata-enhanced** — using the ground truth.\n",
    "\n",
    "For each **review**, basic precision, recall, and F1-score are computed based on binary keyword matching. These metrics are then **averaged across all reviews** to provide an overall performance comparison between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d333544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Create a dictionary to store evaluation results (precision, recall, F1) for each model\n",
    "results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie (same for all reviews)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        \n",
    "        # Column name with the predicted keywords for this model\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Proceed only if the column exists and contains a list of predicted keywords\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "\n",
    "            # Extract only the keyword strings from (keyword, score) tuples\n",
    "            predicted_keywords = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            \n",
    "            # Evaluate the prediction using precision, recall, and F1-score\n",
    "            precision, recall, f1 = evaluate_keywords(predicted_keywords, ground_truth_keywords)\n",
    "            \n",
    "            # Store the result for this specific review\n",
    "            results[model].append({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "# Aggregate the results to compute average metrics for each model\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precisions = [r[\"precision\"] for r in results[model]]\n",
    "    recalls = [r[\"recall\"] for r in results[model]]\n",
    "    f1s = [r[\"f1\"] for r in results[model]]\n",
    "    \n",
    "    # Calculate the average of each metric and round to 4 decimal places\n",
    "    summary[model] = {\n",
    "        \"avg_precision\": round(np.mean(precisions), 4),\n",
    "        \"avg_recall\": round(np.mean(recalls), 4),\n",
    "        \"avg_f1\": round(np.mean(f1s), 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c5d0eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d30cf\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d30cf_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_d30cf_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_d30cf_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d30cf_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_d30cf_row0_col0\" class=\"data row0 col0\" >0.1600</td>\n",
       "      <td id=\"T_d30cf_row0_col1\" class=\"data row0 col1\" >0.0025</td>\n",
       "      <td id=\"T_d30cf_row0_col2\" class=\"data row0 col2\" >0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d30cf_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_d30cf_row1_col0\" class=\"data row1 col0\" >0.1200</td>\n",
       "      <td id=\"T_d30cf_row1_col1\" class=\"data row1 col1\" >0.0019</td>\n",
       "      <td id=\"T_d30cf_row1_col2\" class=\"data row1 col2\" >0.0037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17d328a30>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1-score\",\n",
    "]\n",
    "\n",
    "# Display the summary table nicely\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics and nDCG@k\n",
    "\n",
    "In this extended evaluation, we incorporate the **confidence scores** assigned by the model to each predicted keyword. This enables us to evaluate not only the correctness of predictions, but also how confidently and how well they are ranked.\n",
    "\n",
    "### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Measures how much of the model’s total confidence is assigned to correct keywords. A confident but incorrect prediction hurts the score more.\n",
    "- **Weighted Recall**: Reflects how much of the ground truth is recovered, weighted by the confidence given to each correct prediction.\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall. It balances the trade-off between being accurate and being comprehensive.\n",
    "- **nDCG@k (Normalized Discounted Cumulative Gain)**: A ranking metric that rewards placing relevant keywords at the top of the prediction list.\n",
    "\n",
    "### nDCG@k Definition\n",
    "\n",
    "$nDCG@k = \\frac{DCG@k}{IDCG@k}, \\quad \n",
    "DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i + 1)}$\n",
    "\n",
    "Where:\n",
    "- $rel_i = 1$ if the predicted keyword at rank $i$ is in the ground truth, 0 otherwise  \n",
    "- $IDCG@k$ is the best possible DCG if all relevant keywords were perfectly ranked at the top\n",
    "\n",
    "#### Example (k = 5)\n",
    "\n",
    "Predicted: `[\"scam\", \"family\", \"poverty\", \"cinematography\", \"fraud\"]`  \n",
    "Ground truth: `[\"fraud\", \"poverty\", \"scam\"]`\n",
    "\n",
    "Relevant keywords are found at ranks 1, 3, and 5:\n",
    "\n",
    "$DCG = \\frac{1}{\\log_2(1+1)} + \\frac{1}{\\log_2(3+1)} + \\frac{1}{\\log_2(5+1)} = 1 + 0.5 + 0.386 = 1.886$\n",
    "\n",
    "$IDCG = \\frac{1}{\\log_2(1+1)} + \\frac{1}{\\log_2(2+1)} + \\frac{1}{\\log_2(3+1)} = 1 + 0.6309 + 0.5 = 2.1309$\n",
    "\n",
    "$nDCG@5 = \\frac{1.886}{2.1309} \\approx 0.885$\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **nDCG@5 = 1** → perfect ranking (all correct keywords are at the top)  \n",
    "- **nDCG@5 = 0** → none of the ground truth keywords are in the top-*k*\n",
    "\n",
    "These metrics provide a more nuanced and realistic evaluation by combining prediction accuracy with ranking quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches any ground truth keyword\n",
    "# using a relaxed comparison: exact match or substring containment\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Weighted evaluation function:\n",
    "# Calculates precision, recall, and F1-score using the confidence scores of predicted keywords\n",
    "# - High-confidence correct predictions contribute more\n",
    "# - Precision is score-weighted; recall divides by total ground truth\n",
    "def evaluate_keywords_weighted(predicted_kw_score, gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate predicted keywords with confidence scores using weighted precision, recall, and F1.\n",
    "    \n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with associated confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords (annotated)\n",
    "    \n",
    "    Returns:\n",
    "        (precision, recall, f1): all metrics computed using score-weighted matching\n",
    "    \"\"\"\n",
    "    # Normalize both predicted and ground truth keywords\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords = [(normalize_kw(kw), score) for kw, score in predicted_kw_score if isinstance(kw, str)]\n",
    "    \n",
    "    total_score = sum(score for _, score in pred_keywords)\n",
    "    if total_score == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # Compute total score of predicted keywords that approximately match the ground truth\n",
    "    match_score = sum(score for kw, score in pred_keywords if is_approx_match(kw, gt_keywords))\n",
    "    \n",
    "    # Weighted precision and recall\n",
    "    precision = match_score / total_score\n",
    "    recall = match_score / len(gt_keywords) if gt_keywords else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# Ranking-based evaluation function:\n",
    "# Computes the normalized Discounted Cumulative Gain (nDCG@k) for predicted keywords\n",
    "# Gives more credit when correct keywords appear earlier in the ranking\n",
    "def compute_ndcg(predicted_kw_score, gt_keywords, k=5):\n",
    "    \"\"\"\n",
    "    Compute nDCG@k between predicted keywords (with scores) and the ground truth keywords.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with associated scores\n",
    "        gt_keywords (list of str): ground truth keywords\n",
    "        k (int): number of top keywords to consider\n",
    "    \n",
    "    Returns:\n",
    "        float: nDCG score (between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Normalize the ground truth and predicted keyword strings\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords = [normalize_kw(kw) for kw, _ in predicted_kw_score[:k]]\n",
    "\n",
    "    # Assign binary relevance: 1 if keyword matches ground truth, 0 otherwise\n",
    "    relevance = [1 if is_approx_match(kw, gt_keywords) else 0 for kw in pred_keywords]\n",
    "\n",
    "    # Compute DCG (Discounted Cumulative Gain)\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # Compute IDCG (Ideal DCG): the best possible ordering\n",
    "    ideal_relevance = sorted(relevance, reverse=True)\n",
    "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
    "\n",
    "    # Return normalized DCG\n",
    "    return dcg / idcg if idcg != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)\n",
    "\n",
    "In this section, we apply the score-aware evaluation metrics to each review for both models:\n",
    "\n",
    "- **Weighted Precision, Recall, F1**: accounts for the confidence scores of each predicted keyword.\n",
    "- **nDCG@5**: evaluates the ranking quality of the top-5 keywords based on their alignment with the ground truth.\n",
    "\n",
    "Each review is evaluated individually, and the metrics are then averaged across all reviews to summarize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Initialize results dictionary for weighted metrics and nDCG\n",
    "weighted_results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Loop through each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = row[pred_col]  # list of (kw, score)\n",
    "\n",
    "            # Compute weighted metrics\n",
    "            w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, ground_truth_keywords)\n",
    "\n",
    "            # Compute nDCG@5\n",
    "            ndcg = compute_ndcg(predicted_kw_score, ground_truth_keywords, k=5)\n",
    "\n",
    "            # Save results\n",
    "            weighted_results[model].append({\n",
    "                \"weighted_precision\": w_precision,\n",
    "                \"weighted_recall\": w_recall,\n",
    "                \"weighted_f1\": w_f1,\n",
    "                \"ndcg@5\": ndcg\n",
    "            })\n",
    "\n",
    "# Compute average metrics across all reviews\n",
    "weighted_summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    metrics = weighted_results[model]\n",
    "    weighted_summary[model] = {\n",
    "        \"avg_weighted_precision\": round(np.mean([m[\"weighted_precision\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_recall\": round(np.mean([m[\"weighted_recall\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_f1\": round(np.mean([m[\"weighted_f1\"] for m in metrics]), 4),\n",
    "        \"avg_ndcg@5\": round(np.mean([m[\"ndcg@5\"] for m in metrics]), 4)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21d6be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0d29d\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0d29d_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_0d29d_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_0d29d_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "      <th id=\"T_0d29d_level0_col3\" class=\"col_heading level0 col3\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0d29d_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_0d29d_row0_col0\" class=\"data row0 col0\" >0.1564</td>\n",
       "      <td id=\"T_0d29d_row0_col1\" class=\"data row0 col1\" >0.0012</td>\n",
       "      <td id=\"T_0d29d_row0_col2\" class=\"data row0 col2\" >0.0023</td>\n",
       "      <td id=\"T_0d29d_row0_col3\" class=\"data row0 col3\" >0.3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d29d_level0_row1\" class=\"row_heading level0 row1\" >metadata</th>\n",
       "      <td id=\"T_0d29d_row1_col0\" class=\"data row1 col0\" >0.1186</td>\n",
       "      <td id=\"T_0d29d_row1_col1\" class=\"data row1 col1\" >0.0011</td>\n",
       "      <td id=\"T_0d29d_row1_col2\" class=\"data row1 col2\" >0.0021</td>\n",
       "      <td id=\"T_0d29d_row1_col3\" class=\"data row1 col3\" >0.2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17c0c3340>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "    \"nDCG@5\"\n",
    "]\n",
    "\n",
    "# Display the summary table\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies\n",
    "\n",
    "This section automatically evaluates all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a movie and contains predicted keywords from different models.\n",
    "\n",
    "For each movie:\n",
    "- The associated ground truth keywords are loaded.\n",
    "- The predicted keywords from both models (**base** and **metadata**) are evaluated.\n",
    "- The following metrics are computed for each review:\n",
    "  - **Unweighted**: Precision, Recall, F1-score\n",
    "  - **Score-aware**: Weighted Precision, Weighted Recall, Weighted F1, and nDCG@5\n",
    "\n",
    "Finally, average metrics are computed per movie and model, and compiled into a final summary table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc08dd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_813fb\">\n",
       "  <caption>Full Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_813fb_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_813fb_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_813fb_level0_col2\" class=\"col_heading level0 col2\" >Avg_Precision</th>\n",
       "      <th id=\"T_813fb_level0_col3\" class=\"col_heading level0 col3\" >Avg_Recall</th>\n",
       "      <th id=\"T_813fb_level0_col4\" class=\"col_heading level0 col4\" >Avg_F1</th>\n",
       "      <th id=\"T_813fb_level0_col5\" class=\"col_heading level0 col5\" >Avg_Weighted_Precision</th>\n",
       "      <th id=\"T_813fb_level0_col6\" class=\"col_heading level0 col6\" >Avg_Weighted_Recall</th>\n",
       "      <th id=\"T_813fb_level0_col7\" class=\"col_heading level0 col7\" >Avg_Weighted_F1</th>\n",
       "      <th id=\"T_813fb_level0_col8\" class=\"col_heading level0 col8\" >Avg_nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_813fb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_813fb_row0_col0\" class=\"data row0 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_813fb_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_813fb_row0_col2\" class=\"data row0 col2\" >0.0800</td>\n",
       "      <td id=\"T_813fb_row0_col3\" class=\"data row0 col3\" >0.0014</td>\n",
       "      <td id=\"T_813fb_row0_col4\" class=\"data row0 col4\" >0.0027</td>\n",
       "      <td id=\"T_813fb_row0_col5\" class=\"data row0 col5\" >0.0798</td>\n",
       "      <td id=\"T_813fb_row0_col6\" class=\"data row0 col6\" >0.0008</td>\n",
       "      <td id=\"T_813fb_row0_col7\" class=\"data row0 col7\" >0.0015</td>\n",
       "      <td id=\"T_813fb_row0_col8\" class=\"data row0 col8\" >0.1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_813fb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_813fb_row1_col0\" class=\"data row1 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_813fb_row1_col1\" class=\"data row1 col1\" >metadata</td>\n",
       "      <td id=\"T_813fb_row1_col2\" class=\"data row1 col2\" >0.0800</td>\n",
       "      <td id=\"T_813fb_row1_col3\" class=\"data row1 col3\" >0.0014</td>\n",
       "      <td id=\"T_813fb_row1_col4\" class=\"data row1 col4\" >0.0027</td>\n",
       "      <td id=\"T_813fb_row1_col5\" class=\"data row1 col5\" >0.0827</td>\n",
       "      <td id=\"T_813fb_row1_col6\" class=\"data row1 col6\" >0.0009</td>\n",
       "      <td id=\"T_813fb_row1_col7\" class=\"data row1 col7\" >0.0018</td>\n",
       "      <td id=\"T_813fb_row1_col8\" class=\"data row1 col8\" >0.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_813fb_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_813fb_row2_col0\" class=\"data row2 col0\" >Parasite</td>\n",
       "      <td id=\"T_813fb_row2_col1\" class=\"data row2 col1\" >base</td>\n",
       "      <td id=\"T_813fb_row2_col2\" class=\"data row2 col2\" >0.1600</td>\n",
       "      <td id=\"T_813fb_row2_col3\" class=\"data row2 col3\" >0.0025</td>\n",
       "      <td id=\"T_813fb_row2_col4\" class=\"data row2 col4\" >0.0050</td>\n",
       "      <td id=\"T_813fb_row2_col5\" class=\"data row2 col5\" >0.1564</td>\n",
       "      <td id=\"T_813fb_row2_col6\" class=\"data row2 col6\" >0.0012</td>\n",
       "      <td id=\"T_813fb_row2_col7\" class=\"data row2 col7\" >0.0023</td>\n",
       "      <td id=\"T_813fb_row2_col8\" class=\"data row2 col8\" >0.3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_813fb_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_813fb_row3_col0\" class=\"data row3 col0\" >Parasite</td>\n",
       "      <td id=\"T_813fb_row3_col1\" class=\"data row3 col1\" >metadata</td>\n",
       "      <td id=\"T_813fb_row3_col2\" class=\"data row3 col2\" >0.1200</td>\n",
       "      <td id=\"T_813fb_row3_col3\" class=\"data row3 col3\" >0.0019</td>\n",
       "      <td id=\"T_813fb_row3_col4\" class=\"data row3 col4\" >0.0037</td>\n",
       "      <td id=\"T_813fb_row3_col5\" class=\"data row3 col5\" >0.1186</td>\n",
       "      <td id=\"T_813fb_row3_col6\" class=\"data row3 col6\" >0.0011</td>\n",
       "      <td id=\"T_813fb_row3_col7\" class=\"data row3 col7\" >0.0021</td>\n",
       "      <td id=\"T_813fb_row3_col8\" class=\"data row3 col8\" >0.2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17374be50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load the ground truth once for all movies\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# List of models to evaluate\n",
    "models_to_evaluate = [\"base\", \"metadata\"]\n",
    "\n",
    "# Store results across all movies\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all keyword prediction files\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords for the current movie\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Retrieve ground truth keywords for the selected movie\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Initialize metrics for each model\n",
    "            results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            # Evaluate each review in the dataset\n",
    "            for _, row in selected_film.iterrows():\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "\n",
    "                    # Skip if no predictions or wrong format\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "                        predicted_kw_score = row[pred_col]\n",
    "\n",
    "                        # Extract only keyword strings for unweighted evaluation\n",
    "                        pred_kw_only = [kw for kw, _ in predicted_kw_score if isinstance(kw, str)]\n",
    "\n",
    "                        # Compute basic (unweighted) metrics\n",
    "                        precision, recall, f1 = evaluate_keywords(pred_kw_only, gt_keywords)\n",
    "\n",
    "                        # Compute score-weighted metrics\n",
    "                        w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, gt_keywords)\n",
    "\n",
    "                        # Compute ranking-based metric (nDCG@5)\n",
    "                        ndcg = compute_ndcg(predicted_kw_score, gt_keywords, k=5)\n",
    "\n",
    "                        # Store all metrics for this review\n",
    "                        results[model].append({\n",
    "                            \"precision\": precision,\n",
    "                            \"recall\": recall,\n",
    "                            \"f1\": f1,\n",
    "                            \"w_precision\": w_precision,\n",
    "                            \"w_recall\": w_recall,\n",
    "                            \"w_f1\": w_f1,\n",
    "                            \"ndcg@5\": ndcg\n",
    "                        })\n",
    "\n",
    "            # Compute average metrics per model for the current movie\n",
    "            for model in models_to_evaluate:\n",
    "                if results[model]:\n",
    "                    metrics_df = pd.DataFrame(results[model])\n",
    "                    all_results.append({\n",
    "                        \"Movie\": movie_name,\n",
    "                        \"Model\": model,\n",
    "                        \"Avg_Precision\": round(metrics_df[\"precision\"].mean(), 4),\n",
    "                        \"Avg_Recall\": round(metrics_df[\"recall\"].mean(), 4),\n",
    "                        \"Avg_F1\": round(metrics_df[\"f1\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Precision\": round(metrics_df[\"w_precision\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Recall\": round(metrics_df[\"w_recall\"].mean(), 4),\n",
    "                        \"Avg_Weighted_F1\": round(metrics_df[\"w_f1\"].mean(), 4),\n",
    "                        \"Avg_nDCG@5\": round(metrics_df[\"ndcg@5\"].mean(), 4)\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create the final summary DataFrame\n",
    "final_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display table with clean index and sorted values\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Full Evaluation Summary per Movie and Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
