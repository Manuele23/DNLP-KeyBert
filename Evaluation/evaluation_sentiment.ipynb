{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTSentimentAware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804715a2",
   "metadata": {},
   "source": [
    "### **Keyword Extraction Model Evaluation on Movie Reviews**\n",
    "\n",
    "This notebook evaluates and compares different keyword extraction models applied to movie reviews, with a specific focus on assessing how well each model captures not only **semantic relevance** but also **sentiment awareness** in its keyword rankings.\n",
    "\n",
    "We assess the performance of two models:\n",
    "\n",
    "- **Base** KeyBERT model  \n",
    "- **Sentiment-aware** extension (`KeyBERTSentimentAware`), which integrates a custom sentiment classifier to adjust keyword relevance scores based on predicted sentiment\n",
    "\n",
    "This notebook adopts a **global evaluation approach**: all predicted and ground truth keywords across the entire dataset are aggregated by movie **before** computing each metric. This provides a **holistic view** of each model's performance, unaffected by review-level variance.\n",
    "\n",
    "We use a set of annotated ground truth keywords per movie (from IMDb), and the top-5 predicted keywords (with scores) per review for both models.\n",
    "\n",
    "### **Evaluation Layers**\n",
    "\n",
    "#### 1. **Basic (Unweighted) Metrics**\n",
    "\n",
    "- **Precision**, **Recall**, and **F1-score** computed globally via approximate binary matching.\n",
    "- A predicted keyword is correct if it approximately matches any of the movie’s ground truth keywords.\n",
    "- All keywords from all reviews are flattened and compared in aggregate.\n",
    "\n",
    "#### 2. **Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision**, **Recall**, and **Weighted F1-score**:\n",
    "  - Each predicted keyword is weighted by its score.\n",
    "  - Correct predictions contribute proportionally to their confidence.\n",
    "- **nDCG@5 (Normalized Discounted Cumulative Gain)**:\n",
    "  - Evaluates whether correct keywords are ranked near the top globally.\n",
    "  - Relevance is discounted by position, rewarding better keyword orderings.\n",
    "\n",
    "#### 3. **Semantic Evaluation (Embedding-Based)**\n",
    "\n",
    "- All predicted and ground truth keywords are embedded using a **sentence-transformer** model.\n",
    "- **Cosine similarity** is used to detect approximate **semantic matches**.\n",
    "- A predicted keyword is correct if its similarity with any ground truth keyword exceeds a given threshold (e.g., 0.75).\n",
    "- **Semantic Precision**, **Recall**, and **F1-score** are computed globally based on these soft matches.\n",
    "\n",
    "#### 4. **Sentiment-Aware Evaluation**\n",
    "\n",
    "These metrics assess whether the predicted keywords reflect the **emotional tone** of the content, not just its semantic meaning.\n",
    "\n",
    "- **Average Sentiment Similarity (ASS)**  \n",
    "  Measures how emotionally aligned the predicted keywords are with a reference sentiment value (e.g., average sentiment of the ground truth).  \n",
    "  Each review’s predicted keywords are converted into a sentiment vector and compared to a fixed sentiment reference using **cosine similarity**.  \n",
    "  The final value is the average similarity across all reviews, clamped to the [0, 1] range.  \n",
    "  High values indicate strong emotional coherence with the global reference.\n",
    "\n",
    "- **Review-wise Sampled Sentiment Similarity (RSSS)**  \n",
    "  Introduces per-review robustness by comparing sampled predicted keywords with sampled ground truth keywords.  \n",
    "  For each review, multiple random samples of predicted keywords are compared to the same ground truth sentiment vector.  \n",
    "  The final score is the average of all these similarities, capturing local variations in emotional tone and offering **granular insight** into sentiment alignment.\n",
    "\n",
    "### **Why Sentiment-Aware Evaluation Matters**\n",
    "\n",
    "The base KeyBERT model selects keywords purely based on semantic relevance, while the sentiment-aware version ranks keywords by combining semantic and emotional signals.\n",
    "\n",
    "Traditional evaluation metrics may overlook whether the extracted keywords reflect the emotional tone of a review. The Sentiment Rank Correlation (SRC) addresses this gap by explicitly measuring the relationship between emotional expressiveness and keyword relevance.\n",
    "\n",
    "This multi-dimensional evaluation provides a robust framework for comparing keyword extraction models, revealing differences in both informational content and emotional alignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "numpy is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers is already installed.\n",
      "torch is already installed.\n",
      "scipy is already installed.\n",
      "nltk is already installed.\n",
      "tqdm is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"tqdm\", \"transformers\", \"torch\", \"scipy\", \"nltk\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973bef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/manuelemustari/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library\n",
    "import os      # Operating system interfaces (e.g., file and directory handling)\n",
    "import re      # Regular expressions for advanced pattern matching in text\n",
    "import math    # Mathematical functions (e.g., exp, log, sqrt)\n",
    "import random  # Random sampling utilities (e.g., for bootstrapping or sampling keywords)\n",
    "\n",
    "# Data Handling and Computation\n",
    "import pandas as pd      # Data manipulation and analysis (DataFrames)\n",
    "import numpy as np       # Numerical operations and linear algebra (arrays, vector math)\n",
    "\n",
    "# Progress Monitoring\n",
    "from tqdm import tqdm    # Progress bars for loops and data processing\n",
    "\n",
    "# Machine Learning & Transformers\n",
    "import torch                                       # Core PyTorch tensor operations\n",
    "import torch.nn.functional as F                   # Functional API for layers, activations, loss functions\n",
    "from transformers import AutoTokenizer, AutoModel  # Hugging Face: load pre-trained transformer models and tokenizers\n",
    "\n",
    "# Evaluation and Statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Vector similarity computation (e.g., for sentiment vectors)\n",
    "from scipy.stats import spearmanr                       # Spearman rank correlation for monotonic relationship analysis\n",
    "\n",
    "# Sentiment Analysis\n",
    "import nltk                                              # Natural Language Toolkit (used to load VADER lexicon)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer   # VADER sentiment analyzer (suitable for short texts and keywords)\n",
    "nltk.download(\"vader_lexicon\")                           # Ensure the VADER lexicon is available locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9ee47",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c179",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55796a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602be9",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"IndianaJones\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a94a",
   "metadata": {},
   "source": [
    "This block defines the core utility functions used to evaluate predicted keywords against the ground truth. These functions perform a **binary, unweighted evaluation**, ignoring confidence scores and ranking information.\n",
    "\n",
    "The evaluation pipeline includes the following steps:\n",
    "\n",
    "- **Normalization**: all keywords are lowercased, stripped of punctuation, and cleaned of extra whitespace to ensure consistent text matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule considers two keywords as a match if:\n",
    "  - They are exactly equal (after normalization), or\n",
    "  - One is a substring of the other (e.g., *\"social satire\"* is considered a match with *\"satire\"*).\n",
    "\n",
    "- **Global Evaluation**: for each model, all keywords predicted across the reviews of a given movie are aggregated, and then compared to the global set of ground truth keywords for that movie.\n",
    "\n",
    "- **Metrics**: we compute **Precision**, **Recall**, and **F1-score** based on the number of approximate matches between the predicted and ground truth keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_keywords(all_pred_keywords, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global precision, recall, and F1-score across a dataset using approximate matching.\n",
    "\n",
    "    This function compares predicted keywords to ground truth keywords for each review.\n",
    "    Matching is performed using approximate string comparison, and each ground truth keyword\n",
    "    can be matched only once to ensure fairness. The metrics are aggregated globally,\n",
    "    not per-review.\n",
    "\n",
    "    Args:\n",
    "        all_pred_keywords (List[List[str]]): \n",
    "            A list where each element is a list of predicted keywords for a single review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list where each element is a list of ground truth keywords for the corresponding review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Global precision, recall, and F1-score based on approximate matching.\n",
    "    \"\"\"\n",
    "    global_match_count = 0     # Total number of matched keywords across all reviews\n",
    "    global_pred_count = 0      # Total number of predicted keywords\n",
    "    global_gt_count = 0        # Total number of ground truth keywords\n",
    "\n",
    "    # Iterate through each review's predictions and ground truths\n",
    "    for pred_keywords, gt_keywords in zip(all_pred_keywords, all_gt_keywords):\n",
    "        # Normalize and sort keywords to ensure consistent behavior\n",
    "        pred_keywords = sorted([normalize_kw(k) for k in pred_keywords])\n",
    "        gt_keywords = sorted([normalize_kw(k) for k in gt_keywords])\n",
    "\n",
    "        global_pred_count += len(pred_keywords)\n",
    "        global_gt_count += len(gt_keywords)\n",
    "\n",
    "        matched_gts = set()  # Track which ground truth keywords have already been matched\n",
    "\n",
    "        for pred in pred_keywords:\n",
    "            for gt in gt_keywords:\n",
    "                if gt not in matched_gts and is_approx_match(pred, [gt]):\n",
    "                    global_match_count += 1\n",
    "                    matched_gts.add(gt)  # Avoid matching the same GT keyword multiple times\n",
    "                    break\n",
    "\n",
    "    # Compute global metrics\n",
    "    precision = global_match_count / global_pred_count if global_pred_count else 0\n",
    "    recall = global_match_count / global_gt_count if global_gt_count else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic – Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0199",
   "metadata": {},
   "source": [
    "This section evaluates two keyword extraction models — **base** and **sentiment-enhanced** — against the ground truth annotations.\n",
    "\n",
    "For each model, we collect all predicted keywords across all reviews in the selected movie and compare them to the ground truth keywords using **binary approximate matching**.\n",
    "\n",
    "The evaluation computes **global precision, recall, and F1-score**, considering the entire set of predictions and ground truth keywords as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d333544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_71af6\">\n",
       "  <caption>Global Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_71af6_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_71af6_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_71af6_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_71af6_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_71af6_row0_col0\" class=\"data row0 col0\" >0.9125</td>\n",
       "      <td id=\"T_71af6_row0_col1\" class=\"data row0 col1\" >0.3685</td>\n",
       "      <td id=\"T_71af6_row0_col2\" class=\"data row0 col2\" >0.5250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_71af6_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_71af6_row1_col0\" class=\"data row1 col0\" >0.8805</td>\n",
       "      <td id=\"T_71af6_row1_col1\" class=\"data row1 col1\" >0.3391</td>\n",
       "      <td id=\"T_71af6_row1_col2\" class=\"data row1 col2\" >0.4896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17d630790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie\n",
    "ground_truth_keywords = [normalize_kw(kw) for kw in kw_ground_truth[\"Keyword\"].tolist()]\n",
    "\n",
    "# Dictionary to store all predicted keywords per model (across all reviews)\n",
    "all_predictions = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_keywords = [\n",
    "                normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)\n",
    "            ]\n",
    "            \n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_kw = [kw for kw in predicted_keywords if kw not in seen and not seen.add(kw)]\n",
    "\n",
    "            all_predictions[model].append(unique_kw)\n",
    "\n",
    "# Evaluate each model globally\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precision, recall, f1 = evaluate_keywords(\n",
    "        all_predictions[model],  # List of lists\n",
    "        ground_truth_keywords\n",
    "    )\n",
    "\n",
    "    summary[model] = {\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Convert and display\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "summary_df.columns = [\"Precision\", \"Recall\", \"F1-score\"]\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055848",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the model’s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def score_gt_keywords_from_rank(gt_keywords):\n",
    "    return [\n",
    "        (kw, 1 / math.log2(i + 2)) for i, kw in enumerate(gt_keywords)\n",
    "    ]\n",
    "\n",
    "def evaluate_keywords_weighted(all_predicted_kw_score, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global weighted precision, recall, and F1-score across multiple reviews.\n",
    "    Matching is performed using approximate matching. Each predicted keyword contributes\n",
    "    to the precision proportionally to its confidence score. Each ground truth keyword contributes\n",
    "    to recall proportionally to its rank-based score.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Weighted precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    total_pred_score = 0.0  # Sum of all predicted keyword scores\n",
    "    matched_pred_score = 0.0  # Sum of scores of correctly predicted keywords\n",
    "    total_gt_score = 0.0  # Sum of all ground truth scores\n",
    "    matched_gt_score = 0.0  # Sum of scores of matched ground truth keywords\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize and score ground truth keywords\n",
    "        gt_kw_scored = score_gt_keywords_from_rank([normalize_kw(k) for k in gt_kw])\n",
    "        pred_kw_score = [\n",
    "            (normalize_kw(kw), score) for kw, score in pred_kw_score if isinstance(kw, str)\n",
    "        ]\n",
    "\n",
    "        total_pred_score += sum(score for _, score in pred_kw_score)\n",
    "        total_gt_score += sum(score for _, score in gt_kw_scored)\n",
    "\n",
    "        matched_gts = set()\n",
    "\n",
    "        for kw, score in pred_kw_score:\n",
    "            for gt_kw, gt_score in gt_kw_scored:\n",
    "                if gt_kw not in matched_gts and is_approx_match(kw, [gt_kw]):\n",
    "                    matched_pred_score += score\n",
    "                    matched_gt_score += gt_score\n",
    "                    matched_gts.add(gt_kw)\n",
    "                    break\n",
    "\n",
    "    precision = matched_pred_score / total_pred_score if total_pred_score > 0 else 0\n",
    "    recall = matched_gt_score / total_gt_score if total_gt_score > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80dd7",
   "metadata": {},
   "source": [
    "In this section, we evaluate the overall performance of each model using **score-aware metrics** computed **globally across all reviews**:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**: These metrics incorporate the **confidence scores** assigned to each predicted keyword, reflecting how much of the model’s confidence is placed on correct predictions.\n",
    "\n",
    "This global evaluation provides a holistic view of each model’s effectiveness in ranking and selecting relevant keywords across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_60e30\">\n",
       "  <caption>Global Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_60e30_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_60e30_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_60e30_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_60e30_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_60e30_row0_col0\" class=\"data row0 col0\" >0.9176</td>\n",
       "      <td id=\"T_60e30_row0_col1\" class=\"data row0 col1\" >0.4902</td>\n",
       "      <td id=\"T_60e30_row0_col2\" class=\"data row0 col2\" >0.6390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_60e30_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_60e30_row1_col0\" class=\"data row1 col0\" >0.8955</td>\n",
       "      <td id=\"T_60e30_row1_col1\" class=\"data row1 col1\" >0.4489</td>\n",
       "      <td id=\"T_60e30_row1_col2\" class=\"data row1 col2\" >0.5981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17c1b88e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Prepare data structures to hold predictions for each model\n",
    "all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Collect predictions and GT for each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = [(kw, score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "            \n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "            all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "# Dictionary to store global evaluation results\n",
    "weighted_summary = {}\n",
    "\n",
    "# Evaluate each model globally\n",
    "for model in models_to_evaluate:\n",
    "    preds = all_predicted_kw_score[model]\n",
    "\n",
    "    # Global weighted metrics\n",
    "    w_precision, w_recall, w_f1 = evaluate_keywords_weighted(preds, ground_truth_keywords)\n",
    "\n",
    "    # Store results\n",
    "    weighted_summary[model] = {\n",
    "        \"weighted_precision\": round(w_precision, 4),\n",
    "        \"weighted_recall\": round(w_recall, 4),\n",
    "        \"weighted_f1\": round(w_f1, 4),\n",
    "    }\n",
    "\n",
    "# Convert summary to DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Models as rows\n",
    "\n",
    "# Rename columns\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "]\n",
    "\n",
    "# Display final table\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabef0de",
   "metadata": {},
   "source": [
    "## Semantic Evaluation (Base vs Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16c67f",
   "metadata": {},
   "source": [
    "In this section, we evaluate and compare the **Base** and **Sentiment-enhanced** keyword extraction models using a **semantic similarity approach** based on contextual embeddings.\n",
    "\n",
    "Traditional evaluation metrics rely on exact or approximate string matching between predicted and ground truth keywords. However, this approach may miss semantically related terms that differ lexically but convey the same meaning — such as *\"scam\"* and *\"fraud\"*.\n",
    "\n",
    "To address this limitation, we adopt a **global semantic evaluation**, where all predicted and ground truth keywords across the dataset are compared using **dense sentence embeddings** generated by a pre-trained transformer (e.g., Sentence-BERT).\n",
    "\n",
    "#### **Semantic Evaluation Procedure**\n",
    "\n",
    "1. **Embedding Keywords Globally**  \n",
    "   All predicted and ground truth keywords across all reviews are embedded into high-dimensional vectors using the same transformer model. Ground truth keywords are embedded **once**, and all vectors are normalized to allow cosine similarity comparisons.\n",
    "\n",
    "2. **Computing Similarity Matrix**  \n",
    "   For each model, we compute a cosine similarity matrix between **all predicted keywords** and **all ground truth keywords**.\n",
    "\n",
    "3. **Matching Threshold**  \n",
    "   A predicted keyword is considered a **semantic match** if its cosine similarity with at least one ground truth keyword exceeds a fixed threshold (e.g., **0.65**). This allows for flexible yet meaningful semantic alignment.\n",
    "\n",
    "4. **Global Semantic Precision**  \n",
    "   The proportion of predicted keywords that have at least one semantic match in the ground truth. This reflects how many of the model's predictions are semantically relevant.\n",
    "\n",
    "5. **Global Semantic Recall**  \n",
    "   The proportion of ground truth keywords that are captured by semantically similar predictions. This indicates how well the model covers the key concepts.\n",
    "\n",
    "6. **Global Semantic F1-score**  \n",
    "   The harmonic mean of semantic precision and recall, summarizing both relevance and coverage into a single score.\n",
    "\n",
    "This evaluation:\n",
    "\n",
    "- Is **more robust** than string-based metrics.\n",
    "- **Captures meaning**, not just surface forms.\n",
    "- Helps evaluate models that paraphrase or generalize beyond exact matches.\n",
    "\n",
    "This evaluation complements previous metrics and provides a more **realistic estimate** of how well the models capture the essence of user-annotated keywords in a global and context-aware manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f779851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model from the SentenceTransformers family\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model to generate contextual embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "def embed_keywords(keywords, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute sentence embeddings for a list of keyword strings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    keywords : List[str]\n",
    "        A list of keyword strings to encode.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Normalized embeddings tensor of shape (num_keywords, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Return empty tensor if input list is empty\n",
    "    if not keywords:\n",
    "        return torch.empty(0, encoder.config.hidden_size).to(device)\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the encoder to get hidden states\n",
    "        outputs = encoder(**inputs)\n",
    "\n",
    "        # Use mean pooling on the last hidden state to get fixed-size embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity computations\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def evaluate_semantic_keywords(all_pred_keywords, gt_keywords, threshold=0.65, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute global semantic precision, recall, and F1 score between all predicted keywords\n",
    "    and ground truth keywords using cosine similarity over embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    all_pred_keywords : List[List[str]]\n",
    "        List of predicted keywords for each review.\n",
    "    gt_keywords : List[str]\n",
    "        Global list of ground truth keywords for the movie.\n",
    "    threshold : float\n",
    "        Cosine similarity threshold for considering a match.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    precision : float\n",
    "    recall : float\n",
    "    f1 : float\n",
    "    \"\"\"\n",
    "    # Early return if either set is empty\n",
    "    if len(all_pred_keywords) == 0 or len(gt_keywords) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute embeddings\n",
    "    pred_emb = embed_keywords(all_pred_keywords, device=device)\n",
    "    gt_emb = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    sims = torch.matmul(pred_emb, gt_emb.T)\n",
    "\n",
    "    # Match counting based on threshold\n",
    "    pred_matches = (sims > threshold).any(dim=1).float().sum().item()\n",
    "    gt_matches = (sims > threshold).any(dim=0).float().sum().item()\n",
    "\n",
    "    precision = pred_matches / len(all_pred_keywords)\n",
    "    recall = gt_matches / len(gt_keywords)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce9bda",
   "metadata": {},
   "source": [
    "### Semantic Evaluation of Base and Sentiment Models Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa566c",
   "metadata": {},
   "source": [
    "In this step, we evaluate the **semantic similarity** between the predicted keywords of two models — **Base** and **Sentiment-enhanced** — and the ground truth keywords using **sentence embeddings**.\n",
    "\n",
    "Unlike exact or approximate string matching, this method leverages **contextual embeddings** from a pre-trained transformer to assess how semantically close the predicted keywords are to the reference keywords.\n",
    "\n",
    "The evaluation procedure is as follows:\n",
    "\n",
    "- We extract only the **text** of the predicted keywords for each model, discarding their confidence scores.\n",
    "- We embed all **predicted** and **ground truth** keywords using the same sentence transformer model.\n",
    "- Embeddings are **normalized** to ensure cosine similarity is a valid similarity measure.\n",
    "- For each predicted keyword, we compute the **cosine similarity** with all ground truth keywords.\n",
    "- A predicted keyword is considered a **semantic match** if its similarity with any ground truth keyword exceeds a fixed threshold (e.g., **0.65**).\n",
    "\n",
    "Once all matches are determined across all reviews of the selected movie, we compute:\n",
    "\n",
    "- **Semantic Precision**: Fraction of all predicted keywords (global) that have a semantic match.\n",
    "- **Semantic Recall**: Fraction of all ground truth keywords that are matched by at least one semantically similar predicted keyword.\n",
    "- **Semantic F1-score**: Harmonic mean of semantic precision and recall.\n",
    "\n",
    "This global semantic evaluation better reflects the models’ ability to capture **meaningful and relevant keywords**, even when the wording differs from the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbd11b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_877ad\">\n",
       "  <caption>Global Semantic-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_877ad_level0_col0\" class=\"col_heading level0 col0\" >Semantic_Precision</th>\n",
       "      <th id=\"T_877ad_level0_col1\" class=\"col_heading level0 col1\" >Semantic_Recall</th>\n",
       "      <th id=\"T_877ad_level0_col2\" class=\"col_heading level0 col2\" >Semantic_F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_877ad_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_877ad_row0_col0\" class=\"data row0 col0\" >0.9628</td>\n",
       "      <td id=\"T_877ad_row0_col1\" class=\"data row0 col1\" >0.7978</td>\n",
       "      <td id=\"T_877ad_row0_col2\" class=\"data row0 col2\" >0.8726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_877ad_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_877ad_row1_col0\" class=\"data row1 col0\" >0.9616</td>\n",
       "      <td id=\"T_877ad_row1_col1\" class=\"data row1 col1\" >0.8162</td>\n",
       "      <td id=\"T_877ad_row1_col2\" class=\"data row1 col2\" >0.8830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x105182490>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute embeddings for the ground truth keywords once per selected movie\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Dictionary to collect all predicted keywords per model (without duplicates)\n",
    "all_predictions = {model: set() for model in models_to_evaluate}\n",
    "\n",
    "# Collect predicted keywords across all reviews (as a set for uniqueness)\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            # Extract keyword strings and normalize\n",
    "            pred_kw = [normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            all_predictions[model].update(pred_kw)  # Add to set (no duplicates)\n",
    "\n",
    "# Compute semantic evaluation globally for each model\n",
    "semantic_scores = []\n",
    "for model in models_to_evaluate:\n",
    "    pred_kw = list(all_predictions[model])  # Convert back to list\n",
    "    precision, recall, f1 = evaluate_semantic_keywords(pred_kw, gt_keywords, device=device, threshold=0.65)\n",
    "\n",
    "    semantic_scores.append({\n",
    "        \"Model\": model,\n",
    "        \"Semantic_Precision\": round(precision, 4),\n",
    "        \"Semantic_Recall\": round(recall, 4),\n",
    "        \"Semantic_F1\": round(f1, 4)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and format\n",
    "summary_df = pd.DataFrame(semantic_scores).set_index(\"Model\")\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Semantic-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bd7af",
   "metadata": {},
   "source": [
    "## Sentiment-Aware Evaluation of Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b08c7",
   "metadata": {},
   "source": [
    "In this section, we compare **KeyBERTSentimentAware**, our sentiment-enhanced extension of KeyBERT, with the **original KeyBERT** model from a **sentiment perspective**.  \n",
    "While traditional keyword extraction focuses on semantic relevance, here we aim to evaluate whether the extracted keywords also reflect the **emotional tone** of the content they are drawn from. \n",
    "\n",
    "To address this, we introduce two sentiment-based evaluation metrics that assess how emotionally aligned the predicted keywords are with the sentiment expressed in the original content.\n",
    "\n",
    "#### 1. Average Sentiment Similarity (ASS)  \n",
    "\n",
    "This metric estimates how close the **overall emotional tone** of the predicted keywords is to a reference value derived from the ground truth. It reflects:\n",
    "\n",
    "*“On average, how emotionally aligned are the extracted keywords with the ground truth?”*\n",
    "\n",
    "**How it works**\n",
    "\n",
    "- Each review has a set of predicted keywords.\n",
    "- Each keyword is converted into a sentiment score using a predefined formula:  \n",
    "  `sentiment = 1.0 * pos + 0.5 * neu + 0.0 * neg`\n",
    "- The resulting sentiment vector is compared to a reference vector of the same length, where each element corresponds to the same ground-truth sentiment value.\n",
    "- The similarity between the two vectors is computed using **cosine similarity**, which measures how directionally aligned the vectors are in sentiment space.\n",
    "- The similarity score for each review is **clamped between 0 and 1** to ensure numerical stability and interpretability as a bounded alignment measure.\n",
    "- Finally, the average similarity is computed **across all reviews**, resulting in a single global score.\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- Values close to 1 indicate that the predicted keywords align emotionally with the ground truth.\n",
    "- Values close to 0 suggest an emotional mismatch, even if keywords are semantically reasonable.\n",
    "\n",
    "This metric provides a **global view of emotional alignment**, making it effective when the model captures overall sentiment well. However, since it relies on a uniform reference vector and aggregates all reviews into a single score, it may overlook variability across individual cases or be affected by outlier behavior from neutral terms.\n",
    "\n",
    "#### 2. Review-wise Sampled Sentiment Similarity (RSSS)  \n",
    "\n",
    "To introduce greater robustness and sensitivity to local fluctuations, we extend the previous approach with a **sampled, per-review variant**. This version emphasizes individual reviews and mitigates the effect of biased or skewed sentiment vectors.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "For each review:\n",
    "- Five ground truth keywords are randomly sampled and converted into a sentiment vector using the same scoring formula.\n",
    "- Five predicted keywords are also randomly sampled (this process is repeated multiple times per review).\n",
    "- The cosine similarity is computed between the sampled ground truth and prediction vectors.\n",
    "- Each similarity score is **clamped between 0 and 1** to ensure comparability and avoid values outside the interpretable range due to floating-point errors.\n",
    "- The average similarity for that review is computed across all sampling iterations.\n",
    "- In the end, the final score is the **average of all per-review scores**.\n",
    "\n",
    "This method increases **granularity and robustness**, by accounting for the diversity and variability of both the predicted and ground truth keywords at the review level. Unlike the global method, it captures cases where some predictions are strongly aligned emotionally while others are not.\n",
    "\n",
    "By averaging over multiple random samples and reviews, this metric produces a **more stable and fair estimate** of emotional alignment, especially in scenarios where keyword sets are heterogeneous.\n",
    "\n",
    "#### Importance for our work\n",
    "\n",
    "Our work centers on **topic modeling applied to movie reviews**, where both semantic content and emotional tone are central to user interpretation. Traditional evaluations focus solely on lexical or semantic overlap. However, films are reviewed not just for what they’re about, but for **how they make people feel**. Incorporating sentiment-aware evaluations helps us assess whether a keyword extraction model highlights emotionally charged or affectively neutral aspects of the content. \n",
    "\n",
    "These metrics therefore provide an essential layer of analysis to ensure that the extracted keywords are not only topically relevant, but also **emotionally faithful** to the underlying user opinion — a critical factor in domains like entertainment, recommendation systems, and opinion mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_sentiment_similarity(all_sentiments_per_review, gt_sentiment_value):\n",
    "    \"\"\"\n",
    "    Computes the average cosine similarity between each sentiment vector in a list of reviews\n",
    "    and a ground truth sentiment value vector.\n",
    "\n",
    "    Each sentiment vector is compared to a vector of the same length filled with the ground truth value.\n",
    "    Similarity values are clamped to [0.0, 1.0] before averaging.\n",
    "\n",
    "    Parameters:\n",
    "        all_sentiments_per_review (list of list of float): List of sentiment score sequences per review.\n",
    "        gt_sentiment_value (float): The scalar sentiment reference value (typically derived from ground truth keywords).\n",
    "\n",
    "    Returns:\n",
    "        float or None: The average cosine similarity, or None if no valid sentiment vectors are found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert each sentiment list into a row vector (1 x n), only if it has more than one element\n",
    "    vectors = [np.array(s).reshape(1, -1) for s in all_sentiments_per_review if len(s) > 1]\n",
    "\n",
    "    # If no valid vectors are present, return None\n",
    "    if not vectors:\n",
    "        return None\n",
    "\n",
    "    # Compute cosine similarity between each sentiment vector and a reference vector\n",
    "    # The reference vector is composed of the same value (gt_sentiment_value) repeated\n",
    "    similarities = [\n",
    "        cosine_similarity(v, [[gt_sentiment_value] * v.shape[1]])[0][0]\n",
    "        for v in vectors\n",
    "    ]\n",
    "\n",
    "    # Clamp similarities to be within [0, 1] range to ensure valid similarity values\n",
    "    similarities = [max(0.0, min(1.0, s)) for s in similarities]\n",
    "\n",
    "    # Return the average similarity\n",
    "    return np.mean(similarities)\n",
    "\n",
    "\n",
    "def compute_reviewwise_sentiment_similarity(df, gt_keywords, pred_col, analyzer, k_samples=20, seed=42):\n",
    "    \"\"\"\n",
    "    Computes the average cosine similarity between sentiment vectors of predicted keywords \n",
    "    and a fixed sentiment vector of 5 sampled ground truth keywords, for each review.\n",
    "\n",
    "    For each review, k_samples sentiment vectors are computed by randomly sampling 5 predicted keywords. \n",
    "    These vectors are compared to the ground truth sentiment vector using cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing reviews and predicted keywords.\n",
    "        gt_keywords (list of str): List of ground truth keywords.\n",
    "        pred_col (str): Column name containing predicted keywords as (keyword, score) tuples.\n",
    "        analyzer (object): Sentiment analyzer with a `.polarity_scores(text)` method.\n",
    "        k_samples (int): Number of random samples of predicted keywords per review.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        float or None: Average cosine similarity across all reviews, or None if not computable.\n",
    "    \"\"\"\n",
    "    # Create independent random generator for deterministic sampling\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # Filter valid ground truth keywords\n",
    "    valid_gt = [kw for kw in gt_keywords if isinstance(kw, str)]\n",
    "\n",
    "    # If not enough ground truth keywords to sample 5, exit early\n",
    "    if len(valid_gt) < 5:\n",
    "        return None\n",
    "\n",
    "    # Sample 5 ground truth keywords and compute their sentiment vector\n",
    "    sampled_gt = rng.sample(valid_gt, 5)\n",
    "    gt_vec = [\n",
    "        1.0 * analyzer.polarity_scores(kw)[\"pos\"] + 0.5 * analyzer.polarity_scores(kw)[\"neu\"]\n",
    "        for kw in sampled_gt\n",
    "    ]\n",
    "    gt_vec = np.array(gt_vec).reshape(1, -1)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    # Iterate over each review in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        if pred_col not in row or not isinstance(row[pred_col], list):\n",
    "            continue\n",
    "\n",
    "        # Extract only valid string keywords\n",
    "        pred_keywords = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "        if len(pred_keywords) < 5:\n",
    "            continue\n",
    "\n",
    "        review_sims = []\n",
    "\n",
    "        for _ in range(k_samples):\n",
    "            # Sample 5 predicted keywords using the same RNG\n",
    "            sampled_pred = rng.sample(pred_keywords, 5)\n",
    "\n",
    "            # Compute sentiment vector for the predicted keywords\n",
    "            pred_vec = [\n",
    "                1.0 * analyzer.polarity_scores(kw)[\"pos\"] + 0.5 * analyzer.polarity_scores(kw)[\"neu\"]\n",
    "                for kw in sampled_pred\n",
    "            ]\n",
    "            pred_vec = np.array(pred_vec).reshape(1, -1)\n",
    "\n",
    "            # Compute and clamp cosine similarity\n",
    "            sim = cosine_similarity(gt_vec, pred_vec)[0][0]\n",
    "            review_sims.append(max(0.0, min(1.0, sim)))\n",
    "\n",
    "        # Average similarity for this review\n",
    "        similarities.append(np.mean(review_sims))\n",
    "\n",
    "    if not similarities:\n",
    "        return None\n",
    "\n",
    "    return round(np.mean(similarities), 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c53932",
   "metadata": {},
   "source": [
    "### Sentiment-Aware Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47dae1",
   "metadata": {},
   "source": [
    "In this section, we evaluate the sentiment alignment of each model’s predicted keywords using two complementary metrics: **Average Sentiment Similarity (ASS)** and **Review-wise Sampled Sentiment Similarity (RSSS)**.\n",
    "\n",
    "These metrics are designed to assess whether the keywords extracted by each model reflect the **emotional tone** of the movie they describe. While keyword extraction models typically focus on semantic relevance, our goal is to understand how well they capture **affective meaning**.\n",
    "\n",
    "- **ASS** measures the **global emotional similarity** between the predicted keywords and a sentiment reference derived from the ground truth. It provides a scalar indicator of how closely, on average, the extracted keywords align with the expected emotional tone.\n",
    "\n",
    "- **RSSS** performs a more **fine-grained, review-level evaluation**, comparing the sentiment of predicted keywords to that of sampled ground truth keywords. By introducing random sampling and averaging over reviews, this metric highlights whether the model consistently produces emotionally coherent predictions across different user opinions.\n",
    "\n",
    "Higher values in both metrics suggest that the model is more **emotionally aligned** with the content, favoring keywords that not only describe the topic but also convey its **affective intensity**.\n",
    "\n",
    "The results are summarized in the following table to enable **global comparison across models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b0c92f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7721c\">\n",
       "  <caption>Global Sentiment-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7721c_level0_col0\" class=\"col_heading level0 col0\" >Avg Sentiment Similarity</th>\n",
       "      <th id=\"T_7721c_level0_col1\" class=\"col_heading level0 col1\" >Review-wise Sampled Similarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7721c_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_7721c_row0_col0\" class=\"data row0 col0\" >0.9485</td>\n",
       "      <td id=\"T_7721c_row0_col1\" class=\"data row0 col1\" >0.9214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7721c_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_7721c_row1_col0\" class=\"data row1 col0\" >0.9621</td>\n",
       "      <td id=\"T_7721c_row1_col1\" class=\"data row1 col1\" >0.9344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17e9861f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Ground Truth\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "gt_keywords = [kw for kw in gt_keywords if isinstance(kw, str)]\n",
    "gt_keywords = list(gt_keywords)  # Optional deduplication\n",
    "\n",
    "# Compute reference sentiment from GT keywords\n",
    "gt_sentiments = []\n",
    "for kw in gt_keywords:\n",
    "    score = analyzer.polarity_scores(kw)\n",
    "    sentiment = 1.0 * score[\"pos\"] + 0.5 * score[\"neu\"]\n",
    "    gt_sentiments.append(sentiment)\n",
    "\n",
    "gt_sentiment_value = np.mean(gt_sentiments) if gt_sentiments else 0.5\n",
    "\n",
    "# Models and predicted keyword collection\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "all_predictions = {model: set() for model in models_to_evaluate}\n",
    "all_sentiments_per_model = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        review_sentiments = []\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            for kw, _ in row[pred_col]:\n",
    "                if isinstance(kw, str):\n",
    "                    # Add keyword to global set\n",
    "                    all_predictions[model].add(kw)\n",
    "\n",
    "                    # Compute sentiment score\n",
    "                    score = analyzer.polarity_scores(kw)\n",
    "                    sentiment = 1.0 * score[\"pos\"] + 0.5 * score[\"neu\"]\n",
    "                    review_sentiments.append(sentiment)\n",
    "\n",
    "        if len(review_sentiments) > 1:\n",
    "            all_sentiments_per_model[model].append(review_sentiments)\n",
    "\n",
    "# Evaluation\n",
    "sentiment_scores = []\n",
    "\n",
    "for model in models_to_evaluate:\n",
    "    pred_keywords = list(all_predictions[model])\n",
    "\n",
    "    # ASS: global sentiment similarity with average GT sentiment\n",
    "    ass = compute_average_sentiment_similarity(all_sentiments_per_model[model], gt_sentiment_value)\n",
    "\n",
    "    # RSSS: review-wise cosine similarity with sampled GT\n",
    "    rsss = compute_reviewwise_sentiment_similarity(\n",
    "        selected_film,\n",
    "        gt_keywords=gt_keywords,\n",
    "        pred_col=f\"keywords_{model}\",\n",
    "        analyzer=analyzer,\n",
    "        k_samples=20,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    sentiment_scores.append({\n",
    "        \"Model\": model,\n",
    "        \"Avg Sentiment Similarity\": round(ass, 4) if ass is not None else None,\n",
    "        \"Review-wise Sampled Similarity\": round(rsss, 4) if rsss is not None else None\n",
    "    })\n",
    "\n",
    "# Output as DataFrame\n",
    "sentiment_df = pd.DataFrame(sentiment_scores).set_index(\"Model\")\n",
    "sentiment_df.style.format(precision=4).set_caption(\"Global Sentiment-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187183",
   "metadata": {},
   "source": [
    "This section automatically processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains predicted keywords generated by different models.\n",
    "\n",
    "For **each movie**:\n",
    "\n",
    "- The corresponding **ground truth keywords** are loaded.\n",
    "- Predicted keywords from both models — **Base** and **Sentiment-aware** — are aggregated across all reviews.\n",
    "- The following **global evaluation metrics** are computed:\n",
    "\n",
    "#### Unweighted Metrics\n",
    "- **Precision**, **Recall**, and **F1-score**  \n",
    "  Based on approximate string matching between predicted and ground truth keywords, without considering prediction scores.\n",
    "\n",
    "#### Score-aware Metrics\n",
    "- **Weighted Precision**, **Weighted Recall**, **Weighted F1-score**  \n",
    "  Evaluate prediction correctness while incorporating confidence scores.\n",
    "  \n",
    "- **nDCG@5**  \n",
    "  Measures the ranking quality of the top 5 predicted keywords, rewarding correct keywords ranked higher.\n",
    "\n",
    "#### Semantic Metrics\n",
    "- **Semantic Precision**, **Semantic Recall**, **Semantic F1-score**  \n",
    "  Computed using cosine similarity between **sentence embeddings** of predicted and reference keywords.\n",
    "\n",
    "#### Sentiment Alignment Metrics\n",
    "- **Average Sentiment Similarity (ASS)**  \n",
    "  Measures the global emotional alignment between predicted keywords and the average sentiment of the ground truth keywords.\n",
    "\n",
    "- **Review-wise Sampled Sentiment Similarity (RSSS)**  \n",
    "  Computes a robust, review-level cosine similarity between the sentiment of sampled predicted keywords and sampled ground truth keywords, averaged across reviews.\n",
    "\n",
    "All metrics are computed **globally per movie**, not per review, and the results are compiled into a **comprehensive summary table** for comparison across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79e56b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c1774\">\n",
       "  <caption>Global Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c1774_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_c1774_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_c1774_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_c1774_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_c1774_level0_col4\" class=\"col_heading level0 col4\" >F1-score</th>\n",
       "      <th id=\"T_c1774_level0_col5\" class=\"col_heading level0 col5\" >Weighted Precision</th>\n",
       "      <th id=\"T_c1774_level0_col6\" class=\"col_heading level0 col6\" >Weighted Recall</th>\n",
       "      <th id=\"T_c1774_level0_col7\" class=\"col_heading level0 col7\" >Weighted F1-score</th>\n",
       "      <th id=\"T_c1774_level0_col8\" class=\"col_heading level0 col8\" >Semantic Precision</th>\n",
       "      <th id=\"T_c1774_level0_col9\" class=\"col_heading level0 col9\" >Semantic Recall</th>\n",
       "      <th id=\"T_c1774_level0_col10\" class=\"col_heading level0 col10\" >Semantic F1-score</th>\n",
       "      <th id=\"T_c1774_level0_col11\" class=\"col_heading level0 col11\" >Avg Sentiment Similarity</th>\n",
       "      <th id=\"T_c1774_level0_col12\" class=\"col_heading level0 col12\" >Review-wise Sampled Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c1774_row0_col0\" class=\"data row0 col0\" >GoodBadUgly</td>\n",
       "      <td id=\"T_c1774_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row0_col2\" class=\"data row0 col2\" >0.9341</td>\n",
       "      <td id=\"T_c1774_row0_col3\" class=\"data row0 col3\" >0.3612</td>\n",
       "      <td id=\"T_c1774_row0_col4\" class=\"data row0 col4\" >0.5210</td>\n",
       "      <td id=\"T_c1774_row0_col5\" class=\"data row0 col5\" >0.9352</td>\n",
       "      <td id=\"T_c1774_row0_col6\" class=\"data row0 col6\" >0.4857</td>\n",
       "      <td id=\"T_c1774_row0_col7\" class=\"data row0 col7\" >0.6394</td>\n",
       "      <td id=\"T_c1774_row0_col8\" class=\"data row0 col8\" >0.8691</td>\n",
       "      <td id=\"T_c1774_row0_col9\" class=\"data row0 col9\" >0.7888</td>\n",
       "      <td id=\"T_c1774_row0_col10\" class=\"data row0 col10\" >0.8270</td>\n",
       "      <td id=\"T_c1774_row0_col11\" class=\"data row0 col11\" >0.9321</td>\n",
       "      <td id=\"T_c1774_row0_col12\" class=\"data row0 col12\" >0.6641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c1774_row1_col0\" class=\"data row1 col0\" >GoodBadUgly</td>\n",
       "      <td id=\"T_c1774_row1_col1\" class=\"data row1 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row1_col2\" class=\"data row1 col2\" >0.9074</td>\n",
       "      <td id=\"T_c1774_row1_col3\" class=\"data row1 col3\" >0.3391</td>\n",
       "      <td id=\"T_c1774_row1_col4\" class=\"data row1 col4\" >0.4937</td>\n",
       "      <td id=\"T_c1774_row1_col5\" class=\"data row1 col5\" >0.9104</td>\n",
       "      <td id=\"T_c1774_row1_col6\" class=\"data row1 col6\" >0.4481</td>\n",
       "      <td id=\"T_c1774_row1_col7\" class=\"data row1 col7\" >0.6006</td>\n",
       "      <td id=\"T_c1774_row1_col8\" class=\"data row1 col8\" >0.7477</td>\n",
       "      <td id=\"T_c1774_row1_col9\" class=\"data row1 col9\" >0.8043</td>\n",
       "      <td id=\"T_c1774_row1_col10\" class=\"data row1 col10\" >0.7750</td>\n",
       "      <td id=\"T_c1774_row1_col11\" class=\"data row1 col11\" >0.9600</td>\n",
       "      <td id=\"T_c1774_row1_col12\" class=\"data row1 col12\" >0.6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c1774_row2_col0\" class=\"data row2 col0\" >HarryPotter</td>\n",
       "      <td id=\"T_c1774_row2_col1\" class=\"data row2 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row2_col2\" class=\"data row2 col2\" >0.9194</td>\n",
       "      <td id=\"T_c1774_row2_col3\" class=\"data row2 col3\" >0.3394</td>\n",
       "      <td id=\"T_c1774_row2_col4\" class=\"data row2 col4\" >0.4957</td>\n",
       "      <td id=\"T_c1774_row2_col5\" class=\"data row2 col5\" >0.9202</td>\n",
       "      <td id=\"T_c1774_row2_col6\" class=\"data row2 col6\" >0.4552</td>\n",
       "      <td id=\"T_c1774_row2_col7\" class=\"data row2 col7\" >0.6091</td>\n",
       "      <td id=\"T_c1774_row2_col8\" class=\"data row2 col8\" >0.9016</td>\n",
       "      <td id=\"T_c1774_row2_col9\" class=\"data row2 col9\" >0.8820</td>\n",
       "      <td id=\"T_c1774_row2_col10\" class=\"data row2 col10\" >0.8917</td>\n",
       "      <td id=\"T_c1774_row2_col11\" class=\"data row2 col11\" >0.9694</td>\n",
       "      <td id=\"T_c1774_row2_col12\" class=\"data row2 col12\" >0.8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c1774_row3_col0\" class=\"data row3 col0\" >HarryPotter</td>\n",
       "      <td id=\"T_c1774_row3_col1\" class=\"data row3 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row3_col2\" class=\"data row3 col2\" >0.9023</td>\n",
       "      <td id=\"T_c1774_row3_col3\" class=\"data row3 col3\" >0.3251</td>\n",
       "      <td id=\"T_c1774_row3_col4\" class=\"data row3 col4\" >0.4779</td>\n",
       "      <td id=\"T_c1774_row3_col5\" class=\"data row3 col5\" >0.9043</td>\n",
       "      <td id=\"T_c1774_row3_col6\" class=\"data row3 col6\" >0.4348</td>\n",
       "      <td id=\"T_c1774_row3_col7\" class=\"data row3 col7\" >0.5873</td>\n",
       "      <td id=\"T_c1774_row3_col8\" class=\"data row3 col8\" >0.8966</td>\n",
       "      <td id=\"T_c1774_row3_col9\" class=\"data row3 col9\" >0.8976</td>\n",
       "      <td id=\"T_c1774_row3_col10\" class=\"data row3 col10\" >0.8971</td>\n",
       "      <td id=\"T_c1774_row3_col11\" class=\"data row3 col11\" >0.9623</td>\n",
       "      <td id=\"T_c1774_row3_col12\" class=\"data row3 col12\" >0.8867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c1774_row4_col0\" class=\"data row4 col0\" >IndianaJones</td>\n",
       "      <td id=\"T_c1774_row4_col1\" class=\"data row4 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row4_col2\" class=\"data row4 col2\" >0.9125</td>\n",
       "      <td id=\"T_c1774_row4_col3\" class=\"data row4 col3\" >0.3678</td>\n",
       "      <td id=\"T_c1774_row4_col4\" class=\"data row4 col4\" >0.5243</td>\n",
       "      <td id=\"T_c1774_row4_col5\" class=\"data row4 col5\" >0.9176</td>\n",
       "      <td id=\"T_c1774_row4_col6\" class=\"data row4 col6\" >0.4902</td>\n",
       "      <td id=\"T_c1774_row4_col7\" class=\"data row4 col7\" >0.6390</td>\n",
       "      <td id=\"T_c1774_row4_col8\" class=\"data row4 col8\" >0.9628</td>\n",
       "      <td id=\"T_c1774_row4_col9\" class=\"data row4 col9\" >0.7978</td>\n",
       "      <td id=\"T_c1774_row4_col10\" class=\"data row4 col10\" >0.8726</td>\n",
       "      <td id=\"T_c1774_row4_col11\" class=\"data row4 col11\" >0.9485</td>\n",
       "      <td id=\"T_c1774_row4_col12\" class=\"data row4 col12\" >0.9214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c1774_row5_col0\" class=\"data row5 col0\" >IndianaJones</td>\n",
       "      <td id=\"T_c1774_row5_col1\" class=\"data row5 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row5_col2\" class=\"data row5 col2\" >0.8805</td>\n",
       "      <td id=\"T_c1774_row5_col3\" class=\"data row5 col3\" >0.3385</td>\n",
       "      <td id=\"T_c1774_row5_col4\" class=\"data row5 col4\" >0.4890</td>\n",
       "      <td id=\"T_c1774_row5_col5\" class=\"data row5 col5\" >0.8955</td>\n",
       "      <td id=\"T_c1774_row5_col6\" class=\"data row5 col6\" >0.4489</td>\n",
       "      <td id=\"T_c1774_row5_col7\" class=\"data row5 col7\" >0.5981</td>\n",
       "      <td id=\"T_c1774_row5_col8\" class=\"data row5 col8\" >0.9616</td>\n",
       "      <td id=\"T_c1774_row5_col9\" class=\"data row5 col9\" >0.8162</td>\n",
       "      <td id=\"T_c1774_row5_col10\" class=\"data row5 col10\" >0.8830</td>\n",
       "      <td id=\"T_c1774_row5_col11\" class=\"data row5 col11\" >0.9621</td>\n",
       "      <td id=\"T_c1774_row5_col12\" class=\"data row5 col12\" >0.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c1774_row6_col0\" class=\"data row6 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_c1774_row6_col1\" class=\"data row6 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row6_col2\" class=\"data row6 col2\" >0.8917</td>\n",
       "      <td id=\"T_c1774_row6_col3\" class=\"data row6 col3\" >0.3193</td>\n",
       "      <td id=\"T_c1774_row6_col4\" class=\"data row6 col4\" >0.4702</td>\n",
       "      <td id=\"T_c1774_row6_col5\" class=\"data row6 col5\" >0.9063</td>\n",
       "      <td id=\"T_c1774_row6_col6\" class=\"data row6 col6\" >0.4422</td>\n",
       "      <td id=\"T_c1774_row6_col7\" class=\"data row6 col7\" >0.5943</td>\n",
       "      <td id=\"T_c1774_row6_col8\" class=\"data row6 col8\" >0.8537</td>\n",
       "      <td id=\"T_c1774_row6_col9\" class=\"data row6 col9\" >0.7759</td>\n",
       "      <td id=\"T_c1774_row6_col10\" class=\"data row6 col10\" >0.8129</td>\n",
       "      <td id=\"T_c1774_row6_col11\" class=\"data row6 col11\" >0.9526</td>\n",
       "      <td id=\"T_c1774_row6_col12\" class=\"data row6 col12\" >0.9526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c1774_row7_col0\" class=\"data row7 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_c1774_row7_col1\" class=\"data row7 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row7_col2\" class=\"data row7 col2\" >0.8928</td>\n",
       "      <td id=\"T_c1774_row7_col3\" class=\"data row7 col3\" >0.3126</td>\n",
       "      <td id=\"T_c1774_row7_col4\" class=\"data row7 col4\" >0.4631</td>\n",
       "      <td id=\"T_c1774_row7_col5\" class=\"data row7 col5\" >0.8962</td>\n",
       "      <td id=\"T_c1774_row7_col6\" class=\"data row7 col6\" >0.4212</td>\n",
       "      <td id=\"T_c1774_row7_col7\" class=\"data row7 col7\" >0.5731</td>\n",
       "      <td id=\"T_c1774_row7_col8\" class=\"data row7 col8\" >0.7331</td>\n",
       "      <td id=\"T_c1774_row7_col9\" class=\"data row7 col9\" >0.7897</td>\n",
       "      <td id=\"T_c1774_row7_col10\" class=\"data row7 col10\" >0.7604</td>\n",
       "      <td id=\"T_c1774_row7_col11\" class=\"data row7 col11\" >0.9459</td>\n",
       "      <td id=\"T_c1774_row7_col12\" class=\"data row7 col12\" >0.9463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c1774_row8_col0\" class=\"data row8 col0\" >Oppenheimer</td>\n",
       "      <td id=\"T_c1774_row8_col1\" class=\"data row8 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row8_col2\" class=\"data row8 col2\" >0.9384</td>\n",
       "      <td id=\"T_c1774_row8_col3\" class=\"data row8 col3\" >0.3207</td>\n",
       "      <td id=\"T_c1774_row8_col4\" class=\"data row8 col4\" >0.4781</td>\n",
       "      <td id=\"T_c1774_row8_col5\" class=\"data row8 col5\" >0.9470</td>\n",
       "      <td id=\"T_c1774_row8_col6\" class=\"data row8 col6\" >0.4611</td>\n",
       "      <td id=\"T_c1774_row8_col7\" class=\"data row8 col7\" >0.6202</td>\n",
       "      <td id=\"T_c1774_row8_col8\" class=\"data row8 col8\" >0.9905</td>\n",
       "      <td id=\"T_c1774_row8_col9\" class=\"data row8 col9\" >0.8556</td>\n",
       "      <td id=\"T_c1774_row8_col10\" class=\"data row8 col10\" >0.9181</td>\n",
       "      <td id=\"T_c1774_row8_col11\" class=\"data row8 col11\" >0.9492</td>\n",
       "      <td id=\"T_c1774_row8_col12\" class=\"data row8 col12\" >0.9492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c1774_row9_col0\" class=\"data row9 col0\" >Oppenheimer</td>\n",
       "      <td id=\"T_c1774_row9_col1\" class=\"data row9 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row9_col2\" class=\"data row9 col2\" >0.9247</td>\n",
       "      <td id=\"T_c1774_row9_col3\" class=\"data row9 col3\" >0.3090</td>\n",
       "      <td id=\"T_c1774_row9_col4\" class=\"data row9 col4\" >0.4632</td>\n",
       "      <td id=\"T_c1774_row9_col5\" class=\"data row9 col5\" >0.9219</td>\n",
       "      <td id=\"T_c1774_row9_col6\" class=\"data row9 col6\" >0.4328</td>\n",
       "      <td id=\"T_c1774_row9_col7\" class=\"data row9 col7\" >0.5891</td>\n",
       "      <td id=\"T_c1774_row9_col8\" class=\"data row9 col8\" >0.8451</td>\n",
       "      <td id=\"T_c1774_row9_col9\" class=\"data row9 col9\" >0.9428</td>\n",
       "      <td id=\"T_c1774_row9_col10\" class=\"data row9 col10\" >0.8913</td>\n",
       "      <td id=\"T_c1774_row9_col11\" class=\"data row9 col11\" >0.9458</td>\n",
       "      <td id=\"T_c1774_row9_col12\" class=\"data row9 col12\" >0.9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c1774_row10_col0\" class=\"data row10 col0\" >Parasite</td>\n",
       "      <td id=\"T_c1774_row10_col1\" class=\"data row10 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row10_col2\" class=\"data row10 col2\" >0.9003</td>\n",
       "      <td id=\"T_c1774_row10_col3\" class=\"data row10 col3\" >0.3598</td>\n",
       "      <td id=\"T_c1774_row10_col4\" class=\"data row10 col4\" >0.5142</td>\n",
       "      <td id=\"T_c1774_row10_col5\" class=\"data row10 col5\" >0.9071</td>\n",
       "      <td id=\"T_c1774_row10_col6\" class=\"data row10 col6\" >0.4748</td>\n",
       "      <td id=\"T_c1774_row10_col7\" class=\"data row10 col7\" >0.6234</td>\n",
       "      <td id=\"T_c1774_row10_col8\" class=\"data row10 col8\" >0.9527</td>\n",
       "      <td id=\"T_c1774_row10_col9\" class=\"data row10 col9\" >0.9086</td>\n",
       "      <td id=\"T_c1774_row10_col10\" class=\"data row10 col10\" >0.9301</td>\n",
       "      <td id=\"T_c1774_row10_col11\" class=\"data row10 col11\" >0.9437</td>\n",
       "      <td id=\"T_c1774_row10_col12\" class=\"data row10 col12\" >0.9438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_c1774_row11_col0\" class=\"data row11 col0\" >Parasite</td>\n",
       "      <td id=\"T_c1774_row11_col1\" class=\"data row11 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row11_col2\" class=\"data row11 col2\" >0.8855</td>\n",
       "      <td id=\"T_c1774_row11_col3\" class=\"data row11 col3\" >0.3423</td>\n",
       "      <td id=\"T_c1774_row11_col4\" class=\"data row11 col4\" >0.4938</td>\n",
       "      <td id=\"T_c1774_row11_col5\" class=\"data row11 col5\" >0.8869</td>\n",
       "      <td id=\"T_c1774_row11_col6\" class=\"data row11 col6\" >0.4505</td>\n",
       "      <td id=\"T_c1774_row11_col7\" class=\"data row11 col7\" >0.5975</td>\n",
       "      <td id=\"T_c1774_row11_col8\" class=\"data row11 col8\" >0.9499</td>\n",
       "      <td id=\"T_c1774_row11_col9\" class=\"data row11 col9\" >0.9114</td>\n",
       "      <td id=\"T_c1774_row11_col10\" class=\"data row11 col10\" >0.9302</td>\n",
       "      <td id=\"T_c1774_row11_col11\" class=\"data row11 col11\" >0.9420</td>\n",
       "      <td id=\"T_c1774_row11_col12\" class=\"data row11 col12\" >0.9405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_c1774_row12_col0\" class=\"data row12 col0\" >SW_Episode1</td>\n",
       "      <td id=\"T_c1774_row12_col1\" class=\"data row12 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row12_col2\" class=\"data row12 col2\" >0.9005</td>\n",
       "      <td id=\"T_c1774_row12_col3\" class=\"data row12 col3\" >0.3608</td>\n",
       "      <td id=\"T_c1774_row12_col4\" class=\"data row12 col4\" >0.5152</td>\n",
       "      <td id=\"T_c1774_row12_col5\" class=\"data row12 col5\" >0.9050</td>\n",
       "      <td id=\"T_c1774_row12_col6\" class=\"data row12 col6\" >0.4816</td>\n",
       "      <td id=\"T_c1774_row12_col7\" class=\"data row12 col7\" >0.6287</td>\n",
       "      <td id=\"T_c1774_row12_col8\" class=\"data row12 col8\" >0.8968</td>\n",
       "      <td id=\"T_c1774_row12_col9\" class=\"data row12 col9\" >0.8293</td>\n",
       "      <td id=\"T_c1774_row12_col10\" class=\"data row12 col10\" >0.8617</td>\n",
       "      <td id=\"T_c1774_row12_col11\" class=\"data row12 col11\" >0.8768</td>\n",
       "      <td id=\"T_c1774_row12_col12\" class=\"data row12 col12\" >0.7767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_c1774_row13_col0\" class=\"data row13 col0\" >SW_Episode1</td>\n",
       "      <td id=\"T_c1774_row13_col1\" class=\"data row13 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row13_col2\" class=\"data row13 col2\" >0.8597</td>\n",
       "      <td id=\"T_c1774_row13_col3\" class=\"data row13 col3\" >0.3354</td>\n",
       "      <td id=\"T_c1774_row13_col4\" class=\"data row13 col4\" >0.4826</td>\n",
       "      <td id=\"T_c1774_row13_col5\" class=\"data row13 col5\" >0.8689</td>\n",
       "      <td id=\"T_c1774_row13_col6\" class=\"data row13 col6\" >0.4448</td>\n",
       "      <td id=\"T_c1774_row13_col7\" class=\"data row13 col7\" >0.5884</td>\n",
       "      <td id=\"T_c1774_row13_col8\" class=\"data row13 col8\" >0.9318</td>\n",
       "      <td id=\"T_c1774_row13_col9\" class=\"data row13 col9\" >0.8634</td>\n",
       "      <td id=\"T_c1774_row13_col10\" class=\"data row13 col10\" >0.8963</td>\n",
       "      <td id=\"T_c1774_row13_col11\" class=\"data row13 col11\" >0.9028</td>\n",
       "      <td id=\"T_c1774_row13_col12\" class=\"data row13 col12\" >0.7985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_c1774_row14_col0\" class=\"data row14 col0\" >SW_Episode2</td>\n",
       "      <td id=\"T_c1774_row14_col1\" class=\"data row14 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row14_col2\" class=\"data row14 col2\" >0.9166</td>\n",
       "      <td id=\"T_c1774_row14_col3\" class=\"data row14 col3\" >0.3450</td>\n",
       "      <td id=\"T_c1774_row14_col4\" class=\"data row14 col4\" >0.5013</td>\n",
       "      <td id=\"T_c1774_row14_col5\" class=\"data row14 col5\" >0.9295</td>\n",
       "      <td id=\"T_c1774_row14_col6\" class=\"data row14 col6\" >0.4709</td>\n",
       "      <td id=\"T_c1774_row14_col7\" class=\"data row14 col7\" >0.6251</td>\n",
       "      <td id=\"T_c1774_row14_col8\" class=\"data row14 col8\" >0.8875</td>\n",
       "      <td id=\"T_c1774_row14_col9\" class=\"data row14 col9\" >0.8342</td>\n",
       "      <td id=\"T_c1774_row14_col10\" class=\"data row14 col10\" >0.8600</td>\n",
       "      <td id=\"T_c1774_row14_col11\" class=\"data row14 col11\" >0.8873</td>\n",
       "      <td id=\"T_c1774_row14_col12\" class=\"data row14 col12\" >0.7535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_c1774_row15_col0\" class=\"data row15 col0\" >SW_Episode2</td>\n",
       "      <td id=\"T_c1774_row15_col1\" class=\"data row15 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row15_col2\" class=\"data row15 col2\" >0.8970</td>\n",
       "      <td id=\"T_c1774_row15_col3\" class=\"data row15 col3\" >0.3277</td>\n",
       "      <td id=\"T_c1774_row15_col4\" class=\"data row15 col4\" >0.4800</td>\n",
       "      <td id=\"T_c1774_row15_col5\" class=\"data row15 col5\" >0.9072</td>\n",
       "      <td id=\"T_c1774_row15_col6\" class=\"data row15 col6\" >0.4437</td>\n",
       "      <td id=\"T_c1774_row15_col7\" class=\"data row15 col7\" >0.5960</td>\n",
       "      <td id=\"T_c1774_row15_col8\" class=\"data row15 col8\" >0.8973</td>\n",
       "      <td id=\"T_c1774_row15_col9\" class=\"data row15 col9\" >0.8420</td>\n",
       "      <td id=\"T_c1774_row15_col10\" class=\"data row15 col10\" >0.8687</td>\n",
       "      <td id=\"T_c1774_row15_col11\" class=\"data row15 col11\" >0.9022</td>\n",
       "      <td id=\"T_c1774_row15_col12\" class=\"data row15 col12\" >0.7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_c1774_row16_col0\" class=\"data row16 col0\" >SW_Episode3</td>\n",
       "      <td id=\"T_c1774_row16_col1\" class=\"data row16 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row16_col2\" class=\"data row16 col2\" >0.9185</td>\n",
       "      <td id=\"T_c1774_row16_col3\" class=\"data row16 col3\" >0.3386</td>\n",
       "      <td id=\"T_c1774_row16_col4\" class=\"data row16 col4\" >0.4948</td>\n",
       "      <td id=\"T_c1774_row16_col5\" class=\"data row16 col5\" >0.9327</td>\n",
       "      <td id=\"T_c1774_row16_col6\" class=\"data row16 col6\" >0.4634</td>\n",
       "      <td id=\"T_c1774_row16_col7\" class=\"data row16 col7\" >0.6192</td>\n",
       "      <td id=\"T_c1774_row16_col8\" class=\"data row16 col8\" >0.9489</td>\n",
       "      <td id=\"T_c1774_row16_col9\" class=\"data row16 col9\" >0.8547</td>\n",
       "      <td id=\"T_c1774_row16_col10\" class=\"data row16 col10\" >0.8993</td>\n",
       "      <td id=\"T_c1774_row16_col11\" class=\"data row16 col11\" >0.8954</td>\n",
       "      <td id=\"T_c1774_row16_col12\" class=\"data row16 col12\" >0.7983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_c1774_row17_col0\" class=\"data row17 col0\" >SW_Episode3</td>\n",
       "      <td id=\"T_c1774_row17_col1\" class=\"data row17 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row17_col2\" class=\"data row17 col2\" >0.8932</td>\n",
       "      <td id=\"T_c1774_row17_col3\" class=\"data row17 col3\" >0.3219</td>\n",
       "      <td id=\"T_c1774_row17_col4\" class=\"data row17 col4\" >0.4732</td>\n",
       "      <td id=\"T_c1774_row17_col5\" class=\"data row17 col5\" >0.9077</td>\n",
       "      <td id=\"T_c1774_row17_col6\" class=\"data row17 col6\" >0.4287</td>\n",
       "      <td id=\"T_c1774_row17_col7\" class=\"data row17 col7\" >0.5823</td>\n",
       "      <td id=\"T_c1774_row17_col8\" class=\"data row17 col8\" >0.9703</td>\n",
       "      <td id=\"T_c1774_row17_col9\" class=\"data row17 col9\" >0.8566</td>\n",
       "      <td id=\"T_c1774_row17_col10\" class=\"data row17 col10\" >0.9099</td>\n",
       "      <td id=\"T_c1774_row17_col11\" class=\"data row17 col11\" >0.9155</td>\n",
       "      <td id=\"T_c1774_row17_col12\" class=\"data row17 col12\" >0.8166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_c1774_row18_col0\" class=\"data row18 col0\" >SW_Episode4</td>\n",
       "      <td id=\"T_c1774_row18_col1\" class=\"data row18 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row18_col2\" class=\"data row18 col2\" >0.9030</td>\n",
       "      <td id=\"T_c1774_row18_col3\" class=\"data row18 col3\" >0.3709</td>\n",
       "      <td id=\"T_c1774_row18_col4\" class=\"data row18 col4\" >0.5259</td>\n",
       "      <td id=\"T_c1774_row18_col5\" class=\"data row18 col5\" >0.9205</td>\n",
       "      <td id=\"T_c1774_row18_col6\" class=\"data row18 col6\" >0.4966</td>\n",
       "      <td id=\"T_c1774_row18_col7\" class=\"data row18 col7\" >0.6452</td>\n",
       "      <td id=\"T_c1774_row18_col8\" class=\"data row18 col8\" >0.9679</td>\n",
       "      <td id=\"T_c1774_row18_col9\" class=\"data row18 col9\" >0.7234</td>\n",
       "      <td id=\"T_c1774_row18_col10\" class=\"data row18 col10\" >0.8279</td>\n",
       "      <td id=\"T_c1774_row18_col11\" class=\"data row18 col11\" >0.8975</td>\n",
       "      <td id=\"T_c1774_row18_col12\" class=\"data row18 col12\" >0.7077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_c1774_row19_col0\" class=\"data row19 col0\" >SW_Episode4</td>\n",
       "      <td id=\"T_c1774_row19_col1\" class=\"data row19 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row19_col2\" class=\"data row19 col2\" >0.8733</td>\n",
       "      <td id=\"T_c1774_row19_col3\" class=\"data row19 col3\" >0.3463</td>\n",
       "      <td id=\"T_c1774_row19_col4\" class=\"data row19 col4\" >0.4960</td>\n",
       "      <td id=\"T_c1774_row19_col5\" class=\"data row19 col5\" >0.8908</td>\n",
       "      <td id=\"T_c1774_row19_col6\" class=\"data row19 col6\" >0.4551</td>\n",
       "      <td id=\"T_c1774_row19_col7\" class=\"data row19 col7\" >0.6025</td>\n",
       "      <td id=\"T_c1774_row19_col8\" class=\"data row19 col8\" >0.9771</td>\n",
       "      <td id=\"T_c1774_row19_col9\" class=\"data row19 col9\" >0.7360</td>\n",
       "      <td id=\"T_c1774_row19_col10\" class=\"data row19 col10\" >0.8396</td>\n",
       "      <td id=\"T_c1774_row19_col11\" class=\"data row19 col11\" >0.9216</td>\n",
       "      <td id=\"T_c1774_row19_col12\" class=\"data row19 col12\" >0.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_c1774_row20_col0\" class=\"data row20 col0\" >SW_Episode5</td>\n",
       "      <td id=\"T_c1774_row20_col1\" class=\"data row20 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row20_col2\" class=\"data row20 col2\" >0.9072</td>\n",
       "      <td id=\"T_c1774_row20_col3\" class=\"data row20 col3\" >0.3721</td>\n",
       "      <td id=\"T_c1774_row20_col4\" class=\"data row20 col4\" >0.5277</td>\n",
       "      <td id=\"T_c1774_row20_col5\" class=\"data row20 col5\" >0.9203</td>\n",
       "      <td id=\"T_c1774_row20_col6\" class=\"data row20 col6\" >0.4873</td>\n",
       "      <td id=\"T_c1774_row20_col7\" class=\"data row20 col7\" >0.6372</td>\n",
       "      <td id=\"T_c1774_row20_col8\" class=\"data row20 col8\" >0.9032</td>\n",
       "      <td id=\"T_c1774_row20_col9\" class=\"data row20 col9\" >0.8145</td>\n",
       "      <td id=\"T_c1774_row20_col10\" class=\"data row20 col10\" >0.8566</td>\n",
       "      <td id=\"T_c1774_row20_col11\" class=\"data row20 col11\" >0.8880</td>\n",
       "      <td id=\"T_c1774_row20_col12\" class=\"data row20 col12\" >0.8338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_c1774_row21_col0\" class=\"data row21 col0\" >SW_Episode5</td>\n",
       "      <td id=\"T_c1774_row21_col1\" class=\"data row21 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row21_col2\" class=\"data row21 col2\" >0.8876</td>\n",
       "      <td id=\"T_c1774_row21_col3\" class=\"data row21 col3\" >0.3550</td>\n",
       "      <td id=\"T_c1774_row21_col4\" class=\"data row21 col4\" >0.5071</td>\n",
       "      <td id=\"T_c1774_row21_col5\" class=\"data row21 col5\" >0.9040</td>\n",
       "      <td id=\"T_c1774_row21_col6\" class=\"data row21 col6\" >0.4636</td>\n",
       "      <td id=\"T_c1774_row21_col7\" class=\"data row21 col7\" >0.6129</td>\n",
       "      <td id=\"T_c1774_row21_col8\" class=\"data row21 col8\" >0.8398</td>\n",
       "      <td id=\"T_c1774_row21_col9\" class=\"data row21 col9\" >0.8667</td>\n",
       "      <td id=\"T_c1774_row21_col10\" class=\"data row21 col10\" >0.8530</td>\n",
       "      <td id=\"T_c1774_row21_col11\" class=\"data row21 col11\" >0.9270</td>\n",
       "      <td id=\"T_c1774_row21_col12\" class=\"data row21 col12\" >0.8701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_c1774_row22_col0\" class=\"data row22 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_c1774_row22_col1\" class=\"data row22 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row22_col2\" class=\"data row22 col2\" >0.9199</td>\n",
       "      <td id=\"T_c1774_row22_col3\" class=\"data row22 col3\" >0.3577</td>\n",
       "      <td id=\"T_c1774_row22_col4\" class=\"data row22 col4\" >0.5151</td>\n",
       "      <td id=\"T_c1774_row22_col5\" class=\"data row22 col5\" >0.9155</td>\n",
       "      <td id=\"T_c1774_row22_col6\" class=\"data row22 col6\" >0.4677</td>\n",
       "      <td id=\"T_c1774_row22_col7\" class=\"data row22 col7\" >0.6191</td>\n",
       "      <td id=\"T_c1774_row22_col8\" class=\"data row22 col8\" >0.9296</td>\n",
       "      <td id=\"T_c1774_row22_col9\" class=\"data row22 col9\" >0.7500</td>\n",
       "      <td id=\"T_c1774_row22_col10\" class=\"data row22 col10\" >0.8302</td>\n",
       "      <td id=\"T_c1774_row22_col11\" class=\"data row22 col11\" >0.9112</td>\n",
       "      <td id=\"T_c1774_row22_col12\" class=\"data row22 col12\" >0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_c1774_row23_col0\" class=\"data row23 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_c1774_row23_col1\" class=\"data row23 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row23_col2\" class=\"data row23 col2\" >0.8660</td>\n",
       "      <td id=\"T_c1774_row23_col3\" class=\"data row23 col3\" >0.3289</td>\n",
       "      <td id=\"T_c1774_row23_col4\" class=\"data row23 col4\" >0.4767</td>\n",
       "      <td id=\"T_c1774_row23_col5\" class=\"data row23 col5\" >0.8914</td>\n",
       "      <td id=\"T_c1774_row23_col6\" class=\"data row23 col6\" >0.4416</td>\n",
       "      <td id=\"T_c1774_row23_col7\" class=\"data row23 col7\" >0.5906</td>\n",
       "      <td id=\"T_c1774_row23_col8\" class=\"data row23 col8\" >0.8823</td>\n",
       "      <td id=\"T_c1774_row23_col9\" class=\"data row23 col9\" >0.7831</td>\n",
       "      <td id=\"T_c1774_row23_col10\" class=\"data row23 col10\" >0.8297</td>\n",
       "      <td id=\"T_c1774_row23_col11\" class=\"data row23 col11\" >0.9240</td>\n",
       "      <td id=\"T_c1774_row23_col12\" class=\"data row23 col12\" >0.9239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_c1774_row24_col0\" class=\"data row24 col0\" >SW_Episode7</td>\n",
       "      <td id=\"T_c1774_row24_col1\" class=\"data row24 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row24_col2\" class=\"data row24 col2\" >0.8964</td>\n",
       "      <td id=\"T_c1774_row24_col3\" class=\"data row24 col3\" >0.3609</td>\n",
       "      <td id=\"T_c1774_row24_col4\" class=\"data row24 col4\" >0.5147</td>\n",
       "      <td id=\"T_c1774_row24_col5\" class=\"data row24 col5\" >0.9146</td>\n",
       "      <td id=\"T_c1774_row24_col6\" class=\"data row24 col6\" >0.4898</td>\n",
       "      <td id=\"T_c1774_row24_col7\" class=\"data row24 col7\" >0.6379</td>\n",
       "      <td id=\"T_c1774_row24_col8\" class=\"data row24 col8\" >0.9915</td>\n",
       "      <td id=\"T_c1774_row24_col9\" class=\"data row24 col9\" >0.7651</td>\n",
       "      <td id=\"T_c1774_row24_col10\" class=\"data row24 col10\" >0.8637</td>\n",
       "      <td id=\"T_c1774_row24_col11\" class=\"data row24 col11\" >0.8968</td>\n",
       "      <td id=\"T_c1774_row24_col12\" class=\"data row24 col12\" >0.8458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_c1774_row25_col0\" class=\"data row25 col0\" >SW_Episode7</td>\n",
       "      <td id=\"T_c1774_row25_col1\" class=\"data row25 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row25_col2\" class=\"data row25 col2\" >0.8713</td>\n",
       "      <td id=\"T_c1774_row25_col3\" class=\"data row25 col3\" >0.3445</td>\n",
       "      <td id=\"T_c1774_row25_col4\" class=\"data row25 col4\" >0.4937</td>\n",
       "      <td id=\"T_c1774_row25_col5\" class=\"data row25 col5\" >0.8868</td>\n",
       "      <td id=\"T_c1774_row25_col6\" class=\"data row25 col6\" >0.4532</td>\n",
       "      <td id=\"T_c1774_row25_col7\" class=\"data row25 col7\" >0.5999</td>\n",
       "      <td id=\"T_c1774_row25_col8\" class=\"data row25 col8\" >0.9239</td>\n",
       "      <td id=\"T_c1774_row25_col9\" class=\"data row25 col9\" >0.9253</td>\n",
       "      <td id=\"T_c1774_row25_col10\" class=\"data row25 col10\" >0.9246</td>\n",
       "      <td id=\"T_c1774_row25_col11\" class=\"data row25 col11\" >0.8877</td>\n",
       "      <td id=\"T_c1774_row25_col12\" class=\"data row25 col12\" >0.8379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_c1774_row26_col0\" class=\"data row26 col0\" >SW_Episode8</td>\n",
       "      <td id=\"T_c1774_row26_col1\" class=\"data row26 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row26_col2\" class=\"data row26 col2\" >0.8951</td>\n",
       "      <td id=\"T_c1774_row26_col3\" class=\"data row26 col3\" >0.3662</td>\n",
       "      <td id=\"T_c1774_row26_col4\" class=\"data row26 col4\" >0.5198</td>\n",
       "      <td id=\"T_c1774_row26_col5\" class=\"data row26 col5\" >0.9087</td>\n",
       "      <td id=\"T_c1774_row26_col6\" class=\"data row26 col6\" >0.4863</td>\n",
       "      <td id=\"T_c1774_row26_col7\" class=\"data row26 col7\" >0.6336</td>\n",
       "      <td id=\"T_c1774_row26_col8\" class=\"data row26 col8\" >0.9767</td>\n",
       "      <td id=\"T_c1774_row26_col9\" class=\"data row26 col9\" >0.8659</td>\n",
       "      <td id=\"T_c1774_row26_col10\" class=\"data row26 col10\" >0.9179</td>\n",
       "      <td id=\"T_c1774_row26_col11\" class=\"data row26 col11\" >0.8916</td>\n",
       "      <td id=\"T_c1774_row26_col12\" class=\"data row26 col12\" >0.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_c1774_row27_col0\" class=\"data row27 col0\" >SW_Episode8</td>\n",
       "      <td id=\"T_c1774_row27_col1\" class=\"data row27 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row27_col2\" class=\"data row27 col2\" >0.8714</td>\n",
       "      <td id=\"T_c1774_row27_col3\" class=\"data row27 col3\" >0.3483</td>\n",
       "      <td id=\"T_c1774_row27_col4\" class=\"data row27 col4\" >0.4977</td>\n",
       "      <td id=\"T_c1774_row27_col5\" class=\"data row27 col5\" >0.8904</td>\n",
       "      <td id=\"T_c1774_row27_col6\" class=\"data row27 col6\" >0.4604</td>\n",
       "      <td id=\"T_c1774_row27_col7\" class=\"data row27 col7\" >0.6069</td>\n",
       "      <td id=\"T_c1774_row27_col8\" class=\"data row27 col8\" >0.9711</td>\n",
       "      <td id=\"T_c1774_row27_col9\" class=\"data row27 col9\" >0.9015</td>\n",
       "      <td id=\"T_c1774_row27_col10\" class=\"data row27 col10\" >0.9350</td>\n",
       "      <td id=\"T_c1774_row27_col11\" class=\"data row27 col11\" >0.8611</td>\n",
       "      <td id=\"T_c1774_row27_col12\" class=\"data row27 col12\" >0.7374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_c1774_row28_col0\" class=\"data row28 col0\" >SW_Episode9</td>\n",
       "      <td id=\"T_c1774_row28_col1\" class=\"data row28 col1\" >base</td>\n",
       "      <td id=\"T_c1774_row28_col2\" class=\"data row28 col2\" >0.9167</td>\n",
       "      <td id=\"T_c1774_row28_col3\" class=\"data row28 col3\" >0.3629</td>\n",
       "      <td id=\"T_c1774_row28_col4\" class=\"data row28 col4\" >0.5200</td>\n",
       "      <td id=\"T_c1774_row28_col5\" class=\"data row28 col5\" >0.9192</td>\n",
       "      <td id=\"T_c1774_row28_col6\" class=\"data row28 col6\" >0.4781</td>\n",
       "      <td id=\"T_c1774_row28_col7\" class=\"data row28 col7\" >0.6291</td>\n",
       "      <td id=\"T_c1774_row28_col8\" class=\"data row28 col8\" >0.9791</td>\n",
       "      <td id=\"T_c1774_row28_col9\" class=\"data row28 col9\" >0.8786</td>\n",
       "      <td id=\"T_c1774_row28_col10\" class=\"data row28 col10\" >0.9262</td>\n",
       "      <td id=\"T_c1774_row28_col11\" class=\"data row28 col11\" >0.9038</td>\n",
       "      <td id=\"T_c1774_row28_col12\" class=\"data row28 col12\" >0.8088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1774_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_c1774_row29_col0\" class=\"data row29 col0\" >SW_Episode9</td>\n",
       "      <td id=\"T_c1774_row29_col1\" class=\"data row29 col1\" >sentiment</td>\n",
       "      <td id=\"T_c1774_row29_col2\" class=\"data row29 col2\" >0.8897</td>\n",
       "      <td id=\"T_c1774_row29_col3\" class=\"data row29 col3\" >0.3453</td>\n",
       "      <td id=\"T_c1774_row29_col4\" class=\"data row29 col4\" >0.4975</td>\n",
       "      <td id=\"T_c1774_row29_col5\" class=\"data row29 col5\" >0.9005</td>\n",
       "      <td id=\"T_c1774_row29_col6\" class=\"data row29 col6\" >0.4524</td>\n",
       "      <td id=\"T_c1774_row29_col7\" class=\"data row29 col7\" >0.6023</td>\n",
       "      <td id=\"T_c1774_row29_col8\" class=\"data row29 col8\" >0.9812</td>\n",
       "      <td id=\"T_c1774_row29_col9\" class=\"data row29 col9\" >0.9017</td>\n",
       "      <td id=\"T_c1774_row29_col10\" class=\"data row29 col10\" >0.9398</td>\n",
       "      <td id=\"T_c1774_row29_col11\" class=\"data row29 col11\" >0.9004</td>\n",
       "      <td id=\"T_c1774_row29_col12\" class=\"data row29 col12\" >0.8052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x32d9ef6a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load ground truth keywords\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Container to store results for all movies and models\n",
    "global_results = []\n",
    "\n",
    "# Iterate over all predicted keyword files (one file per movie)\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords DataFrame and movie ID\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Get ground truth keywords for this movie\n",
    "            kw_ground_truth = keywords_ground_truth[\n",
    "                keywords_ground_truth[\"Movie_ID\"] == selected_film_id\n",
    "            ]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].dropna().tolist()\n",
    "\n",
    "            # Compute average sentiment of ground truth keywords\n",
    "            gt_sentiments = []\n",
    "            for kw in gt_keywords:\n",
    "                scores = analyzer.polarity_scores(kw)\n",
    "                sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"]\n",
    "                gt_sentiments.append(sentiment)\n",
    "            sentiment_gt = sum(gt_sentiments) / len(gt_sentiments) if gt_sentiments else 0.5\n",
    "\n",
    "            # Compute average sentiment from all review texts (used only if needed)\n",
    "            review_texts = selected_film[\"Preprocessed_Review\"].dropna().tolist()\n",
    "            review_sentiments = []\n",
    "            for text in review_texts:\n",
    "                scores = analyzer.polarity_scores(text)\n",
    "                sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"]\n",
    "                review_sentiments.append(sentiment)\n",
    "            sentiment_text = sum(review_sentiments) / len(review_sentiments) if review_sentiments else 0.5\n",
    "\n",
    "            # Evaluate each model globally\n",
    "            for model in models_to_evaluate:\n",
    "                pred_col = f\"keywords_{model}\"\n",
    "\n",
    "                # Lists of lists for evaluation functions:\n",
    "                # - pred_kw_per_review: list of lists of keywords (strings) for evaluate_keywords()\n",
    "                # - pred_kwscore_per_review: list of lists of (keyword, score) tuples for evaluate_keywords_weighted()\n",
    "                pred_kw_per_review = []\n",
    "                pred_kwscore_per_review = []\n",
    "                all_sentiments_per_review = []\n",
    "\n",
    "                # Iterate over reviews to build these lists\n",
    "                for _, row in selected_film.iterrows():\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "\n",
    "                        # Extract keywords only for evaluate_keywords()\n",
    "                        pred_kw_only = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "\n",
    "                        # Extract (keyword, score) tuples for evaluate_keywords_weighted()\n",
    "                        pred_kw_score = [\n",
    "                            (kw, score) for kw, score in row[pred_col]\n",
    "                            if isinstance(kw, str) and isinstance(score, (float, int))\n",
    "                        ]\n",
    "\n",
    "                        # Append per review keyword lists if not empty\n",
    "                        if pred_kw_only:\n",
    "                            pred_kw_per_review.append(pred_kw_only)\n",
    "                        if pred_kw_score:\n",
    "                            pred_kwscore_per_review.append(pred_kw_score)\n",
    "\n",
    "                        # Compute sentiment of predicted keywords for sentiment alignment\n",
    "                        review_sentiments = []\n",
    "                        for kw, _ in pred_kw_score:\n",
    "                            scores = analyzer.polarity_scores(kw)\n",
    "                            sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"]\n",
    "                            review_sentiments.append(sentiment)\n",
    "                        if review_sentiments:\n",
    "                            all_sentiments_per_review.append(review_sentiments)\n",
    "\n",
    "                # Compute classic precision, recall, F1\n",
    "                precision, recall, f1 = evaluate_keywords(pred_kw_per_review, gt_keywords)\n",
    "\n",
    "                # Compute weighted precision, recall, F1\n",
    "                w_precision, w_recall, w_f1 = evaluate_keywords_weighted(pred_kwscore_per_review, gt_keywords)\n",
    "\n",
    "                # Compute semantic precision, recall, F1\n",
    "                flat_kw_list = [kw for review in pred_kw_per_review for kw in review]\n",
    "                flat_kw_list = list(set(flat_kw_list))\n",
    "                semantic_precision, semantic_recall, semantic_f1 = evaluate_semantic_keywords(\n",
    "                    flat_kw_list, gt_keywords, device=device, threshold=0.65\n",
    "                )\n",
    "\n",
    "                # Compute Avg Sentiment Similarity\n",
    "                ass = compute_average_sentiment_similarity(all_sentiments_per_review, sentiment_gt)\n",
    "\n",
    "                # Compute Review-wise Sampled Sentiment Similarity (Top-5 sampling)\n",
    "                rsss = compute_reviewwise_sentiment_similarity(\n",
    "                    selected_film, gt_keywords, pred_col, analyzer, k_samples=20, seed=42\n",
    "                )\n",
    "\n",
    "                # Store results for this movie-model pair\n",
    "                global_results.append({\n",
    "                    \"Movie\": movie_name,\n",
    "                    \"Model\": model,\n",
    "                    \"Precision\": round(precision, 4),\n",
    "                    \"Recall\": round(recall, 4),\n",
    "                    \"F1-score\": round(f1, 4),\n",
    "                    \"Weighted Precision\": round(w_precision, 4),\n",
    "                    \"Weighted Recall\": round(w_recall, 4),\n",
    "                    \"Weighted F1-score\": round(w_f1, 4),\n",
    "                    \"Semantic Precision\": round(semantic_precision, 4),\n",
    "                    \"Semantic Recall\": round(semantic_recall, 4),\n",
    "                    \"Semantic F1-score\": round(semantic_f1, 4),\n",
    "                    \"Avg Sentiment Similarity\": round(ass, 4) if ass is not None else None,\n",
    "                    \"Review-wise Sampled Similarity\": round(rsss, 4) if rsss is not None else None\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create final DataFrame and sort results\n",
    "final_df = pd.DataFrame(global_results)\n",
    "final_df = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df.style.format(precision=4).set_caption(\"Global Evaluation Summary per Movie and Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15262642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df \n",
    "final_df.to_csv(\"base_vs_sentiment_evaluation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
