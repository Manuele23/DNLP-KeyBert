{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTSentimentAware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804715a2",
   "metadata": {},
   "source": [
    "This notebook evaluates and compares different keyword extraction models applied to movie reviews, with a specific focus on assessing how well each model captures not only **semantic relevance** but also **sentiment alignment** with the content of the reviews.\n",
    "\n",
    "We assess the performance of two models:\n",
    "\n",
    "- **Base** KeyBERT model  \n",
    "- **Sentiment-aware** extension (KeyBERTSentimentAware), which integrates a custom sentiment classifier to adjust keyword relevance scores based on predicted sentiment\n",
    "\n",
    "This notebook adopts a **global evaluation approach**: all predicted and ground truth keywords across the entire dataset are aggregated **before** computing each metric. This provides a **holistic view** of each model's performance, unaffected by review-level variance.\n",
    "\n",
    "We use a set of annotated ground truth keywords per movie (from IMDb), and the top-5 predicted keywords (with scores) per review for both models.\n",
    "\n",
    "#### **Evaluation Layers**\n",
    "\n",
    "#### 1. **Basic (Unweighted) Metrics**\n",
    "\n",
    "- **Precision**, **Recall**, and **F1-score** computed globally via approximate binary matching.\n",
    "- A predicted keyword is correct if it approximately matches any of the movieâ€™s ground truth keywords.\n",
    "- All keywords from all reviews are flattened and compared in aggregate.\n",
    "\n",
    "#### 2. **Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision**, **Recall**, and **F1-score**:\n",
    "  - Each predicted keyword is weighted by its score.\n",
    "  - Correct predictions contribute proportionally to their confidence.\n",
    "- **nDCG@5 (Normalized Discounted Cumulative Gain)**:\n",
    "  - Evaluates whether correct keywords are ranked near the top globally.\n",
    "  - Relevance is discounted by position, rewarding better keyword orderings.\n",
    "\n",
    "#### 3. **Semantic Evaluation (Embedding-Based)**\n",
    "\n",
    "- All predicted and ground truth keywords are embedded using a **sentence-transformer** model.\n",
    "- **Cosine similarity** is used to detect approximate **semantic matches**.\n",
    "- A predicted keyword is correct if its similarity with any ground truth keyword exceeds a given threshold (e.g., **0.75**).\n",
    "- **Semantic Precision**, **Recall**, and **F1-score** are computed globally based on these soft matches.\n",
    "\n",
    "#### 4. **Sentiment Appropriateness Score (SAS)**\n",
    "\n",
    "This novel metric evaluates **how well the sentiment of predicted keywords aligns with the sentiment of the review**.\n",
    "\n",
    "Two global variants are computed:\n",
    "\n",
    "- **SAS_from_keywords**:  \n",
    "  - Computes the average sentiment of predicted keywords (via VADER or custom classifier).  \n",
    "  - Compares it to the average sentiment of the ground truth keywords for the movie.\n",
    "\n",
    "- **SAS_from_text**:  \n",
    "  - Compares the average sentiment of the predicted keywords to the sentiment of the **full review text**.\n",
    "\n",
    "SAS values are normalized in [0, 1], where values closer to 1 indicate higher emotional coherence.\n",
    "\n",
    "#### **Why Sentiment-Aware Evaluation Matters**\n",
    "\n",
    "The **base** KeyBERT model selects keywords based only on semantic relevance, while the **sentiment-aware** version ranks keywords based on a combination of **semantic and emotional cues**.\n",
    "\n",
    "Traditional evaluations may overlook whether the extracted keywords convey the **emotional tone** of the review.  \n",
    "By introducing **Sentiment Appropriateness Scores**, we quantify this alignment explicitly and verify whether integrating sentiment enhances both **relevance** and **emotional fidelity**.\n",
    "\n",
    "This global, multi-dimensional evaluation provides a robust framework for comparing keyword extraction systems beyond surface-level matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers is already installed.\n",
      "torch is already installed.\n",
      "pandas is already installed.\n",
      "tqdm is already installed.\n",
      "vaderSentiment is already installed.\n",
      "numpy is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"tqdm\", \"transformers\", \"torch\", \"vaderSentiment\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973bef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops\n",
    "\n",
    "# Transformers and PyTorch for embeddings and models\n",
    "from transformers import AutoTokenizer, AutoModel # type:ignore\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9ee47",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c179",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55796a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602be9",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"SW_Episode6\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic â€“ Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a94a",
   "metadata": {},
   "source": [
    "This block defines the core utility functions used to evaluate predicted keywords against the ground truth. These functions perform a **binary, unweighted evaluation**, ignoring confidence scores and ranking information.\n",
    "\n",
    "The evaluation pipeline includes the following steps:\n",
    "\n",
    "- **Normalization**: all keywords are lowercased, stripped of punctuation, and cleaned of extra whitespace to ensure consistent text matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule considers two keywords as a match if:\n",
    "  - They are exactly equal (after normalization), or\n",
    "  - One is a substring of the other (e.g., *\"social satire\"* is considered a match with *\"satire\"*).\n",
    "\n",
    "- **Global Evaluation**: for each model, all keywords predicted across the reviews of a given movie are aggregated, and then compared to the global set of ground truth keywords for that movie.\n",
    "\n",
    "- **Metrics**: we compute **Precision**, **Recall**, and **F1-score** based on the number of approximate matches between the predicted and ground truth keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_keywords(all_pred_keywords, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global precision, recall, and F1-score across a dataset using approximate matching.\n",
    "\n",
    "    This function compares predicted keywords to ground truth keywords for each review.\n",
    "    Matching is performed using approximate string comparison, and each ground truth keyword\n",
    "    can be matched only once to ensure fairness. The metrics are aggregated globally,\n",
    "    not per-review.\n",
    "\n",
    "    Args:\n",
    "        all_pred_keywords (List[List[str]]): \n",
    "            A list where each element is a list of predicted keywords for a single review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list where each element is a list of ground truth keywords for the corresponding review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Global precision, recall, and F1-score based on approximate matching.\n",
    "    \"\"\"\n",
    "    global_match_count = 0     # Total number of matched keywords across all reviews\n",
    "    global_pred_count = 0      # Total number of predicted keywords\n",
    "    global_gt_count = 0        # Total number of ground truth keywords\n",
    "\n",
    "    # Iterate through each review's predictions and ground truths\n",
    "    for pred_keywords, gt_keywords in zip(all_pred_keywords, all_gt_keywords):\n",
    "        # Normalize and sort keywords to ensure consistent behavior\n",
    "        pred_keywords = sorted([normalize_kw(k) for k in pred_keywords])\n",
    "        gt_keywords = sorted([normalize_kw(k) for k in gt_keywords])\n",
    "\n",
    "        global_pred_count += len(pred_keywords)\n",
    "        global_gt_count += len(gt_keywords)\n",
    "\n",
    "        matched_gts = set()  # Track which ground truth keywords have already been matched\n",
    "\n",
    "        for pred in pred_keywords:\n",
    "            for gt in gt_keywords:\n",
    "                if gt not in matched_gts and is_approx_match(pred, [gt]):\n",
    "                    global_match_count += 1\n",
    "                    matched_gts.add(gt)  # Avoid matching the same GT keyword multiple times\n",
    "                    break\n",
    "\n",
    "    # Compute global metrics\n",
    "    precision = global_match_count / global_pred_count if global_pred_count else 0\n",
    "    recall = global_match_count / global_gt_count if global_gt_count else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic â€“ Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0199",
   "metadata": {},
   "source": [
    "This section evaluates two keyword extraction models â€” **base** and **sentiment-enhanced** â€” against the ground truth annotations.\n",
    "\n",
    "For each model, we collect all predicted keywords across all reviews in the selected movie and compare them to the ground truth keywords using **binary approximate matching**.\n",
    "\n",
    "The evaluation computes **global precision, recall, and F1-score**, considering the entire set of predictions and ground truth keywords as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d333544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_86eff\">\n",
       "  <caption>Global Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_86eff_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_86eff_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_86eff_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_86eff_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_86eff_row0_col0\" class=\"data row0 col0\" >0.9199</td>\n",
       "      <td id=\"T_86eff_row0_col1\" class=\"data row0 col1\" >0.3577</td>\n",
       "      <td id=\"T_86eff_row0_col2\" class=\"data row0 col2\" >0.5151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86eff_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_86eff_row1_col0\" class=\"data row1 col0\" >0.8660</td>\n",
       "      <td id=\"T_86eff_row1_col1\" class=\"data row1 col1\" >0.3289</td>\n",
       "      <td id=\"T_86eff_row1_col2\" class=\"data row1 col2\" >0.4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17992e9d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie\n",
    "ground_truth_keywords = [normalize_kw(kw) for kw in kw_ground_truth[\"Keyword\"].tolist()]\n",
    "\n",
    "# Dictionary to store all predicted keywords per model (across all reviews)\n",
    "all_predictions = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_keywords = [\n",
    "                normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)\n",
    "            ]\n",
    "            \n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_kw = [kw for kw in predicted_keywords if kw not in seen and not seen.add(kw)]\n",
    "\n",
    "            all_predictions[model].append(unique_kw)\n",
    "\n",
    "# Evaluate each model globally\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precision, recall, f1 = evaluate_keywords(\n",
    "        all_predictions[model],  # List of lists\n",
    "        ground_truth_keywords\n",
    "    )\n",
    "\n",
    "    summary[model] = {\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Convert and display\n",
    "summary_df = pd.DataFrame(summary).T\n",
    "summary_df.columns = [\"Precision\", \"Recall\", \"F1-score\"]\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055848",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the modelâ€™s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_keywords_weighted(all_predicted_kw_score, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global weighted precision, recall, and F1-score across multiple reviews.\n",
    "\n",
    "    This function accounts for confidence scores assigned to predicted keywords.\n",
    "    Matching is performed using approximate matching. Each keyword score contributes\n",
    "    to the precision and recall based on whether it matches a ground truth keyword.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Weighted precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    total_score = 0.0         # Sum of all predicted keyword scores\n",
    "    match_score = 0.0         # Sum of scores of correctly predicted keywords\n",
    "    total_gt = 0              # Total number of ground truth keywords across all reviews\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize keywords\n",
    "        gt_kw = [normalize_kw(k) for k in gt_kw]\n",
    "        pred_kw_score = [\n",
    "            (normalize_kw(kw), score) for kw, score in pred_kw_score if isinstance(kw, str)\n",
    "        ]\n",
    "\n",
    "        total_score += sum(score for _, score in pred_kw_score)\n",
    "        total_gt += len(gt_kw)\n",
    "\n",
    "        matched_gts = set()  # Track ground truth keywords already matched\n",
    "\n",
    "        for kw, score in pred_kw_score:\n",
    "            for gt in gt_kw:\n",
    "                if gt not in matched_gts and is_approx_match(kw, [gt]):\n",
    "                    match_score += score\n",
    "                    matched_gts.add(gt)\n",
    "                    break\n",
    "\n",
    "    precision = match_score / total_score if total_score > 0 else 0\n",
    "    recall = match_score / total_gt if total_gt > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80dd7",
   "metadata": {},
   "source": [
    "In this section, we evaluate the overall performance of each model using **score-aware metrics** computed **globally across all reviews**:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**: These metrics incorporate the **confidence scores** assigned to each predicted keyword, reflecting how much of the modelâ€™s confidence is placed on correct predictions.\n",
    "\n",
    "This global evaluation provides a holistic view of each modelâ€™s effectiveness in ranking and selecting relevant keywords across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_94d9f\">\n",
       "  <caption>Global Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_94d9f_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_94d9f_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_94d9f_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_94d9f_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_94d9f_row0_col0\" class=\"data row0 col0\" >0.9155</td>\n",
       "      <td id=\"T_94d9f_row0_col1\" class=\"data row0 col1\" >0.1789</td>\n",
       "      <td id=\"T_94d9f_row0_col2\" class=\"data row0 col2\" >0.2993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_94d9f_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_94d9f_row1_col0\" class=\"data row1 col0\" >0.8914</td>\n",
       "      <td id=\"T_94d9f_row1_col1\" class=\"data row1 col1\" >0.1597</td>\n",
       "      <td id=\"T_94d9f_row1_col2\" class=\"data row1 col2\" >0.2709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x161071fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Prepare data structures to hold predictions for each model\n",
    "all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Collect predictions and GT for each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = [(kw, score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "            \n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "            all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "# Dictionary to store global evaluation results\n",
    "weighted_summary = {}\n",
    "\n",
    "# Evaluate each model globally\n",
    "for model in models_to_evaluate:\n",
    "    preds = all_predicted_kw_score[model]\n",
    "\n",
    "    # Global weighted metrics\n",
    "    w_precision, w_recall, w_f1 = evaluate_keywords_weighted(preds, ground_truth_keywords)\n",
    "\n",
    "    # Store results\n",
    "    weighted_summary[model] = {\n",
    "        \"weighted_precision\": round(w_precision, 4),\n",
    "        \"weighted_recall\": round(w_recall, 4),\n",
    "        \"weighted_f1\": round(w_f1, 4),\n",
    "    }\n",
    "\n",
    "# Convert summary to DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Models as rows\n",
    "\n",
    "# Rename columns\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "]\n",
    "\n",
    "# Display final table\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabef0de",
   "metadata": {},
   "source": [
    "## Semantic Evaluation (Base vs Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16c67f",
   "metadata": {},
   "source": [
    "In this section, we evaluate and compare the **Base** and **Sentiment-enhanced** keyword extraction models using a **semantic similarity approach** based on contextual embeddings.\n",
    "\n",
    "Traditional evaluation metrics rely on exact or approximate string matching between predicted and ground truth keywords. However, this approach may miss semantically related terms that differ lexically but convey the same meaning â€” such as *\"scam\"* and *\"fraud\"*.\n",
    "\n",
    "To address this limitation, we adopt a **global semantic evaluation**, where all predicted and ground truth keywords across the dataset are compared using **dense sentence embeddings** generated by a pre-trained transformer (e.g., Sentence-BERT).\n",
    "\n",
    "#### **Semantic Evaluation Procedure**\n",
    "\n",
    "1. **Embedding Keywords Globally**  \n",
    "   All predicted and ground truth keywords across all reviews are embedded into high-dimensional vectors using the same transformer model. Ground truth keywords are embedded **once**, and all vectors are normalized to allow cosine similarity comparisons.\n",
    "\n",
    "2. **Computing Similarity Matrix**  \n",
    "   For each model, we compute a cosine similarity matrix between **all predicted keywords** and **all ground truth keywords**.\n",
    "\n",
    "3. **Matching Threshold**  \n",
    "   A predicted keyword is considered a **semantic match** if its cosine similarity with at least one ground truth keyword exceeds a fixed threshold (e.g., **0.75**). This allows for flexible yet meaningful semantic alignment.\n",
    "\n",
    "4. **Global Semantic Precision**  \n",
    "   The proportion of predicted keywords that have at least one semantic match in the ground truth. This reflects how many of the model's predictions are semantically relevant.\n",
    "\n",
    "5. **Global Semantic Recall**  \n",
    "   The proportion of ground truth keywords that are captured by semantically similar predictions. This indicates how well the model covers the key concepts.\n",
    "\n",
    "6. **Global Semantic F1-score**  \n",
    "   The harmonic mean of semantic precision and recall, summarizing both relevance and coverage into a single score.\n",
    "\n",
    "This evaluation:\n",
    "\n",
    "- Is **more robust** than string-based metrics.\n",
    "- **Captures meaning**, not just surface forms.\n",
    "- Helps evaluate models that paraphrase or generalize beyond exact matches.\n",
    "\n",
    "This evaluation complements previous metrics and provides a more **realistic estimate** of how well the models capture the essence of user-annotated keywords in a global and context-aware manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f779851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model from the SentenceTransformers family\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model to generate contextual embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "def embed_keywords(keywords, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute sentence embeddings for a list of keyword strings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    keywords : List[str]\n",
    "        A list of keyword strings to encode.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Normalized embeddings tensor of shape (num_keywords, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Return empty tensor if input list is empty\n",
    "    if not keywords:\n",
    "        return torch.empty(0, encoder.config.hidden_size).to(device)\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the encoder to get hidden states\n",
    "        outputs = encoder(**inputs)\n",
    "\n",
    "        # Use mean pooling on the last hidden state to get fixed-size embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity computations\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def evaluate_semantic_keywords(all_pred_keywords, gt_keywords, threshold=0.75, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute global semantic precision, recall, and F1 score between all predicted keywords\n",
    "    and ground truth keywords using cosine similarity over embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    all_pred_keywords : List[List[str]]\n",
    "        List of predicted keywords for each review.\n",
    "    gt_keywords : List[str]\n",
    "        Global list of ground truth keywords for the movie.\n",
    "    threshold : float\n",
    "        Cosine similarity threshold for considering a match.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    precision : float\n",
    "    recall : float\n",
    "    f1 : float\n",
    "    \"\"\"\n",
    "    # Early return if either set is empty\n",
    "    if len(all_pred_keywords) == 0 or len(gt_keywords) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute embeddings\n",
    "    pred_emb = embed_keywords(all_pred_keywords, device=device)\n",
    "    gt_emb = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    sims = torch.matmul(pred_emb, gt_emb.T)\n",
    "\n",
    "    # Match counting based on threshold\n",
    "    pred_matches = (sims > threshold).any(dim=1).float().sum().item()\n",
    "    gt_matches = (sims > threshold).any(dim=0).float().sum().item()\n",
    "\n",
    "    precision = pred_matches / len(all_pred_keywords)\n",
    "    recall = gt_matches / len(gt_keywords)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce9bda",
   "metadata": {},
   "source": [
    "### Semantic Evaluation of Base and Sentiment Models Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa566c",
   "metadata": {},
   "source": [
    "In this step, we evaluate the **semantic similarity** between the predicted keywords of two models â€” **Base** and **Sentiment-enhanced** â€” and the ground truth keywords using **sentence embeddings**.\n",
    "\n",
    "Unlike exact or approximate string matching, this method leverages **contextual embeddings** from a pre-trained transformer to assess how semantically close the predicted keywords are to the reference keywords.\n",
    "\n",
    "The evaluation procedure is as follows:\n",
    "\n",
    "- We extract only the **text** of the predicted keywords for each model, discarding their confidence scores.\n",
    "- We embed all **predicted** and **ground truth** keywords using the same sentence transformer model.\n",
    "- Embeddings are **normalized** to ensure cosine similarity is a valid similarity measure.\n",
    "- For each predicted keyword, we compute the **cosine similarity** with all ground truth keywords.\n",
    "- A predicted keyword is considered a **semantic match** if its similarity with any ground truth keyword exceeds a fixed threshold (e.g., **0.75**).\n",
    "\n",
    "Once all matches are determined across all reviews of the selected movie, we compute:\n",
    "\n",
    "- **Semantic Precision**: Fraction of all predicted keywords (global) that have a semantic match.\n",
    "- **Semantic Recall**: Fraction of all ground truth keywords that are matched by at least one semantically similar predicted keyword.\n",
    "- **Semantic F1-score**: Harmonic mean of semantic precision and recall.\n",
    "\n",
    "This global semantic evaluation better reflects the modelsâ€™ ability to capture **meaningful and relevant keywords**, even when the wording differs from the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbbd11b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9ed35\">\n",
       "  <caption>Global Semantic-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9ed35_level0_col0\" class=\"col_heading level0 col0\" >Semantic_Precision</th>\n",
       "      <th id=\"T_9ed35_level0_col1\" class=\"col_heading level0 col1\" >Semantic_Recall</th>\n",
       "      <th id=\"T_9ed35_level0_col2\" class=\"col_heading level0 col2\" >Semantic_F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9ed35_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_9ed35_row0_col0\" class=\"data row0 col0\" >0.6736</td>\n",
       "      <td id=\"T_9ed35_row0_col1\" class=\"data row0 col1\" >0.4890</td>\n",
       "      <td id=\"T_9ed35_row0_col2\" class=\"data row0 col2\" >0.5666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9ed35_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_9ed35_row1_col0\" class=\"data row1 col0\" >0.4180</td>\n",
       "      <td id=\"T_9ed35_row1_col1\" class=\"data row1 col1\" >0.5441</td>\n",
       "      <td id=\"T_9ed35_row1_col2\" class=\"data row1 col2\" >0.4728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x161071550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute embeddings for the ground truth keywords once per selected movie\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Dictionary to collect all predicted keywords per model (without duplicates)\n",
    "all_predictions = {model: set() for model in models_to_evaluate}\n",
    "\n",
    "# Collect predicted keywords across all reviews (as a set for uniqueness)\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            # Extract keyword strings and normalize\n",
    "            pred_kw = [normalize_kw(kw) for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            all_predictions[model].update(pred_kw)  # Add to set (no duplicates)\n",
    "\n",
    "# Compute semantic evaluation globally for each model\n",
    "semantic_scores = []\n",
    "for model in models_to_evaluate:\n",
    "    pred_kw = list(all_predictions[model])  # Convert back to list\n",
    "    precision, recall, f1 = evaluate_semantic_keywords(pred_kw, gt_keywords, device=device)\n",
    "\n",
    "    semantic_scores.append({\n",
    "        \"Model\": model,\n",
    "        \"Semantic_Precision\": round(precision, 4),\n",
    "        \"Semantic_Recall\": round(recall, 4),\n",
    "        \"Semantic_F1\": round(f1, 4)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and format\n",
    "summary_df = pd.DataFrame(semantic_scores).set_index(\"Model\")\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Semantic-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bd7af",
   "metadata": {},
   "source": [
    "## Sentiment Appropriateness Score (SAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b08c7",
   "metadata": {},
   "source": [
    "To further evaluate the quality of predicted keywords from a sentiment-aware perspective, we introduce the **Sentiment Appropriateness Score (SAS)**. This metric assesses how well the **overall sentiment of the predicted keywords** aligns with the **sentiment of the ground truth keywords** or the **sentiment of the full review text**, providing a complementary dimension to traditional keyword evaluation.\n",
    "\n",
    "Unlike standard metrics like **Precision**, **Recall**, or **F1**, which measure lexical or semantic correctness, SAS explicitly measures **emotional alignment**.\n",
    "\n",
    "#### **Two Global Evaluation Schemes**\n",
    "\n",
    "1. **SAS from Ground Truth Keywords**\n",
    "\n",
    "   - The sentiment of the reference is approximated using all ground truth keywords for the movie.\n",
    "   - Each ground truth keyword is analyzed using **VADER** sentiment analyzer.\n",
    "   - The sentiment score is calculated as a weighted combination:\n",
    "     - `pos` â†’ 1.0\n",
    "     - `neu` â†’ 0.5\n",
    "     - `neg` â†’ 0.0\n",
    "   - The average sentiment across all ground truth keywords forms the **reference sentiment**.\n",
    "   - This is compared to the **global average sentiment** of all predicted keywords (across reviews).\n",
    "\n",
    "2. **SAS from Review Text**\n",
    "\n",
    "   - The sentiment of the review set is approximated by aggregating the sentiments of the full review texts.\n",
    "   - Each review is processed with VADER using the same weighted scoring.\n",
    "   - The resulting average sentiment is compared to the sentiment of all predicted keywords.\n",
    "\n",
    "#### **Formula**\n",
    "\n",
    "$$\n",
    "\\text{SAS} = 1 - \\left| \\text{Sentiment}_{\\text{predicted}} - \\text{Sentiment}_{\\text{reference}} \\right|\n",
    "$$\n",
    "\n",
    "SAS is a value in **\\[0, 1\\]**, where values closer to **1** indicate stronger emotional coherence between the predicted keywords and the reference source (either ground truth keywords or full review text).\n",
    "\n",
    "#### **Why VADER?**\n",
    "\n",
    "We choose **VADER (Valence Aware Dictionary and sEntiment Reasoner)** for its suitability in this context:\n",
    "\n",
    "- It is optimized for **short, informal text** like tags and keywords.\n",
    "- It works well on **single words and short phrases**, which represent our prediction outputs.\n",
    "- It provides interpretable and **probabilistic sentiment scores** (`pos`, `neu`, `neg`).\n",
    "- It is lightweight, efficient, and scalable for large-scale global evaluations.\n",
    "\n",
    "Although transformer-based sentiment models may offer more nuanced analysis on long text, VADER provides the best **balance of performance, interpretability, and scalability** for evaluating the emotional tone of predicted keywords in our keyword extraction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75a839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sas_from_keywords(all_predicted_keywords, ground_truth_keywords=None, analyzer=None, sentiment_gt=None):\n",
    "    \"\"\"\n",
    "    Computes global SAS by comparing the average sentiment of all predicted keywords (across reviews)\n",
    "    to the sentiment of the ground truth keywords (either computed or pre-given).\n",
    "\n",
    "    Args:\n",
    "        all_predicted_keywords (list of list of dict): list of predicted keyword dicts per review (each dict has 'sentiment_score' âˆˆ [0,1])\n",
    "        ground_truth_keywords (list of str): optional, used if sentiment_gt is not given\n",
    "        analyzer (SentimentIntensityAnalyzer): optional, used if sentiment_gt is not given\n",
    "        sentiment_gt (float): optional precomputed global ground truth sentiment âˆˆ [0,1]\n",
    "\n",
    "    Returns:\n",
    "        float: SAS âˆˆ [0,1] â€” higher means better emotional alignment with ground truth\n",
    "    \"\"\"\n",
    "    # Compute GT sentiment if not precomputed\n",
    "    if sentiment_gt is None:\n",
    "        if not ground_truth_keywords or analyzer is None:\n",
    "            return None\n",
    "        gt_scores = [\n",
    "            analyzer.polarity_scores(kw)\n",
    "            for kw in ground_truth_keywords\n",
    "        ]\n",
    "        sentiments_gt = [\n",
    "            1.0 * s[\"pos\"] + 0.5 * s[\"neu\"] + 0.0 * s[\"neg\"]\n",
    "            for s in gt_scores\n",
    "        ]\n",
    "        sentiment_gt = sum(sentiments_gt) / len(sentiments_gt) if sentiments_gt else 0.0\n",
    "\n",
    "    # Collect all predicted sentiments across reviews\n",
    "    all_sentiments_pred = []\n",
    "    for review_preds in all_predicted_keywords:\n",
    "        all_sentiments_pred.extend(\n",
    "            [kw['sentiment_score'] for kw in review_preds]\n",
    "        )\n",
    "\n",
    "    # Compute global average of predicted sentiment\n",
    "    if not all_sentiments_pred:\n",
    "        return None\n",
    "    sentiment_pred = sum(all_sentiments_pred) / len(all_sentiments_pred)\n",
    "\n",
    "    # Global SAS\n",
    "    return 1 - abs(sentiment_pred - sentiment_gt)\n",
    "\n",
    "\n",
    "def compute_sas_from_text(all_predicted_keywords, review_texts=None, analyzer=None, sentiment_text=None):\n",
    "    \"\"\"\n",
    "    Computes global SAS by comparing the average sentiment of all predicted keywords\n",
    "    to the sentiment of the full movie text corpus (or average of review texts).\n",
    "\n",
    "    Args:\n",
    "        all_predicted_keywords (list of list of dict): list of predicted keyword dicts per review (each dict has 'sentiment_score' âˆˆ [0,1])\n",
    "        review_texts (list of str): optional, used if sentiment_text is not given\n",
    "        analyzer (SentimentIntensityAnalyzer): optional, used if sentiment_text is not given\n",
    "        sentiment_text (float): optional precomputed global sentiment of review text âˆˆ [0,1]\n",
    "\n",
    "    Returns:\n",
    "        float: SAS âˆˆ [0,1] â€” higher means better emotional alignment with full review sentiment\n",
    "    \"\"\"\n",
    "    # Compute sentiment from full text if not precomputed\n",
    "    if sentiment_text is None:\n",
    "        if not review_texts or analyzer is None:\n",
    "            return None\n",
    "        text_scores = [\n",
    "            analyzer.polarity_scores(text)\n",
    "            for text in review_texts if isinstance(text, str)\n",
    "        ]\n",
    "        sentiments_text = [\n",
    "            1.0 * s[\"pos\"] + 0.5 * s[\"neu\"] + 0.0 * s[\"neg\"]\n",
    "            for s in text_scores\n",
    "        ]\n",
    "        sentiment_text = sum(sentiments_text) / len(sentiments_text) if sentiments_text else 0.0\n",
    "\n",
    "    # Collect all predicted keyword sentiments across reviews\n",
    "    all_sentiments_pred = []\n",
    "    for review_preds in all_predicted_keywords:\n",
    "        all_sentiments_pred.extend(\n",
    "            [kw['sentiment_score'] for kw in review_preds]\n",
    "        )\n",
    "\n",
    "    if not all_sentiments_pred:\n",
    "        return None\n",
    "    sentiment_pred = sum(all_sentiments_pred) / len(all_sentiments_pred)\n",
    "\n",
    "    # Global SAS\n",
    "    return 1 - abs(sentiment_pred - sentiment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c53932",
   "metadata": {},
   "source": [
    "### Sentiment Appropriateness Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47dae1",
   "metadata": {},
   "source": [
    "In this section, we evaluate the sentiment alignment of predicted keywords for each model using the **Sentiment Appropriateness Score (SAS)**.\n",
    "\n",
    "Unlike traditional metrics, SAS assesses how well the **overall sentiment of the predicted keywords** reflects the sentiment of the movieâ€™s content, based on two global references:\n",
    "\n",
    "- **SAS from Ground Truth Keywords**  \n",
    "  Compares the average sentiment of all predicted keywords to that of the ground truth keywords using **VADER**.\n",
    "\n",
    "- **SAS from Review Texts**  \n",
    "  Compares the predicted sentiment to the overall sentiment of all preprocessed reviews.\n",
    "\n",
    "Both reference sentiments are computed **globally per movie**, and each model's predictions are evaluated accordingly.  \n",
    "SAS is defined as:\n",
    "\n",
    "$$\n",
    "\\text{SAS} = 1 - \\left| \\text{sentiment}_{\\text{pred}} - \\text{sentiment}_{\\text{ref}} \\right|\n",
    "$$\n",
    "\n",
    "Higher scores indicate better emotional alignment between the predicted keywords and the movie's actual sentiment.\n",
    "\n",
    "The results are summarized in a table for **global comparison across models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b0c92f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6b198\">\n",
       "  <caption>Sentiment Appropriateness Score (SAS) - Global Evaluation</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6b198_level0_col0\" class=\"col_heading level0 col0\" >SAS from keywords</th>\n",
       "      <th id=\"T_6b198_level0_col1\" class=\"col_heading level0 col1\" >SAS from text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6b198_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_6b198_row0_col0\" class=\"data row0 col0\" >0.9835</td>\n",
       "      <td id=\"T_6b198_row0_col1\" class=\"data row0 col1\" >0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6b198_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_6b198_row1_col0\" class=\"data row1 col0\" >0.9917</td>\n",
       "      <td id=\"T_6b198_row1_col1\" class=\"data row1 col1\" >0.9442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x323800b80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Precompute global ground truth sentiment (from all keywords in the movie)\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "sentiments_gt = []\n",
    "for kw in gt_keywords:\n",
    "    scores = analyzer.polarity_scores(kw)\n",
    "    sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "    sentiments_gt.append(sentiment)\n",
    "sentiment_gt = sum(sentiments_gt) / len(sentiments_gt) if sentiments_gt else None\n",
    "\n",
    "# Precompute global review sentiment (from all review texts)\n",
    "all_review_texts = selected_film[\"Preprocessed_Review\"].dropna().tolist()\n",
    "sentiments_text = []\n",
    "for text in all_review_texts:\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "    sentiments_text.append(sentiment)\n",
    "sentiment_text = sum(sentiments_text) / len(sentiments_text) if sentiments_text else None\n",
    "\n",
    "# Collect SAS results\n",
    "sas_scores = []\n",
    "\n",
    "for model in models_to_evaluate:\n",
    "    all_predicted_keywords = []\n",
    "\n",
    "    for _, row in selected_film.iterrows():\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            for kw, score in row[pred_col]:\n",
    "                if isinstance(kw, str) and isinstance(score, (float, int)):\n",
    "                    all_predicted_keywords.append({\n",
    "                        \"keyword\": kw,\n",
    "                        \"sentiment_score\": float(score)\n",
    "                    })\n",
    "\n",
    "    if not all_predicted_keywords:\n",
    "        continue\n",
    "\n",
    "    # Compute SAS from GT keywords\n",
    "    sas_kw = compute_sas_from_keywords([all_predicted_keywords], sentiment_gt=sentiment_gt)\n",
    "\n",
    "    # Compute SAS from full review text\n",
    "    sas_text = compute_sas_from_text([all_predicted_keywords], sentiment_text=sentiment_text)\n",
    "\n",
    "    result = {\"Model\": model}\n",
    "    if sas_kw is not None:\n",
    "        result[\"SAS from keywords\"] = round(sas_kw, 4)\n",
    "    if sas_text is not None:\n",
    "        result[\"SAS from text\"] = round(sas_text, 4)\n",
    "\n",
    "    sas_scores.append(result)\n",
    "\n",
    "# Create and display summary DataFrame\n",
    "sas_df = pd.DataFrame(sas_scores).set_index(\"Model\")\n",
    "sas_df.style.format(precision=4).set_caption(\"Sentiment Appropriateness Score (SAS) - Global Evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187183",
   "metadata": {},
   "source": [
    "This section automatically processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains predicted keywords generated by different models.\n",
    "\n",
    "For **each movie**:\n",
    "\n",
    "- The corresponding **ground truth keywords** are loaded.\n",
    "- Predicted keywords from both models â€” **Base** and **Sentiment-aware** â€” are aggregated across all reviews.\n",
    "- The following **global evaluation metrics** are computed:\n",
    "\n",
    "#### Unweighted Metrics\n",
    "- **Precision**, **Recall**, and **F1-score**  \n",
    "  Based on approximate string matching between predicted and ground truth keywords, without considering prediction scores.\n",
    "\n",
    "#### Score-aware Metrics\n",
    "- **Weighted Precision**, **Weighted Recall**, **Weighted F1-score**  \n",
    "  Evaluate prediction correctness while incorporating confidence scores.\n",
    "  \n",
    "- **nDCG@5**  \n",
    "  Measures the ranking quality of the top 5 predicted keywords, rewarding correct keywords ranked higher.\n",
    "\n",
    "#### Semantic Metrics\n",
    "- **Semantic Precision**, **Semantic Recall**, **Semantic F1-score**  \n",
    "  Computed using cosine similarity between **sentence embeddings** of predicted and reference keywords.\n",
    "\n",
    "#### Sentiment Alignment Metrics\n",
    "- **SAS_from_keywords**:  \n",
    "  Measures how well the average sentiment of predicted keywords aligns with the sentiment of the ground truth keywords (via VADER).\n",
    "  \n",
    "- **SAS_from_text**:  \n",
    "  Measures alignment with the sentiment of the full review texts.\n",
    "\n",
    "All metrics are computed **globally per movie**, not per review, and the results are compiled into a **comprehensive summary table** for comparison across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79e56b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b1964\">\n",
       "  <caption>Global Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b1964_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_b1964_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_b1964_level0_col2\" class=\"col_heading level0 col2\" >Precision</th>\n",
       "      <th id=\"T_b1964_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_b1964_level0_col4\" class=\"col_heading level0 col4\" >F1-score</th>\n",
       "      <th id=\"T_b1964_level0_col5\" class=\"col_heading level0 col5\" >Weighted Precision</th>\n",
       "      <th id=\"T_b1964_level0_col6\" class=\"col_heading level0 col6\" >Weighted Recall</th>\n",
       "      <th id=\"T_b1964_level0_col7\" class=\"col_heading level0 col7\" >Weighted F1-score</th>\n",
       "      <th id=\"T_b1964_level0_col8\" class=\"col_heading level0 col8\" >Semantic Precision</th>\n",
       "      <th id=\"T_b1964_level0_col9\" class=\"col_heading level0 col9\" >Semantic Recall</th>\n",
       "      <th id=\"T_b1964_level0_col10\" class=\"col_heading level0 col10\" >Semantic F1-score</th>\n",
       "      <th id=\"T_b1964_level0_col11\" class=\"col_heading level0 col11\" >SAS from Keywords</th>\n",
       "      <th id=\"T_b1964_level0_col12\" class=\"col_heading level0 col12\" >SAS from text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b1964_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b1964_row0_col0\" class=\"data row0 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_b1964_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_b1964_row0_col2\" class=\"data row0 col2\" >0.8897</td>\n",
       "      <td id=\"T_b1964_row0_col3\" class=\"data row0 col3\" >0.3460</td>\n",
       "      <td id=\"T_b1964_row0_col4\" class=\"data row0 col4\" >0.4982</td>\n",
       "      <td id=\"T_b1964_row0_col5\" class=\"data row0 col5\" >0.9155</td>\n",
       "      <td id=\"T_b1964_row0_col6\" class=\"data row0 col6\" >0.1789</td>\n",
       "      <td id=\"T_b1964_row0_col7\" class=\"data row0 col7\" >0.2993</td>\n",
       "      <td id=\"T_b1964_row0_col8\" class=\"data row0 col8\" >0.6736</td>\n",
       "      <td id=\"T_b1964_row0_col9\" class=\"data row0 col9\" >0.4890</td>\n",
       "      <td id=\"T_b1964_row0_col10\" class=\"data row0 col10\" >0.5666</td>\n",
       "      <td id=\"T_b1964_row0_col11\" class=\"data row0 col11\" >0.9835</td>\n",
       "      <td id=\"T_b1964_row0_col12\" class=\"data row0 col12\" >0.9690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b1964_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b1964_row1_col0\" class=\"data row1 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_b1964_row1_col1\" class=\"data row1 col1\" >sentiment</td>\n",
       "      <td id=\"T_b1964_row1_col2\" class=\"data row1 col2\" >0.8471</td>\n",
       "      <td id=\"T_b1964_row1_col3\" class=\"data row1 col3\" >0.3217</td>\n",
       "      <td id=\"T_b1964_row1_col4\" class=\"data row1 col4\" >0.4663</td>\n",
       "      <td id=\"T_b1964_row1_col5\" class=\"data row1 col5\" >0.8914</td>\n",
       "      <td id=\"T_b1964_row1_col6\" class=\"data row1 col6\" >0.1597</td>\n",
       "      <td id=\"T_b1964_row1_col7\" class=\"data row1 col7\" >0.2709</td>\n",
       "      <td id=\"T_b1964_row1_col8\" class=\"data row1 col8\" >0.4180</td>\n",
       "      <td id=\"T_b1964_row1_col9\" class=\"data row1 col9\" >0.5441</td>\n",
       "      <td id=\"T_b1964_row1_col10\" class=\"data row1 col10\" >0.4728</td>\n",
       "      <td id=\"T_b1964_row1_col11\" class=\"data row1 col11\" >0.9917</td>\n",
       "      <td id=\"T_b1964_row1_col12\" class=\"data row1 col12\" >0.9442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x323997790>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load ground truth keywords\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Container to store results for all movies and models\n",
    "global_results = []\n",
    "\n",
    "# Iterate over all predicted keyword files (one file per movie)\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords DataFrame and movie ID\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Get ground truth keywords for this movie\n",
    "            kw_ground_truth = keywords_ground_truth[\n",
    "                keywords_ground_truth[\"Movie_ID\"] == selected_film_id\n",
    "            ]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Compute average sentiment of ground truth keywords\n",
    "            gt_sentiments = []\n",
    "            for kw in gt_keywords:\n",
    "                scores = analyzer.polarity_scores(kw)\n",
    "                sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"]\n",
    "                gt_sentiments.append(sentiment)\n",
    "            sentiment_gt = sum(gt_sentiments) / len(gt_sentiments) if gt_sentiments else None\n",
    "\n",
    "            # Compute average sentiment from all review texts\n",
    "            review_texts = selected_film[\"Preprocessed_Review\"].dropna().tolist()\n",
    "            review_sentiments = []\n",
    "            for text in review_texts:\n",
    "                scores = analyzer.polarity_scores(text)\n",
    "                sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"]\n",
    "                review_sentiments.append(sentiment)\n",
    "            sentiment_text = sum(review_sentiments) / len(review_sentiments) if review_sentiments else None\n",
    "\n",
    "            # Evaluate each model globally\n",
    "            for model in models_to_evaluate:\n",
    "                pred_col = f\"keywords_{model}\"\n",
    "\n",
    "                # Lists of lists for evaluation functions:\n",
    "                # - pred_kw_per_review: list of lists of keywords (strings) for evaluate_keywords()\n",
    "                # - pred_kwscore_per_review: list of lists of (keyword, score) tuples for evaluate_keywords_weighted()\n",
    "                pred_kw_per_review = []\n",
    "                pred_kwscore_per_review = []\n",
    "                \n",
    "                # Flat list of dicts for SAS functions\n",
    "                flat_keyword_list = []\n",
    "\n",
    "                # Iterate over reviews to build these lists\n",
    "                for _, row in selected_film.iterrows():\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "\n",
    "                        # Extract keywords only for evaluate_keywords()\n",
    "                        pred_kw_only = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "\n",
    "                        # Extract (keyword, score) tuples for evaluate_keywords_weighted()\n",
    "                        pred_kw_score = [\n",
    "                            (kw, score) for kw, score in row[pred_col]\n",
    "                            if isinstance(kw, str) and isinstance(score, (float, int))\n",
    "                        ]\n",
    "\n",
    "                        # Append per review keyword lists if not empty\n",
    "                        if pred_kw_only:\n",
    "                            pred_kw_per_review.append(pred_kw_only)\n",
    "                        if pred_kw_score:\n",
    "                            pred_kwscore_per_review.append(pred_kw_score)\n",
    "\n",
    "                        # Append flat dicts for SAS calculations\n",
    "                        for kw, score in pred_kw_score:\n",
    "                            flat_keyword_list.append({\n",
    "                                \"keyword\": kw,\n",
    "                                \"sentiment_score\": float(score)\n",
    "                            })\n",
    "\n",
    "                # Compute classic precision, recall, F1\n",
    "                precision, recall, f1 = evaluate_keywords(pred_kw_per_review, gt_keywords)\n",
    "\n",
    "                # Compute weighted precision, recall, F1\n",
    "                w_precision, w_recall, w_f1 = evaluate_keywords_weighted(pred_kwscore_per_review, gt_keywords)\n",
    "\n",
    "                # Compute semantic precision, recall, F1\n",
    "                flat_kw_list = [kw for review in pred_kw_per_review for kw in review]\n",
    "                flat_kw_list = list(set(flat_kw_list))\n",
    "\n",
    "                semantic_precision, semantic_recall, semantic_f1 = evaluate_semantic_keywords(\n",
    "                    flat_kw_list, gt_keywords, device=device\n",
    "                )\n",
    "\n",
    "                # Compute SAS (Sentiment Appropriateness Score)\n",
    "                sas_kw = compute_sas_from_keywords([flat_keyword_list], sentiment_gt=sentiment_gt)\n",
    "                sas_txt = compute_sas_from_text([flat_keyword_list], sentiment_text=sentiment_text)\n",
    "\n",
    "                # Store results for this movie-model pair\n",
    "                global_results.append({\n",
    "                    \"Movie\": movie_name,\n",
    "                    \"Model\": model,\n",
    "                    \"Precision\": precision,\n",
    "                    \"Recall\": recall,\n",
    "                    \"F1-score\": f1,\n",
    "                    \"Weighted Precision\": w_precision,\n",
    "                    \"Weighted Recall\": w_recall,\n",
    "                    \"Weighted F1-score\": w_f1,\n",
    "                    \"Semantic Precision\": semantic_precision,\n",
    "                    \"Semantic Recall\": semantic_recall,\n",
    "                    \"Semantic F1-score\": semantic_f1,\n",
    "                    \"SAS from Keywords\": sas_kw,\n",
    "                    \"SAS from text\": sas_txt\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create final DataFrame and sort results\n",
    "final_df = pd.DataFrame(global_results)\n",
    "final_df = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df.style.format(precision=4).set_caption(\"Global Evaluation Summary per Movie and Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
