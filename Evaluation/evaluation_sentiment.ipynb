{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421a1a",
   "metadata": {},
   "source": [
    "# Evaluation of KeyBERTSentimentAware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804715a2",
   "metadata": {},
   "source": [
    "This notebook evaluates and compares different keyword extraction models applied to movie reviews, with a specific focus on assessing how well each model captures not only semantic relevance but also **sentiment alignment** with the content of the reviews.\n",
    "\n",
    "We aim to assess the performance of two models:\n",
    "\n",
    "- **Base** KeyBERT model\n",
    "\n",
    "- **Sentiment-aware** version that integrates a custom sentiment classifier to enrich the keyword scoring process (KeyBERTSentimentAware)\n",
    "\n",
    "The evaluation is performed on a set of reviews for each movie, where both models predict a ranked list of top-5 keywords per review. Each keyword is associated with a confidence score; in the sentiment-aware model, this score incorporates both relevance and predicted sentiment intensity.\n",
    "\n",
    "The notebook uses:\n",
    "\n",
    "- A **ground truth dataset** of annotated keywords per movie retrieved from IMDB\n",
    "\n",
    "- **Model outputs**: top-5 predicted keywords per review, each with a relevance (and possibly sentiment-based) score\n",
    "\n",
    "Four complementary evaluation layers are used to provide a comprehensive comparison:\n",
    "\n",
    "**1. Basic (Unweighted) Metrics**\n",
    "\n",
    "- **Precision**, **Recall**, and **F1-score** based on approximate binary matching\n",
    "\n",
    "- A keyword is considered correct if it approximately matches any of the ground truth keywords for the movie\n",
    "\n",
    "- Each review is evaluated independently and the scores are averaged\n",
    "\n",
    "**2. Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision/Recall/F1**: correct predictions are weighted based on the modelâ€™s score (semantic or sentiment-aware)\n",
    "\n",
    "- **nDCG@5**: evaluates how well the top-ranked predictions align with the expected set of keywords, rewarding correct keywords appearing earlier\n",
    "\n",
    "**3. Semantic Evaluation (Embedding-Based)**\n",
    "\n",
    "- Uses **sentence-transformer embeddings** and **cosine similarity** to detect approximate semantic matches between predicted and ground truth keywords\n",
    "\n",
    "- A predicted keyword is considered correct if it reaches a similarity threshold (e.g., 0.75) with any ground truth keyword\n",
    "\n",
    "- Semantic Precision, Recall, and F1-score are then computed based on these soft matches\n",
    "\n",
    "**4. Sentiment Appropriateness Score (SAS)**\n",
    "\n",
    "- Evaluates how well the **average sentiment of the predicted keywords** aligns with the **sentiment of the review**\n",
    "\n",
    "- Two variants are computed:\n",
    "\n",
    "  - `SAS_from_keywords`: compares the sentiment of predicted keywords to the average sentiment of the ground truth keywords (estimated using VADER)\n",
    "\n",
    "  - `SAS_from_text`: compares the sentiment of predicted keywords to the sentiment of the full preprocessed review text\n",
    "\n",
    "- SAS values closer to 1 indicate better emotional alignment between the predictions and the reference\n",
    "\n",
    "The sentiment-aware model is designed not just to select relevant keywords, but also to reflect the emotional tone of the review in its predictions. While the base KeyBERT model selects keywords based solely on semantic similarity, the sentiment-aware version explicitly integrates sentiment prediction into the keyword ranking.\n",
    "\n",
    "By introducing **SAS**, we quantify this difference and validate whether the added sentiment signal results in predictions that are not only relevant but also emotionally coherent with the content.\n",
    "\n",
    "This sentiment-aware evaluation helps bridge the gap between **semantic correctness** and **emotional alignment**, offering a more holistic understanding of keyword quality in user-generated content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526626c",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0baee66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "vaderSentiment is already installed.\n",
      "numpy is already installed.\n",
      "tqdm is already installed.\n",
      "torch is already installed.\n",
      "transformers is already installed.\n",
      "pandas is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"scikit-learn\", \"tqdm\", \"transformers\", \"torch\", \"vaderSentiment\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "973bef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "import math    # Mathematical functions (e.g., logarithms for nDCG calculation)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Transformers and PyTorch for embeddings and models\n",
    "from transformers import AutoTokenizer, AutoModel # type:ignore\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9ee47",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189c179",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55796a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edbda1",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602be9",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71a8e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"Parasite\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fd95b",
   "metadata": {},
   "source": [
    "## Keyword Matching and Evaluation Functions (Basic â€“ Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a94a",
   "metadata": {},
   "source": [
    "This block defines the baseline utility functions used to evaluate predicted keywords against the ground truth. These functions do **not** take into account keyword confidence scores or rankingâ€”they perform **binary, unweighted evaluation**.\n",
    "\n",
    "Specifically, this implementation includes:\n",
    "\n",
    "- **Normalization**: keywords are converted to lowercase, stripped of punctuation, and cleaned of extra whitespace to ensure consistent matching.\n",
    "\n",
    "- **Approximate Matching**: a relaxed rule that considers two keywords as matching if they are identical or if one is a substring of the other (e.g., *\"social satire\"* â‰ˆ *\"satire\"*).\n",
    "\n",
    "- **Evaluation**: standard metrics â€” **precision**, **recall**, and **F1-score** â€” are calculated based on the number of approximate matches between predicted and ground truth keywords.\n",
    "\n",
    "This provides a basic but interpretable way to assess keyword extraction quality without considering the ranking or confidence scores assigned by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f79c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches the ground truth exactly\n",
    "# or if either keyword contains the other as a substring\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Evaluation function for a single prediction instance:\n",
    "# - Normalizes both predicted and ground truth keywords\n",
    "# - Computes how many predicted keywords approximately match the ground truth\n",
    "# - Calculates precision, recall, and F1-score\n",
    "def evaluate_keywords(pred_keywords, gt_keywords):\n",
    "    pred_keywords = [normalize_kw(k) for k in pred_keywords]\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    \n",
    "    # Count how many predicted keywords match approximately with any ground truth keyword\n",
    "    match_count = sum([is_approx_match(k, gt_keywords) for k in pred_keywords])\n",
    "    \n",
    "    # Precision: percentage of predicted keywords that are correct\n",
    "    precision = match_count / len(pred_keywords) if pred_keywords else 0\n",
    "\n",
    "    # Recall: percentage of ground truth keywords that were correctly predicted\n",
    "    recall = match_count / len(gt_keywords) if gt_keywords else 0\n",
    "\n",
    "    # F1-score: harmonic mean of precision and recall\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cda19",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Basic â€“ Unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0199",
   "metadata": {},
   "source": [
    "This section evaluates two keyword extraction models â€” **base** and **sentiment-enhanced** â€” using the ground truth.\n",
    "\n",
    "For each **review**, basic precision, recall, and F1-score are computed based on binary keyword matching. These metrics are then **averaged across all reviews** to provide an overall performance comparison between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d333544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1a157\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1a157_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_1a157_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_1a157_level0_col2\" class=\"col_heading level0 col2\" >F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1a157_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_1a157_row0_col0\" class=\"data row0 col0\" >0.1600</td>\n",
       "      <td id=\"T_1a157_row0_col1\" class=\"data row0 col1\" >0.0025</td>\n",
       "      <td id=\"T_1a157_row0_col2\" class=\"data row0 col2\" >0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1a157_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_1a157_row1_col0\" class=\"data row1 col0\" >0.2800</td>\n",
       "      <td id=\"T_1a157_row1_col1\" class=\"data row1 col1\" >0.0044</td>\n",
       "      <td id=\"T_1a157_row1_col2\" class=\"data row1 col2\" >0.0087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x171f36280>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Create a dictionary to store evaluation results (precision, recall, F1) for each model\n",
    "results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Extract the list of ground truth keywords for the selected movie (same for all reviews)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Iterate over each review in the selected film's predictions\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        \n",
    "        # Column name with the predicted keywords for this model\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Proceed only if the column exists and contains a list of predicted keywords\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "\n",
    "            # Extract only the keyword strings from (keyword, score) tuples\n",
    "            predicted_keywords = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "            \n",
    "            # Evaluate the prediction using precision, recall, and F1-score\n",
    "            precision, recall, f1 = evaluate_keywords(predicted_keywords, ground_truth_keywords)\n",
    "            \n",
    "            # Store the result for this specific review\n",
    "            results[model].append({\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "# Aggregate the results to compute average metrics for each model\n",
    "summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    precisions = [r[\"precision\"] for r in results[model]]\n",
    "    recalls = [r[\"recall\"] for r in results[model]]\n",
    "    f1s = [r[\"f1\"] for r in results[model]]\n",
    "    \n",
    "    # Calculate the average of each metric and round to 4 decimal places\n",
    "    summary[model] = {\n",
    "        \"avg_precision\": round(np.mean(precisions), 4),\n",
    "        \"avg_recall\": round(np.mean(recalls), 4),\n",
    "        \"avg_f1\": round(np.mean(f1s), 4)\n",
    "    }\n",
    "\n",
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1-score\",\n",
    "]\n",
    "\n",
    "# Display the summary table nicely\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc68d6",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0055848",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the modelâ€™s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b56b5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches any ground truth keyword\n",
    "# using a relaxed comparison: exact match or substring containment\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Weighted evaluation function:\n",
    "# Calculates precision, recall, and F1-score using the confidence scores of predicted keywords\n",
    "# - High-confidence correct predictions contribute more\n",
    "# - Precision is score-weighted; recall divides by total ground truth\n",
    "def evaluate_keywords_weighted(predicted_kw_score, gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate predicted keywords with confidence scores using weighted precision, recall, and F1.\n",
    "    \n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with associated confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords (annotated)\n",
    "    \n",
    "    Returns:\n",
    "        (precision, recall, f1): all metrics computed using score-weighted matching\n",
    "    \"\"\"\n",
    "    # Normalize both predicted and ground truth keywords\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords = [(normalize_kw(kw), score) for kw, score in predicted_kw_score if isinstance(kw, str)]\n",
    "    \n",
    "    total_score = sum(score for _, score in pred_keywords)\n",
    "    if total_score == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # Compute total score of predicted keywords that approximately match the ground truth\n",
    "    match_score = sum(score for kw, score in pred_keywords if is_approx_match(kw, gt_keywords))\n",
    "    \n",
    "    # Weighted precision and recall\n",
    "    precision = match_score / total_score\n",
    "    recall = match_score / len(gt_keywords) if gt_keywords else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ddf38",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c80dd7",
   "metadata": {},
   "source": [
    "In this section, we apply the score-aware evaluation metrics to each review for both models:\n",
    "\n",
    "- **Weighted Precision, Recall, F1**: accounts for the confidence scores of each predicted keyword.\n",
    "- **nDCG@5**: evaluates the ranking quality of the top-5 keywords based on their alignment with the ground truth.\n",
    "\n",
    "Each review is evaluated individually, and the metrics are then averaged across all reviews to summarize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "909cc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a1fa2\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a1fa2_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_a1fa2_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_a1fa2_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a1fa2_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_a1fa2_row0_col0\" class=\"data row0 col0\" >0.1564</td>\n",
       "      <td id=\"T_a1fa2_row0_col1\" class=\"data row0 col1\" >0.0012</td>\n",
       "      <td id=\"T_a1fa2_row0_col2\" class=\"data row0 col2\" >0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a1fa2_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_a1fa2_row1_col0\" class=\"data row1 col0\" >0.2800</td>\n",
       "      <td id=\"T_a1fa2_row1_col1\" class=\"data row1 col1\" >0.0015</td>\n",
       "      <td id=\"T_a1fa2_row1_col2\" class=\"data row1 col2\" >0.0031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x32dd2dee0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Initialize results dictionary for weighted metrics and nDCG\n",
    "weighted_results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Loop through each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = row[pred_col]  # list of (kw, score)\n",
    "\n",
    "            # Compute weighted metrics\n",
    "            w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, ground_truth_keywords)\n",
    "\n",
    "            # Save results\n",
    "            weighted_results[model].append({\n",
    "                \"weighted_precision\": w_precision,\n",
    "                \"weighted_recall\": w_recall,\n",
    "                \"weighted_f1\": w_f1,\n",
    "            })\n",
    "\n",
    "# Compute average metrics across all reviews\n",
    "weighted_summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    metrics = weighted_results[model]\n",
    "    weighted_summary[model] = {\n",
    "        \"avg_weighted_precision\": round(np.mean([m[\"weighted_precision\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_recall\": round(np.mean([m[\"weighted_recall\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_f1\": round(np.mean([m[\"weighted_f1\"] for m in metrics]), 4),\n",
    "    }\n",
    "\n",
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "]\n",
    "\n",
    "# Display the summary table\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabef0de",
   "metadata": {},
   "source": [
    "## Semantic Evaluation (Base vs Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16c67f",
   "metadata": {},
   "source": [
    "In this section, we evaluate and compare the **Base** and **Sentiment-enhanced** keyword extraction models using a **semantic similarity approach** based on contextual embeddings.\n",
    "\n",
    "Traditional evaluation metrics typically check for exact or approximate string matches between predicted and ground truth keywords. However, this can miss semantically related terms that are lexically different but convey the same meaning â€” for example, *\"scam\"* and *\"fraud\"*.\n",
    "\n",
    "To overcome this limitation, we leverage **sentence embeddings** generated by a pre-trained transformer model (such as a sentence-transformer). Each keyword â€” both predicted and ground truth â€” is converted into a dense vector representation that captures its semantic content.\n",
    "\n",
    "How the semantic evaluation works in detail:\n",
    "\n",
    "1. **Embedding keywords**:  \n",
    "    Both predicted keywords and ground truth keywords are embedded into high-dimensional vectors using the same model. The ground truth keywords are embedded **once beforehand** to avoid redundant computations. These embeddings are normalized to ensure cosine similarity is a valid similarity measure.\n",
    "\n",
    "2. **Computing similarity scores**:  \n",
    "   We calculate the **cosine similarity** between every predicted keyword embedding and every ground truth keyword embedding, resulting in a similarity matrix.\n",
    "\n",
    "3. **Determining matches using a threshold**:  \n",
    "   A predicted keyword is considered a **semantic match** if its cosine similarity with at least one ground truth keyword exceeds a set threshold (e.g., 0.75). This threshold balances between strictness and flexibility in matching semantic content.\n",
    "\n",
    "4. **Calculating semantic precision**:  \n",
    "   This is the fraction of predicted keywords that have a semantic match in the ground truth. It reflects how many of the modelâ€™s predictions are meaningful and relevant.\n",
    "\n",
    "5. **Calculating semantic recall**:  \n",
    "   This is the fraction of ground truth keywords that are captured by semantically similar predicted keywords. It indicates how well the model covers the essential concepts of the ground truth.\n",
    "\n",
    "6. **Calculating semantic F1-score**:  \n",
    "   The harmonic mean of semantic precision and recall, providing a single measure that balances both aspects.\n",
    "\n",
    "By evaluating keyword extraction in this semantic space, the metric is more robust to lexical variation and better reflects the true relevance of the predictions. This provides a deeper understanding of how well each model captures the **meaning** behind the ground truth keywords, beyond surface-level text matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f779851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentence embedding model from the SentenceTransformers family\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer and model to generate contextual embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "def embed_keywords(keywords, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute sentence embeddings for a list of keyword strings.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    keywords : List[str]\n",
    "        A list of keyword strings to encode.\n",
    "    device : str\n",
    "        Device to run the model on ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Normalized embeddings tensor of shape (num_keywords, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Return empty tensor if input list is empty\n",
    "    if not keywords:\n",
    "        return torch.empty(0, encoder.config.hidden_size).to(device)\n",
    "\n",
    "    # Tokenize and prepare inputs for the model\n",
    "    inputs = tokenizer(keywords, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the encoder to get hidden states\n",
    "        outputs = encoder(**inputs)\n",
    "\n",
    "        # Use mean pooling on the last hidden state to get fixed-size embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Normalize embeddings to unit length for cosine similarity computations\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_semantic_metrics(pred_keywords, gt_embeddings, threshold=0.75, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute semantic precision, recall, and F1 score between predicted keywords and\n",
    "    ground truth embeddings based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    pred_keywords : List[str]\n",
    "        List of predicted keywords for a single review.\n",
    "    gt_embeddings : torch.Tensor\n",
    "        Pre-computed normalized embeddings of ground truth keywords.\n",
    "    threshold : float\n",
    "        Cosine similarity threshold above which a predicted keyword is considered\n",
    "        semantically matching a ground truth keyword.\n",
    "    device : str\n",
    "        Device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    precision : float\n",
    "        Fraction of predicted keywords that match any ground truth keyword semantically.\n",
    "    recall : float\n",
    "        Fraction of ground truth keywords that are matched by any predicted keyword.\n",
    "    f1 : float\n",
    "        Harmonic mean of precision and recall.\n",
    "    \"\"\"\n",
    "    # Handle empty predictions or empty ground truth embeddings edge cases\n",
    "    if len(pred_keywords) == 0 or gt_embeddings.shape[0] == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute embeddings for the predicted keywords only\n",
    "    pred_emb = embed_keywords(pred_keywords, device=device)\n",
    "\n",
    "    # Compute cosine similarity matrix between predicted and ground truth embeddings\n",
    "    # Shape: (num_predicted_keywords, num_ground_truth_keywords)\n",
    "    sims = torch.matmul(pred_emb, gt_embeddings.T)\n",
    "\n",
    "    # A predicted keyword is counted as a match if it has cosine similarity above\n",
    "    # the threshold with at least one ground truth keyword\n",
    "    pred_matches = (sims > threshold).any(dim=1).float().sum().item()\n",
    "\n",
    "    # Similarly, a ground truth keyword is matched if any predicted keyword exceeds threshold\n",
    "    gt_matches = (sims > threshold).any(dim=0).float().sum().item()\n",
    "\n",
    "    # Calculate precision: matched predictions / total predictions\n",
    "    precision = pred_matches / len(pred_keywords)\n",
    "\n",
    "    # Calculate recall: matched ground truths / total ground truth keywords\n",
    "    recall = gt_matches / gt_embeddings.shape[0]\n",
    "\n",
    "    # Compute harmonic mean for F1 score, handling zero division\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce9bda",
   "metadata": {},
   "source": [
    "### Semantic Evaluation of Base and Sentiment Models Using Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa566c",
   "metadata": {},
   "source": [
    "In this step, we evaluate the semantic similarity between the predicted keywords of two models â€” **Base** and **Sentiment-enhanced** â€” and the ground truth keywords using **sentence embeddings**.\n",
    "\n",
    "Unlike previous evaluations based on exact or approximate matching, this method leverages contextual embeddings from a pre-trained transformer to measure how semantically close the predicted keywords are to the reference keywords.\n",
    "\n",
    "For each review:\n",
    "- We extract only the **text of the predicted keywords**, ignoring their confidence scores.\n",
    "- We compute **semantic precision, recall, and F1** based on cosine similarity between embeddings.\n",
    "- We then average these metrics across all reviews for each model, providing an overall **semantic performance assessment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbbd11b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Semantic metrics - base: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 63.71it/s]\n",
      "Semantic metrics - sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 60.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_26e35\">\n",
       "  <caption>Semantic-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_26e35_level0_col0\" class=\"col_heading level0 col0\" >Semantic_Precision</th>\n",
       "      <th id=\"T_26e35_level0_col1\" class=\"col_heading level0 col1\" >Semantic_Recall</th>\n",
       "      <th id=\"T_26e35_level0_col2\" class=\"col_heading level0 col2\" >Semantic_F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26e35_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_26e35_row0_col0\" class=\"data row0 col0\" >0.0800</td>\n",
       "      <td id=\"T_26e35_row0_col1\" class=\"data row0 col1\" >0.0006</td>\n",
       "      <td id=\"T_26e35_row0_col2\" class=\"data row0 col2\" >0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26e35_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_26e35_row1_col0\" class=\"data row1 col0\" >0.0400</td>\n",
       "      <td id=\"T_26e35_row1_col1\" class=\"data row1 col1\" >0.0013</td>\n",
       "      <td id=\"T_26e35_row1_col2\" class=\"data row1 col2\" >0.0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x318f286a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute embeddings for the ground truth keywords once per selected movie\n",
    "# This avoids redundant computation when comparing against multiple predicted keywords\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "gt_emb = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# List to collect semantic evaluation results for each model\n",
    "semantic_scores = []\n",
    "\n",
    "# Loop over each model to evaluate semantic metrics separately\n",
    "for model in models_to_evaluate:\n",
    "    # Lists to accumulate precision, recall, and F1 scores for each review\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "\n",
    "    # Iterate over each review (row) in the selected movie's predictions, with a progress bar\n",
    "    for _, row in tqdm(selected_film.iterrows(), total=len(selected_film), desc=f\"Semantic metrics - {model}\"):\n",
    "        pred_col = f\"keywords_{model}\"  # Column name for predicted keywords of the current model\n",
    "\n",
    "        # Check if the predicted keywords column exists and contains a list\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            # Extract only the keyword strings (ignore confidence scores)\n",
    "            pred_kw = [kw for kw, _ in row[pred_col] if isinstance(kw, str)]\n",
    "\n",
    "            # Compute semantic precision, recall, and F1 between predicted keywords and precomputed GT embeddings\n",
    "            precision, recall, f1 = compute_semantic_metrics(pred_kw, gt_emb, device=device)\n",
    "\n",
    "            # Append the scores for aggregation later\n",
    "            all_precisions.append(precision)\n",
    "            all_recalls.append(recall)\n",
    "            all_f1s.append(f1)\n",
    "\n",
    "    # After processing all reviews, calculate average semantic scores for the current model\n",
    "    if all_f1s:\n",
    "        semantic_scores.append({\n",
    "            \"Model\": model,\n",
    "            \"Semantic_Precision\": round(sum(all_precisions) / len(all_precisions), 4),\n",
    "            \"Semantic_Recall\": round(sum(all_recalls) / len(all_recalls), 4),\n",
    "            \"Semantic_F1\": round(sum(all_f1s) / len(all_f1s), 4)\n",
    "        })\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame and set 'Model' as the index\n",
    "summary_df = pd.DataFrame(semantic_scores).set_index(\"Model\")\n",
    "\n",
    "# Display the semantic evaluation summary as a nicely formatted table with 4 decimal places\n",
    "summary_df.style.format(precision=4).set_caption(\"Semantic-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997bd7af",
   "metadata": {},
   "source": [
    "## Sentiment Appropriateness Score (SAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b08c7",
   "metadata": {},
   "source": [
    "To further evaluate the quality of the predicted keywords from a sentiment-aware perspective, we introduce the **Sentiment Appropriateness Score (SAS)**. This metric measures how well the **overall sentiment of the predicted keywords aligns with the sentiment of the ground truth** or the original review text.\n",
    "\n",
    "Traditional evaluation metrics such as precision, recall, and F1-score focus on **syntactic correctness**, but they do not assess whether the predicted keywords **reflect the emotional tone** of the review. SAS addresses this by incorporating sentiment into the evaluation process.\n",
    "\n",
    "We compute SAS in two ways:\n",
    "\n",
    "1. **SAS from Ground Truth Keywords**  \n",
    "   The sentiment of the review is approximated using the set of ground truth keywords. Each keyword is analyzed using **VADER**, and its sentiment is mapped using the following weights:  \n",
    "   - `pos` â†’ 1.0  \n",
    "   - `neu` â†’ 0.5  \n",
    "   - `neg` â†’ 0.0  \n",
    "   The final sentiment is the average of these values. This reference sentiment is compared to the average sentiment of the predicted keywords.\n",
    "\n",
    "2. **SAS from Review Text**  \n",
    "   If the full review text is available, we compute its sentiment using the same weighted VADER scheme and compare it to the sentiment of the predicted keywords.\n",
    "\n",
    "In both cases, SAS is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{SAS} = 1 - |\\text{sentiment}_{\\text{predicted}} - \\text{sentiment}_{\\text{reference}}|\n",
    "$$\n",
    "\n",
    "Values closer to **1** indicate that the predicted keywords are **emotionally aligned** with the ground truth or the review.\n",
    "\n",
    "We use **VADER (Valence Aware Dictionary and sEntiment Reasoner)** because:\n",
    "\n",
    "- It is optimized for **short, informal text** like keywords or tags.  \n",
    "- It works well for **single words and short phrases**, which are the output of our models.  \n",
    "- It provides class-level probabilities (`pos`, `neu`, `neg`) that are directly usable for **interpretable sentiment scoring**.  \n",
    "- It is lightweight and efficient, making it suitable for **large-scale evaluation**.\n",
    "\n",
    "While transformer-based sentiment models may be more powerful for longer text, **VADER offers the best trade-off** between accuracy, interpretability, and scalability for our keyword-level sentiment matching task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75a839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SAS based on ground truth keywords\n",
    "def compute_sas_from_keywords(predicted_keywords, ground_truth_keywords=None, analyzer=None, sentiment_gt=None):\n",
    "    \"\"\"\n",
    "    Computes SAS by comparing the average sentiment of predicted keywords to the sentiment of the ground truth keywords.\n",
    "\n",
    "    You can pass:\n",
    "    - Either the ground_truth_keywords and analyzer (to compute sentiment on the fly)\n",
    "    - Or directly a precomputed sentiment_gt value\n",
    "\n",
    "    Sentiment is computed as: 1.0 * pos + 0.5 * neu + 0.0 * neg\n",
    "\n",
    "    Args:\n",
    "        predicted_keywords (list of dict): each dict has 'sentiment_score' âˆˆ [0,1]\n",
    "        ground_truth_keywords (list of str): optional, used if sentiment_gt not given\n",
    "        analyzer (SentimentIntensityAnalyzer): optional, used if sentiment_gt not given\n",
    "        sentiment_gt (float): optional, precomputed GT sentiment âˆˆ [0,1]\n",
    "\n",
    "    Returns:\n",
    "        float: SAS âˆˆ [0,1] â€” higher is better alignment with GT sentiment\n",
    "    \"\"\"\n",
    "    # Compute GT sentiment only if not precomputed\n",
    "    if sentiment_gt is None:\n",
    "        if not ground_truth_keywords or analyzer is None:\n",
    "            return None\n",
    "        sentiments_gt = []\n",
    "        for kw in ground_truth_keywords:\n",
    "            scores = analyzer.polarity_scores(kw)\n",
    "            sentiment = 1.0 * scores['pos'] + 0.5 * scores['neu'] + 0.0 * scores['neg']\n",
    "            sentiments_gt.append(sentiment)\n",
    "        sentiment_gt = sum(sentiments_gt) / len(sentiments_gt)\n",
    "\n",
    "    # Compute average sentiment of predicted keywords\n",
    "    sentiments_pred = [kw['sentiment_score'] for kw in predicted_keywords]\n",
    "    sentiment_pred = sum(sentiments_pred) / len(sentiments_pred)\n",
    "\n",
    "    # SAS = 1 - absolute difference between predicted and GT sentiment\n",
    "    return 1 - abs(sentiment_pred - sentiment_gt)\n",
    "\n",
    "\n",
    "# Compute SAS based on review text\n",
    "def compute_sas_from_text(predicted_keywords, review_text=None, analyzer=None, sentiment_text=None):\n",
    "    \"\"\"\n",
    "    Computes SAS by comparing the average sentiment of predicted keywords to the sentiment of the full review.\n",
    "\n",
    "    You can pass:\n",
    "    - Either the review_text and analyzer (to compute sentiment on the fly)\n",
    "    - Or directly a precomputed sentiment_text value\n",
    "\n",
    "    Sentiment is computed as: 1.0 * pos + 0.5 * neu + 0.0 * neg\n",
    "\n",
    "    Args:\n",
    "        predicted_keywords (list of dict): each dict has 'sentiment_score' âˆˆ [0,1]\n",
    "        review_text (str): optional, used if sentiment_text not given\n",
    "        analyzer (SentimentIntensityAnalyzer): optional, used if sentiment_text not given\n",
    "        sentiment_text (float): optional, precomputed review sentiment âˆˆ [0,1]\n",
    "\n",
    "    Returns:\n",
    "        float: SAS âˆˆ [0,1] â€” higher is better alignment with review sentiment\n",
    "    \"\"\"\n",
    "    # Compute review sentiment only if not precomputed\n",
    "    if sentiment_text is None:\n",
    "        if not review_text or analyzer is None:\n",
    "            return None\n",
    "        scores = analyzer.polarity_scores(review_text)\n",
    "        sentiment_text = 1.0 * scores['pos'] + 0.5 * scores['neu'] + 0.0 * scores['neg']\n",
    "\n",
    "    # Compute average sentiment of predicted keywords\n",
    "    sentiments_pred = [kw['sentiment_score'] for kw in predicted_keywords]\n",
    "    sentiment_pred = sum(sentiments_pred) / len(sentiments_pred)\n",
    "\n",
    "    # SAS = 1 - absolute difference between predicted and review sentiment\n",
    "    return 1 - abs(sentiment_pred - sentiment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c53932",
   "metadata": {},
   "source": [
    "### Sentiment Appropriateness Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47dae1",
   "metadata": {},
   "source": [
    "This section evaluates the **Sentiment Appropriateness Score (SAS)** for each model (`base` and `sentiment`) using precomputed sentiment references to improve efficiency.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- **Ground truth sentiment** is computed **once per movie** by averaging the sentiment of all annotated keywords, using VADERâ€™s class probabilities mapped as follows:  \n",
    "  - Positive â†’ 1.0  \n",
    "  - Neutral â†’ 0.5  \n",
    "  - Negative â†’ 0.0\n",
    "\n",
    "- For each review:\n",
    "  - Predicted keywords are extracted and formatted as a list of dictionaries with their sentiment scores.\n",
    "  - **SAS from keywords (`SAS_from_keywords`)** is computed by comparing the average sentiment of predicted keywords to the **precomputed ground truth sentiment**.\n",
    "  - **SAS from text (`SAS_from_text`)** is computed by comparing the predicted sentiment to that of the **full preprocessed review**, also using the VADER class probabilities.\n",
    "\n",
    "- SAS is calculated as:  \n",
    "  `SAS = 1 - |predicted_sentiment - reference_sentiment|`\n",
    "\n",
    "- The final output is a table reporting the average `SAS_from_keywords` and `SAS_from_text` for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b0c92f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAS Evaluation - base: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 66.37it/s]\n",
      "SAS Evaluation - sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 70.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_15db6\">\n",
       "  <caption>Sentiment Appropriateness Score (SAS) Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_15db6_level0_col0\" class=\"col_heading level0 col0\" >SAS_from_keywords</th>\n",
       "      <th id=\"T_15db6_level0_col1\" class=\"col_heading level0 col1\" >SAS_from_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_15db6_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_15db6_row0_col0\" class=\"data row0 col0\" >0.9558</td>\n",
       "      <td id=\"T_15db6_row0_col1\" class=\"data row0 col1\" >0.9065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15db6_level0_row1\" class=\"row_heading level0 row1\" >sentiment</th>\n",
       "      <td id=\"T_15db6_row1_col0\" class=\"data row1 col0\" >0.9059</td>\n",
       "      <td id=\"T_15db6_row1_col1\" class=\"data row1 col1\" >0.8075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x318efc070>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize VADER once\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define the models to evaluate\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Collect SAS results for each model\n",
    "sas_scores = []\n",
    "\n",
    "# Precompute GT sentiment (once per movie)\n",
    "gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "sentiments_gt = []\n",
    "for kw in gt_keywords:\n",
    "    scores = analyzer.polarity_scores(kw)\n",
    "    sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "    sentiments_gt.append(sentiment)\n",
    "sentiment_gt = sum(sentiments_gt) / len(sentiments_gt) if sentiments_gt else None\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_to_evaluate:\n",
    "    all_sas_keyword = []\n",
    "    all_sas_text = []\n",
    "\n",
    "    # Iterate over each review\n",
    "    for _, row in tqdm(selected_film.iterrows(), total=len(selected_film), desc=f\"SAS Evaluation - {model}\"):\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_keywords = []\n",
    "            for kw, score in row[pred_col]:\n",
    "                if isinstance(kw, str) and isinstance(score, (float, int)):\n",
    "                    predicted_keywords.append({\"keyword\": kw, \"sentiment_score\": float(score)})\n",
    "\n",
    "            if not predicted_keywords:\n",
    "                continue\n",
    "\n",
    "            # SAS A: use precomputed GT sentiment\n",
    "            sas_kw = compute_sas_from_keywords(predicted_keywords, sentiment_gt=sentiment_gt)\n",
    "            if sas_kw is not None:\n",
    "                all_sas_keyword.append(sas_kw)\n",
    "\n",
    "            # SAS B: precompute sentiment of full review (once per row)\n",
    "            if \"Preprocessed_Review\" in row and isinstance(row[\"Preprocessed_Review\"], str):\n",
    "                review_text = row[\"Preprocessed_Review\"]\n",
    "                scores = analyzer.polarity_scores(review_text)\n",
    "                sentiment_text = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "\n",
    "                sas_text = compute_sas_from_text(predicted_keywords, sentiment_text=sentiment_text)\n",
    "                if sas_text is not None:\n",
    "                    all_sas_text.append(sas_text)\n",
    "\n",
    "    # Average and store results\n",
    "    if all_sas_keyword or all_sas_text:\n",
    "        result = {\"Model\": model}\n",
    "        if all_sas_keyword:\n",
    "            result[\"SAS_from_keywords\"] = round(sum(all_sas_keyword) / len(all_sas_keyword), 4)\n",
    "        if all_sas_text:\n",
    "            result[\"SAS_from_text\"] = round(sum(all_sas_text) / len(all_sas_text), 4)\n",
    "        sas_scores.append(result)\n",
    "\n",
    "# Create DataFrame and display results\n",
    "sas_df = pd.DataFrame(sas_scores).set_index(\"Model\")\n",
    "sas_df.style.format(precision=4).set_caption(\"Sentiment Appropriateness Score (SAS) Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c28df9",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187183",
   "metadata": {},
   "source": [
    "This section automatically processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains predicted keywords generated by different models.\n",
    "\n",
    "For each movie:\n",
    "- The corresponding ground truth keywords are loaded.\n",
    "- Predicted keywords from both models â€” **Base** and **Sentiment-aware** â€” are evaluated.\n",
    "- For each review, the following metrics are computed:\n",
    "\n",
    "  - **Unweighted Metrics**: Precision, Recall, and F1-score based on approximate matching.\n",
    "\n",
    "  - **Score-aware Metrics**: Weighted Precision, Weighted Recall, Weighted F1, and nDCG@5 to evaluate prediction confidence and ranking quality.\n",
    "\n",
    "  - **Semantic Metrics**: Semantic Precision, Semantic Recall, and Semantic F1 computed using cosine similarity between sentence embeddings.\n",
    "\n",
    "  - **Sentiment Alignment Metrics**:  \n",
    "    - `SAS_from_keywords`: alignment between the average sentiment of predicted keywords and that of the ground truth keywords.  \n",
    "    - `SAS_from_text`: alignment between the average sentiment of predicted keywords and that of the full review text.\n",
    "\n",
    "All metrics are averaged per movie and per model, and compiled into a comprehensive summary table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc08dd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_15b9c\">\n",
       "  <caption>Full Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_15b9c_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_15b9c_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_15b9c_level0_col2\" class=\"col_heading level0 col2\" >Avg_Precision</th>\n",
       "      <th id=\"T_15b9c_level0_col3\" class=\"col_heading level0 col3\" >Avg_Recall</th>\n",
       "      <th id=\"T_15b9c_level0_col4\" class=\"col_heading level0 col4\" >Avg_F1</th>\n",
       "      <th id=\"T_15b9c_level0_col5\" class=\"col_heading level0 col5\" >Avg_Weighted_Precision</th>\n",
       "      <th id=\"T_15b9c_level0_col6\" class=\"col_heading level0 col6\" >Avg_Weighted_Recall</th>\n",
       "      <th id=\"T_15b9c_level0_col7\" class=\"col_heading level0 col7\" >Avg_Weighted_F1</th>\n",
       "      <th id=\"T_15b9c_level0_col8\" class=\"col_heading level0 col8\" >Avg_Semantic_Precision</th>\n",
       "      <th id=\"T_15b9c_level0_col9\" class=\"col_heading level0 col9\" >Avg_Semantic_Recall</th>\n",
       "      <th id=\"T_15b9c_level0_col10\" class=\"col_heading level0 col10\" >Avg_Semantic_F1</th>\n",
       "      <th id=\"T_15b9c_level0_col11\" class=\"col_heading level0 col11\" >Avg_SAS_from_keywords</th>\n",
       "      <th id=\"T_15b9c_level0_col12\" class=\"col_heading level0 col12\" >Avg_SAS_from_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_15b9c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_15b9c_row0_col0\" class=\"data row0 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_15b9c_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_15b9c_row0_col2\" class=\"data row0 col2\" >0.0800</td>\n",
       "      <td id=\"T_15b9c_row0_col3\" class=\"data row0 col3\" >0.0014</td>\n",
       "      <td id=\"T_15b9c_row0_col4\" class=\"data row0 col4\" >0.0027</td>\n",
       "      <td id=\"T_15b9c_row0_col5\" class=\"data row0 col5\" >0.0798</td>\n",
       "      <td id=\"T_15b9c_row0_col6\" class=\"data row0 col6\" >0.0008</td>\n",
       "      <td id=\"T_15b9c_row0_col7\" class=\"data row0 col7\" >0.0015</td>\n",
       "      <td id=\"T_15b9c_row0_col8\" class=\"data row0 col8\" >0.5200</td>\n",
       "      <td id=\"T_15b9c_row0_col9\" class=\"data row0 col9\" >0.0524</td>\n",
       "      <td id=\"T_15b9c_row0_col10\" class=\"data row0 col10\" >0.0872</td>\n",
       "      <td id=\"T_15b9c_row0_col11\" class=\"data row0 col11\" >0.9302</td>\n",
       "      <td id=\"T_15b9c_row0_col12\" class=\"data row0 col12\" >0.9140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15b9c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_15b9c_row1_col0\" class=\"data row1 col0\" >LaLaLand</td>\n",
       "      <td id=\"T_15b9c_row1_col1\" class=\"data row1 col1\" >sentiment</td>\n",
       "      <td id=\"T_15b9c_row1_col2\" class=\"data row1 col2\" >0.2000</td>\n",
       "      <td id=\"T_15b9c_row1_col3\" class=\"data row1 col3\" >0.0034</td>\n",
       "      <td id=\"T_15b9c_row1_col4\" class=\"data row1 col4\" >0.0068</td>\n",
       "      <td id=\"T_15b9c_row1_col5\" class=\"data row1 col5\" >0.1987</td>\n",
       "      <td id=\"T_15b9c_row1_col6\" class=\"data row1 col6\" >0.0016</td>\n",
       "      <td id=\"T_15b9c_row1_col7\" class=\"data row1 col7\" >0.0032</td>\n",
       "      <td id=\"T_15b9c_row1_col8\" class=\"data row1 col8\" >0.6000</td>\n",
       "      <td id=\"T_15b9c_row1_col9\" class=\"data row1 col9\" >0.1786</td>\n",
       "      <td id=\"T_15b9c_row1_col10\" class=\"data row1 col10\" >0.2539</td>\n",
       "      <td id=\"T_15b9c_row1_col11\" class=\"data row1 col11\" >0.8672</td>\n",
       "      <td id=\"T_15b9c_row1_col12\" class=\"data row1 col12\" >0.8574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15b9c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_15b9c_row2_col0\" class=\"data row2 col0\" >Parasite</td>\n",
       "      <td id=\"T_15b9c_row2_col1\" class=\"data row2 col1\" >base</td>\n",
       "      <td id=\"T_15b9c_row2_col2\" class=\"data row2 col2\" >0.1600</td>\n",
       "      <td id=\"T_15b9c_row2_col3\" class=\"data row2 col3\" >0.0025</td>\n",
       "      <td id=\"T_15b9c_row2_col4\" class=\"data row2 col4\" >0.0050</td>\n",
       "      <td id=\"T_15b9c_row2_col5\" class=\"data row2 col5\" >0.1564</td>\n",
       "      <td id=\"T_15b9c_row2_col6\" class=\"data row2 col6\" >0.0012</td>\n",
       "      <td id=\"T_15b9c_row2_col7\" class=\"data row2 col7\" >0.0023</td>\n",
       "      <td id=\"T_15b9c_row2_col8\" class=\"data row2 col8\" >0.4400</td>\n",
       "      <td id=\"T_15b9c_row2_col9\" class=\"data row2 col9\" >0.0297</td>\n",
       "      <td id=\"T_15b9c_row2_col10\" class=\"data row2 col10\" >0.0538</td>\n",
       "      <td id=\"T_15b9c_row2_col11\" class=\"data row2 col11\" >0.9558</td>\n",
       "      <td id=\"T_15b9c_row2_col12\" class=\"data row2 col12\" >0.9065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15b9c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_15b9c_row3_col0\" class=\"data row3 col0\" >Parasite</td>\n",
       "      <td id=\"T_15b9c_row3_col1\" class=\"data row3 col1\" >sentiment</td>\n",
       "      <td id=\"T_15b9c_row3_col2\" class=\"data row3 col2\" >0.2800</td>\n",
       "      <td id=\"T_15b9c_row3_col3\" class=\"data row3 col3\" >0.0044</td>\n",
       "      <td id=\"T_15b9c_row3_col4\" class=\"data row3 col4\" >0.0087</td>\n",
       "      <td id=\"T_15b9c_row3_col5\" class=\"data row3 col5\" >0.2800</td>\n",
       "      <td id=\"T_15b9c_row3_col6\" class=\"data row3 col6\" >0.0015</td>\n",
       "      <td id=\"T_15b9c_row3_col7\" class=\"data row3 col7\" >0.0031</td>\n",
       "      <td id=\"T_15b9c_row3_col8\" class=\"data row3 col8\" >0.4800</td>\n",
       "      <td id=\"T_15b9c_row3_col9\" class=\"data row3 col9\" >0.1829</td>\n",
       "      <td id=\"T_15b9c_row3_col10\" class=\"data row3 col10\" >0.2225</td>\n",
       "      <td id=\"T_15b9c_row3_col11\" class=\"data row3 col11\" >0.9059</td>\n",
       "      <td id=\"T_15b9c_row3_col12\" class=\"data row3 col12\" >0.8075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x171d16670>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load the ground truth once\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "models_to_evaluate = [\"base\", \"sentiment\"]\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Store results across all movies\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all .pkl keyword prediction files\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords and Movie_ID\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Ground truth keywords\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "            gt_embeddings = embed_keywords(gt_keywords, device=device)\n",
    "\n",
    "            # --- Precompute GT sentiment ---\n",
    "            sentiments_gt = []\n",
    "            for kw in gt_keywords:\n",
    "                scores = analyzer.polarity_scores(kw)\n",
    "                sentiment = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "                sentiments_gt.append(sentiment)\n",
    "            sentiment_gt = sum(sentiments_gt) / len(sentiments_gt) if sentiments_gt else None\n",
    "\n",
    "            # Init model-specific results\n",
    "            results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            # Evaluate each review\n",
    "            for _, row in selected_film.iterrows():\n",
    "                # Precompute review text sentiment if available\n",
    "                sentiment_text = None\n",
    "                if \"Preprocessed_Review\" in row and isinstance(row[\"Preprocessed_Review\"], str):\n",
    "                    scores = analyzer.polarity_scores(row[\"Preprocessed_Review\"])\n",
    "                    sentiment_text = 1.0 * scores[\"pos\"] + 0.5 * scores[\"neu\"] + 0.0 * scores[\"neg\"]\n",
    "\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "                    if pred_col not in row or not isinstance(row[pred_col], list):\n",
    "                        continue\n",
    "\n",
    "                    predicted_kw_score = row[pred_col]\n",
    "                    pred_kw_only = [kw for kw, _ in predicted_kw_score if isinstance(kw, str)]\n",
    "                    predicted_kw_structured = [\n",
    "                        {\"keyword\": kw, \"sentiment_score\": float(score)}\n",
    "                        for kw, score in predicted_kw_score\n",
    "                        if isinstance(kw, str) and isinstance(score, (float, int))\n",
    "                    ]\n",
    "\n",
    "                    if not pred_kw_only or not predicted_kw_structured:\n",
    "                        continue\n",
    "\n",
    "                    # Compute classic metrics\n",
    "                    precision, recall, f1 = evaluate_keywords(pred_kw_only, gt_keywords)\n",
    "                    w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, gt_keywords)\n",
    "\n",
    "                    # Compute semantic metrics\n",
    "                    semantic_precision, semantic_recall, semantic_f1 = compute_semantic_metrics(\n",
    "                        pred_kw_only, gt_embeddings, threshold=0.5, device=device\n",
    "                    )\n",
    "\n",
    "                    # SAS metrics using precomputed values\n",
    "                    sas_kw = compute_sas_from_keywords(predicted_kw_structured, sentiment_gt=sentiment_gt)\n",
    "                    sas_txt = compute_sas_from_text(predicted_kw_structured, sentiment_text=sentiment_text) if sentiment_text is not None else None\n",
    "\n",
    "                    # Store metrics\n",
    "                    results[model].append({\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1,\n",
    "                        \"w_precision\": w_precision,\n",
    "                        \"w_recall\": w_recall,\n",
    "                        \"w_f1\": w_f1,\n",
    "                        \"semantic_precision\": semantic_precision,\n",
    "                        \"semantic_recall\": semantic_recall,\n",
    "                        \"semantic_f1\": semantic_f1,\n",
    "                        \"sas_keywords\": sas_kw,\n",
    "                        \"sas_text\": sas_txt\n",
    "                    })\n",
    "\n",
    "            # Average results per model\n",
    "            for model in models_to_evaluate:\n",
    "                if results[model]:\n",
    "                    metrics_df = pd.DataFrame(results[model])\n",
    "                    avg_metrics = {\n",
    "                        \"Movie\": movie_name,\n",
    "                        \"Model\": model,\n",
    "                        \"Avg_Precision\": round(metrics_df[\"precision\"].mean(), 4),\n",
    "                        \"Avg_Recall\": round(metrics_df[\"recall\"].mean(), 4),\n",
    "                        \"Avg_F1\": round(metrics_df[\"f1\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Precision\": round(metrics_df[\"w_precision\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Recall\": round(metrics_df[\"w_recall\"].mean(), 4),\n",
    "                        \"Avg_Weighted_F1\": round(metrics_df[\"w_f1\"].mean(), 4),\n",
    "                        \"Avg_Semantic_Precision\": round(metrics_df[\"semantic_precision\"].mean(), 4),\n",
    "                        \"Avg_Semantic_Recall\": round(metrics_df[\"semantic_recall\"].mean(), 4),\n",
    "                        \"Avg_Semantic_F1\": round(metrics_df[\"semantic_f1\"].mean(), 4),\n",
    "                    }\n",
    "\n",
    "                    if \"sas_keywords\" in metrics_df:\n",
    "                        avg_metrics[\"Avg_SAS_from_keywords\"] = round(metrics_df[\"sas_keywords\"].mean(), 4)\n",
    "                    if \"sas_text\" in metrics_df and metrics_df[\"sas_text\"].notna().any():\n",
    "                        avg_metrics[\"Avg_SAS_from_text\"] = round(metrics_df[\"sas_text\"].dropna().mean(), 4)\n",
    "\n",
    "                    all_results.append(avg_metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "\n",
    "# Display final summary\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Full Evaluation Summary per Movie and Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
