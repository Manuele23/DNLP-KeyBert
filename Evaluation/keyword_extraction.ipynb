{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686bacc8",
   "metadata": {},
   "source": [
    "# Movie Reviews Keyword Extraction with KeyBERT Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b0d91",
   "metadata": {},
   "source": [
    "This notebook focuses on **extracting representative keywords** from movie reviews using **KeyBERT** and its extended variants. The goal is to generate concise, meaningful keyword sets for each review that can later be used for evaluation.\n",
    "\n",
    "## Models Used\n",
    "We compare and apply the following keyword extraction models:\n",
    "- **KeyBERT (base)**: Extracts keywords based on semantic similarity using BERT embeddings.\n",
    "- **KeyBERT + Sentiment Reranker**: Reranks keywords based on their sentiment alignment with the review.\n",
    "- **KeyBERT + Sentiment-Aware Selection**: Integrates sentiment in the candidate selection phase using a continuous sentiment model.\n",
    "- **KeyBERT + Metadata**: Enriches document and candidate embeddings using review-level metadata (utility, length, polarity, recency).\n",
    "\n",
    "## Workflow\n",
    "1. **Select a movie** from the dataset (`.pkl` files).\n",
    "2. **Load and run all models** to extract the top keywords for each review.\n",
    "3. **Save the output** to a new `.pkl` file containing all extracted keyword columns.\n",
    "4. **(Optional)**: Load and inspect the saved file to ensure correctness.\n",
    "\n",
    "> This setup allows us to perform a comparative analysis of keyword extraction techniques with a focus on enhancing semantic quality through additional signals like sentiment and metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de38a3",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7725851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "tqdm is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keybert is already installed.\n",
      "Installing sentence-transformers...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\", \"tqdm\", \"keybert\", \"sentence-transformers\",\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cd75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the custom module path for KeyBERTSentimentAware\n",
    "sys.path.append(\"../KeyBERTSentimentAware\")\n",
    "\n",
    "# Add the custom module path for KeyBERTMetadata\n",
    "sys.path.append(\"../KeyBERTMetadata\")    \n",
    "\n",
    "# Import custom extension of KeyBERT that integrates sentiment awareness in keyword scoring\n",
    "from models.KeyBertSentimentAware import KeyBERTSentimentAware  # type: ignore\n",
    "\n",
    "# Import custom reranker that uses sentiment polarity after keyword extraction to re-score them\n",
    "from models.KeyBertSentimentReranker import KeyBERTSentimentReranker  # type: ignore\n",
    "\n",
    "# Import custom extension of KeyBERT that enriches embeddings with metadata\n",
    "from KeyBertMetadata import KeyBERTMetadata  # type: ignore\n",
    "\n",
    "# Import the original KeyBERT model for semantic-based keyword extraction\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# Import the SentenceTransformer class from the sentence-transformers library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import os for file path operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed335e6",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset\n",
    "\n",
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension).\n",
    "- These names can then be used to dynamically select a movie for keyword extraction.\n",
    "\n",
    "> This allows flexible selection and processing of any movie in the dataset without hardcoding paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9031f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50dab1",
   "metadata": {},
   "source": [
    "## Select and Load a Specific Movie\n",
    "\n",
    "In this step, we manually select one of the available movies listed earlier.\n",
    "\n",
    "- Set the `movie_name` variable to one of the printed movie titles.\n",
    "- The script constructs the full file path and loads the corresponding `.pkl` file using `pandas`.\n",
    "- It then displays the number of reviews loaded for that movie.\n",
    "\n",
    "> This forms the input dataset for keyword extraction using various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44901c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Parasite with 5 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Choose the movie (manually change this)\n",
    "movie_name = \"Parasite\"  # Choose from printed list\n",
    "\n",
    "# Load the selected movie\n",
    "movie_path = os.path.join(root_dir, f\"{movie_name}.pkl\")\n",
    "selected_film = pd.read_pickle(movie_path)\n",
    "\n",
    "selected_film = selected_film.head(10)\n",
    "\n",
    "print(f\"Loaded {movie_name} with {len(selected_film)} reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626a5bb",
   "metadata": {},
   "source": [
    "## Keyword Extraction with Multiple Models\n",
    "\n",
    "In this section, we perform keyword extraction from movie reviews using four different models:\n",
    "\n",
    "- `base`: The standard KeyBERT model using semantic similarity.\n",
    "- `reranker`: A version that re-ranks extracted keywords using post-hoc sentiment alignment.\n",
    "- `sentiment`: A sentiment-aware model that incorporates sentiment into keyword selection during the extraction process.\n",
    "- `metadata`: A custom model that leverages review metadata to improve keyword selection, using a batch embedding strategy.\n",
    "\n",
    "### Process Overview:\n",
    "1. Metadata for all reviews is extracted once.\n",
    "2. Each model is applied to the `Preprocessed_Review` text of each review.\n",
    "3. For the `metadata` model, batch embeddings are computed for efficiency.\n",
    "4. Extracted keywords are stored (only the keyword strings, scores are removed).\n",
    "5. The results are stored in a new DataFrame `keywords_df` with the following columns:\n",
    "   - `Movie_ID`\n",
    "   - `Review_Text`\n",
    "   - `keywords_base`\n",
    "   - `keywords_reranker`\n",
    "   - `keywords_sentiment`\n",
    "   - `keywords_metadata`\n",
    "\n",
    "This DataFrame will later be saved and evaluated to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f848a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence embedding model to be used\n",
    "model_name = \"all-MiniLM-L6-v2\"  # A compact and fast transformer model from SentenceTransformers\n",
    "embedding_model = SentenceTransformer(model_name)  # Load the model to generate sentence embeddings\n",
    "\n",
    "# Initialize the keyword extraction models\n",
    "models = {\n",
    "    \"base\": KeyBERT(embedding_model),  # Standard KeyBERT model using only semantic similarity\n",
    "    \"reranker\": KeyBERTSentimentReranker(embedding_model),  # KeyBERT variant that reranks keywords based on sentiment alignment\n",
    "    \"sentiment\": KeyBERTSentimentAware(embedding_model),  # KeyBERT variant integrating sentiment in the candidate selection phase\n",
    "    \"metadata\": KeyBERTMetadata(embedding_model),  # KeyBERT variant that incorporates external metadata for keyword extraction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f5d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords with base: 100%|██████████| 5/5 [00:02<00:00,  1.79it/s]\n",
      "Extracting keywords with reranker: 100%|██████████| 5/5 [00:05<00:00,  1.04s/it]\n",
      "Extracting keywords with sentiment: 100%|██████████| 5/5 [00:17<00:00,  3.51s/it]\n",
      "Extracting keywords with metadata: 100%|██████████| 5/5 [00:00<00:00, 93.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata once for the entire dataset\n",
    "metadata = KeyBERTMetadata.extract_metadata(selected_film)\n",
    "\n",
    "# Define the n-gram range for keyword candidates\n",
    "keyphrase_ngram_range = (1, 2)  # Unigrams and bigrams\n",
    "top_n = 5  # Number of top keywords to extract\n",
    "\n",
    "# Prepare a dictionary to collect results from all models\n",
    "keyword_results = {\n",
    "    \"Movie_ID\": selected_film[\"Movie_ID\"].tolist(),\n",
    "    \"Review_ID\": selected_film[\"Review_ID\"].tolist(),\n",
    "    \"Review_Text\": selected_film[\"Review_Text\"].tolist()\n",
    "}\n",
    "\n",
    "# Iterate through each keyword extraction model\n",
    "for model_name, model in models.items():\n",
    "    tqdm.pandas(desc=f\"Extracting keywords with {model_name}\")\n",
    "\n",
    "    # Metadata-aware model requires batch embedding\n",
    "    if model_name == \"metadata\":\n",
    "        try:\n",
    "            # Compute document and candidate embeddings using metadata\n",
    "            doc_emb, word_emb = model.extract_embeddings_mean(\n",
    "                docs=list(selected_film[\"Preprocessed_Review\"]),\n",
    "                metadata=metadata,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "            )\n",
    "\n",
    "            keyword_results[f\"keywords_{model_name}\"] = selected_film[\"Preprocessed_Review\"].progress_apply(\n",
    "                lambda _: model.extract_keywords(\n",
    "                    docs = list(selected_film[\"Preprocessed_Review\"]),\n",
    "                    doc_embeddings=doc_emb,\n",
    "                    word_embeddings=word_emb,\n",
    "                    keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "                    top_n=top_n\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Convert results to a list of tuples (keyword, score)\n",
    "            keyword_results[f\"keywords_{model_name}\"] = keyword_results[f\"keywords_{model_name}\"].apply(\n",
    "                lambda x: [kw for kw_group in x for kw in kw_group]\n",
    "            ).tolist()\n",
    "\n",
    "        \n",
    "        # Handle any exceptions that may occur during batch processing\n",
    "        except Exception as e:\n",
    "            print(f\"Batch error in metadata model: {e}\")\n",
    "            keyword_results[f\"keywords_{model_name}\"] = [[] for _ in range(len(selected_film))]\n",
    "\n",
    "    # All other models use per-review keyword extraction\n",
    "    else:\n",
    "        keyword_results[f\"keywords_{model_name}\"] = selected_film[\"Preprocessed_Review\"].progress_apply(\n",
    "            lambda text: [(kw[0], kw[1]) for kw in model.extract_keywords(\n",
    "                text,\n",
    "                top_n=top_n,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range\n",
    "            )]\n",
    "        ).tolist()\n",
    "\n",
    "# Create final DataFrame with keywords from all models\n",
    "keywords_df = pd.DataFrame(keyword_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801c753",
   "metadata": {},
   "source": [
    "## Save Extracted Keywords to File\n",
    "\n",
    "After extracting the keywords for each review using all models, we save the results for future evaluation or analysis.\n",
    "\n",
    "### What this cell does:\n",
    "1. **Extracts the movie name** from the original `.pkl` file path.\n",
    "2. **Defines an output path** with the prefix `kw_` (e.g., `kw_Parasite.pkl`) inside the `../Dataset/Extracted_Keywords` directory.\n",
    "3. **Ensures the output directory exists**, creating it if necessary.\n",
    "4. **Saves the `keywords_df`** (containing Movie ID, original text, and all extracted keyword columns) as a pickle file.\n",
    "\n",
    "This allows us to reuse extracted keywords without re-running the extraction pipeline.\n",
    "\n",
    "When complete, a message confirms the save location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47eb3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved keywords for 'Parasite' to: ../Dataset/Extracted_Keywords/kw_Parasite.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract movie name from the original file path\n",
    "movie_name = os.path.splitext(os.path.basename(movie_path))[0]\n",
    "\n",
    "# Define output path with prefix 'kw_'\n",
    "output_dir = \"../Dataset/Extracted_Keywords\"\n",
    "output_path = os.path.join(output_dir, f\"kw_{movie_name}.pkl\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a pickle file\n",
    "keywords_df.to_pickle(output_path)\n",
    "\n",
    "print(f\"Saved keywords for '{movie_name}' to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086a4ac",
   "metadata": {},
   "source": [
    "## Load Extracted Keywords from File\n",
    "\n",
    "This cell verifies that the extracted keywords for the selected movie were correctly saved and can be successfully reloaded for further analysis or evaluation.\n",
    "\n",
    "### What this cell does:\n",
    "1. **Builds the input file path** using the `movie_name` (e.g., `kw_Parasite.pkl`).\n",
    "2. **Attempts to load the DataFrame** using `pandas.read_pickle()`.\n",
    "3. **Handles errors gracefully**, printing a clear message if the file is not found or any other issue occurs.\n",
    "4. **Displays the first few rows** of the loaded DataFrame to confirm its content.\n",
    "\n",
    "Use this to ensure that the extraction pipeline completed correctly and the output is ready for use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8f3ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded file: ../Dataset/Extracted_Keywords/kw_Parasite.pkl\n",
      "\n",
      "DataFrame shape: (5, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review_ID</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>keywords_base</th>\n",
       "      <th>keywords_reranker</th>\n",
       "      <th>keywords_sentiment</th>\n",
       "      <th>keywords_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt6751668</td>\n",
       "      <td>9637661</td>\n",
       "      <td>I'm genuinely baffled this film won not only b...</td>\n",
       "      <td>[(korean culture, 0.5476), (seeing korean, 0.5...</td>\n",
       "      <td>[(korean culture, 0.4473), (korean, 0.4152), (...</td>\n",
       "      <td>[(tried hard, 0.4114), (films, 0.3814), (break...</td>\n",
       "      <td>[(korean culture, 0.6205), (seeing korean, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt6751668</td>\n",
       "      <td>5510542</td>\n",
       "      <td>Just watch it. It has everything; entertainmen...</td>\n",
       "      <td>[(suspense drama, 0.4686), (drama tragedy, 0.4...</td>\n",
       "      <td>[(movie messages, 0.3052), (shown metaphorical...</td>\n",
       "      <td>[(watch entertainment, 0.3874), (movie, 0.3463...</td>\n",
       "      <td>[(korean culture, 0.6205), (seeing korean, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt6751668</td>\n",
       "      <td>5182892</td>\n",
       "      <td>First Hit: I really enjoyed this story as it d...</td>\n",
       "      <td>[(korean family, 0.6138), (family kim, 0.539),...</td>\n",
       "      <td>[(korean family, 0.4633), (family kim, 0.4168)...</td>\n",
       "      <td>[(ki woo, 0.4314), (perfect, 0.4024), (woo, 0....</td>\n",
       "      <td>[(korean culture, 0.6205), (seeing korean, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt6751668</td>\n",
       "      <td>5499682</td>\n",
       "      <td>I was not expecting that much of this movie. N...</td>\n",
       "      <td>[(expecting movie, 0.5192), (expect movie, 0.4...</td>\n",
       "      <td>[(expect movie, 0.691), (expecting movie, 0.44...</td>\n",
       "      <td>[(original oscar, 0.4264), (oscar deserved, 0....</td>\n",
       "      <td>[(korean culture, 0.6205), (seeing korean, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt6751668</td>\n",
       "      <td>6094155</td>\n",
       "      <td>Good acting, cinematography, twists and screen...</td>\n",
       "      <td>[(screenplay liked, 0.5552), (good acting, 0.5...</td>\n",
       "      <td>[(good acting, 0.6689), (screenplay liked, 0.5...</td>\n",
       "      <td>[(liked location, 0.611), (really good, 0.6048...</td>\n",
       "      <td>[(korean culture, 0.6205), (seeing korean, 0.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Movie_ID Review_ID                                        Review_Text  \\\n",
       "0  tt6751668   9637661  I'm genuinely baffled this film won not only b...   \n",
       "1  tt6751668   5510542  Just watch it. It has everything; entertainmen...   \n",
       "2  tt6751668   5182892  First Hit: I really enjoyed this story as it d...   \n",
       "3  tt6751668   5499682  I was not expecting that much of this movie. N...   \n",
       "4  tt6751668   6094155  Good acting, cinematography, twists and screen...   \n",
       "\n",
       "                                       keywords_base  \\\n",
       "0  [(korean culture, 0.5476), (seeing korean, 0.5...   \n",
       "1  [(suspense drama, 0.4686), (drama tragedy, 0.4...   \n",
       "2  [(korean family, 0.6138), (family kim, 0.539),...   \n",
       "3  [(expecting movie, 0.5192), (expect movie, 0.4...   \n",
       "4  [(screenplay liked, 0.5552), (good acting, 0.5...   \n",
       "\n",
       "                                   keywords_reranker  \\\n",
       "0  [(korean culture, 0.4473), (korean, 0.4152), (...   \n",
       "1  [(movie messages, 0.3052), (shown metaphorical...   \n",
       "2  [(korean family, 0.4633), (family kim, 0.4168)...   \n",
       "3  [(expect movie, 0.691), (expecting movie, 0.44...   \n",
       "4  [(good acting, 0.6689), (screenplay liked, 0.5...   \n",
       "\n",
       "                                  keywords_sentiment  \\\n",
       "0  [(tried hard, 0.4114), (films, 0.3814), (break...   \n",
       "1  [(watch entertainment, 0.3874), (movie, 0.3463...   \n",
       "2  [(ki woo, 0.4314), (perfect, 0.4024), (woo, 0....   \n",
       "3  [(original oscar, 0.4264), (oscar deserved, 0....   \n",
       "4  [(liked location, 0.611), (really good, 0.6048...   \n",
       "\n",
       "                                   keywords_metadata  \n",
       "0  [(korean culture, 0.6205), (seeing korean, 0.5...  \n",
       "1  [(korean culture, 0.6205), (seeing korean, 0.5...  \n",
       "2  [(korean culture, 0.6205), (seeing korean, 0.5...  \n",
       "3  [(korean culture, 0.6205), (seeing korean, 0.5...  \n",
       "4  [(korean culture, 0.6205), (seeing korean, 0.5...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the input path for the keywords DataFrame\n",
    "input_path = os.path.join(\"../Dataset/Extracted_Keywords\", f\"kw_{movie_name}.pkl\")\n",
    "\n",
    "# Load the DataFrame\n",
    "try:\n",
    "    loaded_df = pd.read_pickle(input_path)\n",
    "    print(f\"Successfully loaded file: {input_path}\\n\")\n",
    "    print(f\"DataFrame shape: {loaded_df.shape}\")\n",
    "    display(loaded_df.head())  # Show the first few rows if in Jupyter\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {input_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
