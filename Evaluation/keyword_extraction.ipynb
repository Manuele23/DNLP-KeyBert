{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686bacc8",
   "metadata": {},
   "source": [
    "# Movie Reviews Keyword Extraction with KeyBERT Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b0d91",
   "metadata": {},
   "source": [
    "This notebook focuses on **extracting representative keywords** from movie reviews using **KeyBERT** and its extended variants. The goal is to generate concise, meaningful keyword sets for each review that can later be used for evaluation.\n",
    "\n",
    "## Models Used\n",
    "We compare and apply the following keyword extraction models:\n",
    "- **KeyBERT (base)**: Extracts keywords based on semantic similarity using BERT embeddings.\n",
    "- **KeyBERT + Sentiment Reranker**: Reranks keywords based on their sentiment alignment with the review.\n",
    "- **KeyBERT + Sentiment-Aware Selection**: Integrates sentiment in the candidate selection phase using a continuous sentiment model.\n",
    "- **KeyBERT + Metadata**: Enriches document and candidate embeddings using review-level metadata (utility, length, polarity, recency).\n",
    "\n",
    "## Workflow\n",
    "1. **Select a movie** from the dataset (`.pkl` files).\n",
    "2. **Load and run all models** to extract the top keywords for each review.\n",
    "3. **Save the output** to a new `.pkl` file containing all extracted keyword columns.\n",
    "4. **(Optional)**: Load and inspect the saved file to ensure correctness.\n",
    "\n",
    "> This setup allows us to perform a comparative analysis of keyword extraction techniques with a focus on enhancing semantic quality through additional signals like sentiment and metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de38a3",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7725851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "tqdm is already installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/manuelemustari/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keybert is already installed.\n",
      "Installing sentence-transformers...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.67.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/manuelemustari/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\", \"tqdm\", \"keybert\", \"sentence-transformers\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cd75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the custom module path for KeyBERTSentimentAware\n",
    "sys.path.append(\"../KeyBERTSentimentAware\")\n",
    "\n",
    "# Add the custom module path for KeyBERTMetadata\n",
    "sys.path.append(\"../KeyBERTMetadata\")    \n",
    "\n",
    "# Import custom extension of KeyBERT that integrates sentiment awareness in keyword scoring\n",
    "from models.KeyBertSentimentAware import KeyBERTSentimentAware  # type: ignore\n",
    "\n",
    "# Import custom reranker that uses sentiment polarity after keyword extraction to re-score them\n",
    "from models.KeyBertSentimentReranker import KeyBERTSentimentReranker  # type: ignore\n",
    "\n",
    "# Import custom extension of KeyBERT that enriches embeddings with metadata\n",
    "from KeyBertMetadata import KeyBERTMetadata  # type: ignore\n",
    "\n",
    "# Import the original KeyBERT model for semantic-based keyword extraction\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# Import the SentenceTransformer class from the sentence-transformers library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import os for file path operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed335e6",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset\n",
    "\n",
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension).\n",
    "- These names can then be used to dynamically select a movie for keyword extraction.\n",
    "\n",
    "> This allows flexible selection and processing of any movie in the dataset without hardcoding paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9031f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50dab1",
   "metadata": {},
   "source": [
    "## Select and Load a Specific Movie\n",
    "\n",
    "In this step, we manually select one of the available movies listed earlier.\n",
    "\n",
    "- Set the `movie_name` variable to one of the printed movie titles.\n",
    "- The script constructs the full file path and loads the corresponding `.pkl` file using `pandas`.\n",
    "- It then displays the number of reviews loaded for that movie.\n",
    "\n",
    "> This forms the input dataset for keyword extraction using various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44901c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LaLaLand with 5 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Choose the movie (manually change this)\n",
    "movie_name = \"LaLaLand\"  # Choose from printed list\n",
    "\n",
    "# Load the selected movie\n",
    "movie_path = os.path.join(root_dir, f\"{movie_name}.pkl\")\n",
    "selected_film = pd.read_pickle(movie_path)\n",
    "\n",
    "selected_film = selected_film.head(5)\n",
    "\n",
    "print(f\"Loaded {movie_name} with {len(selected_film)} reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626a5bb",
   "metadata": {},
   "source": [
    "## Keyword Extraction with Multiple Models\n",
    "\n",
    "In this section, we perform keyword extraction from movie reviews using four different models:\n",
    "\n",
    "- `base`: The standard KeyBERT model using semantic similarity.\n",
    "- `reranker`: A version that re-ranks extracted keywords using post-hoc sentiment alignment.\n",
    "- `sentiment`: A sentiment-aware model that incorporates sentiment into keyword selection during the extraction process.\n",
    "- `metadata`: A custom model that leverages review metadata to improve keyword selection, using a batch embedding strategy.\n",
    "\n",
    "### Process Overview:\n",
    "1. Metadata for all reviews is extracted once.\n",
    "2. Each model is applied to the `Preprocessed_Review` text of each review.\n",
    "3. For the `metadata` model, batch embeddings are computed for efficiency.\n",
    "4. Extracted keywords are stored (only the keyword strings, scores are removed).\n",
    "5. The results are stored in a new DataFrame `keywords_df` with the following columns:\n",
    "   - `Movie_ID`\n",
    "   - `Review_Text`\n",
    "   - `keywords_base`\n",
    "   - `keywords_reranker`\n",
    "   - `keywords_sentiment`\n",
    "   - `keywords_metadata`\n",
    "\n",
    "This DataFrame will later be saved and evaluated to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f848a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence embedding model to be used\n",
    "model_name = \"all-MiniLM-L6-v2\"  # A compact and fast transformer model from SentenceTransformers\n",
    "embedding_model = SentenceTransformer(model_name)  # Load the model to generate sentence embeddings\n",
    "\n",
    "# Initialize the keyword extraction models\n",
    "models = {\n",
    "    \"base\": KeyBERT(embedding_model),  # Standard KeyBERT model using only semantic similarity\n",
    "    \"reranker\": KeyBERTSentimentReranker(embedding_model),  # KeyBERT variant that reranks keywords based on sentiment alignment\n",
    "    \"sentiment\": KeyBERTSentimentAware(embedding_model),  # KeyBERT variant integrating sentiment in the candidate selection phase\n",
    "    \"metadata\": KeyBERTMetadata(embedding_model),  # KeyBERT variant that incorporates external metadata for keyword extraction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f5d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords with base: 100%|██████████| 5/5 [00:02<00:00,  1.71it/s]\n",
      "Extracting keywords with reranker: 100%|██████████| 5/5 [00:06<00:00,  1.25s/it]\n",
      "Extracting keywords with sentiment: 100%|██████████| 5/5 [00:20<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting keywords with metadata...\n",
      "Finished extracting keywords with metadata.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract metadata once for the entire dataset\n",
    "metadata = KeyBERTMetadata.extract_metadata(selected_film)\n",
    "\n",
    "# Define the n-gram range for keyword candidates\n",
    "keyphrase_ngram_range = (1, 2)  # Unigrams and bigrams\n",
    "top_n = 5  # Number of top keywords to extract\n",
    "\n",
    "# Prepare a dictionary to collect results from all models\n",
    "keyword_results = {\n",
    "    \"Movie_ID\": selected_film[\"Movie_ID\"].tolist(),\n",
    "    \"Review_ID\": selected_film[\"Review_ID\"].tolist(),\n",
    "    \"Preprocessed_Review\": selected_film[\"Preprocessed_Review\"].tolist()\n",
    "}\n",
    "\n",
    "# Iterate through each keyword extraction model\n",
    "for model_name, model in models.items():\n",
    "    tqdm.pandas(desc=f\"Extracting keywords with {model_name}\")\n",
    "\n",
    "    if model_name == \"metadata\":\n",
    "        print(\"Extracting keywords with metadata...\")\n",
    "        try:\n",
    "            # Compute document and candidate embeddings using metadata\n",
    "            doc_emb, word_emb = model.extract_embeddings_mean(\n",
    "                docs=list(selected_film[\"Preprocessed_Review\"]),\n",
    "                metadata=metadata,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "            )\n",
    "\n",
    "            # Batch keyword extraction\n",
    "            keywords_all = model.extract_keywords(\n",
    "                docs=list(selected_film[\"Preprocessed_Review\"]),\n",
    "                doc_embeddings=doc_emb,\n",
    "                word_embeddings=word_emb,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "                top_n=top_n\n",
    "            )\n",
    "\n",
    "            # Each element is a list of (keyword, score), already usable\n",
    "            keyword_results[f\"keywords_{model_name}\"] = keywords_all\n",
    "            print(\"Finished extracting keywords with metadata.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Batch error in metadata model: {e}\")\n",
    "            keyword_results[f\"keywords_{model_name}\"] = [[] for _ in range(len(selected_film))]\n",
    "\n",
    "    else:\n",
    "        keyword_results[f\"keywords_{model_name}\"] = selected_film[\"Preprocessed_Review\"].progress_apply(\n",
    "            lambda text: [(kw[0], kw[1]) for kw in model.extract_keywords(\n",
    "                text,\n",
    "                top_n=top_n,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range\n",
    "            )]\n",
    "        ).tolist()\n",
    "\n",
    "# Create final DataFrame with keywords from all models\n",
    "keywords_df = pd.DataFrame(keyword_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801c753",
   "metadata": {},
   "source": [
    "## Save Extracted Keywords to File\n",
    "\n",
    "After extracting the keywords for each review using all models, we save the results for future evaluation or analysis.\n",
    "\n",
    "### What this cell does:\n",
    "1. **Extracts the movie name** from the original `.pkl` file path.\n",
    "2. **Defines an output path** with the prefix `kw_` (e.g., `kw_Parasite.pkl`) inside the `../Dataset/Extracted_Keywords` directory.\n",
    "3. **Ensures the output directory exists**, creating it if necessary.\n",
    "4. **Saves the `keywords_df`** (containing Movie ID, original text, and all extracted keyword columns) as a pickle file.\n",
    "\n",
    "This allows us to reuse extracted keywords without re-running the extraction pipeline.\n",
    "\n",
    "When complete, a message confirms the save location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47eb3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved keywords for 'LaLaLand' to: ../Dataset/Extracted_Keywords/kw_LaLaLand.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract movie name from the original file path\n",
    "movie_name = os.path.splitext(os.path.basename(movie_path))[0]\n",
    "\n",
    "# Define output path with prefix 'kw_'\n",
    "output_dir = \"../Dataset/Extracted_Keywords\"\n",
    "output_path = os.path.join(output_dir, f\"kw_{movie_name}.pkl\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a pickle file\n",
    "keywords_df.to_pickle(output_path)\n",
    "\n",
    "print(f\"Saved keywords for '{movie_name}' to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086a4ac",
   "metadata": {},
   "source": [
    "## Load Extracted Keywords from File\n",
    "\n",
    "This cell verifies that the extracted keywords for the selected movie were correctly saved and can be successfully reloaded for further analysis or evaluation.\n",
    "\n",
    "### What this cell does:\n",
    "1. **Builds the input file path** using the `movie_name` (e.g., `kw_Parasite.pkl`).\n",
    "2. **Attempts to load the DataFrame** using `pandas.read_pickle()`.\n",
    "3. **Handles errors gracefully**, printing a clear message if the file is not found or any other issue occurs.\n",
    "4. **Displays the first few rows** of the loaded DataFrame to confirm its content.\n",
    "\n",
    "Use this to ensure that the extraction pipeline completed correctly and the output is ready for use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8f3ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded file: ../Dataset/Extracted_Keywords/kw_LaLaLand.pkl\n",
      "\n",
      "DataFrame shape: (5, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_ID</th>\n",
       "      <th>Review_ID</th>\n",
       "      <th>Preprocessed_Review</th>\n",
       "      <th>keywords_base</th>\n",
       "      <th>keywords_reranker</th>\n",
       "      <th>keywords_sentiment</th>\n",
       "      <th>keywords_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt3783958</td>\n",
       "      <td>4027807</td>\n",
       "      <td>I simply cannot understand why this film rated...</td>\n",
       "      <td>[(film rated, 0.5212), (film musicals, 0.4781)...</td>\n",
       "      <td>[(film rated, 0.3777), (stone acting, 0.2705),...</td>\n",
       "      <td>[(rating, 0.3109), (gosling, 0.2734), (problem...</td>\n",
       "      <td>[(film rated, 0.6326), (film musicals, 0.5996)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt3783958</td>\n",
       "      <td>3709335</td>\n",
       "      <td>I love movie musicals. They're hard to find th...</td>\n",
       "      <td>[(movie musicals, 0.6412), (movie musical, 0.6...</td>\n",
       "      <td>[(movie musical, 0.7473), (movie musicals, 0.7...</td>\n",
       "      <td>[(singing dancing, 0.6298), (musicals, 0.6156)...</td>\n",
       "      <td>[(movie musicals, 0.6979), (movie musical, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt3783958</td>\n",
       "      <td>6633178</td>\n",
       "      <td>Let's just say during the approximately thirty...</td>\n",
       "      <td>[(plot acting, 0.4788), (acting bland, 0.3978)...</td>\n",
       "      <td>[(acting bland, 0.6721), (plot acting, 0.4434)...</td>\n",
       "      <td>[(acting bland, 0.6721), (unhappy decision, 0....</td>\n",
       "      <td>[(plot acting, 0.5633), (acting bland, 0.4953)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt3783958</td>\n",
       "      <td>3755401</td>\n",
       "      <td>La La Land is so beautiful, poetic and full of...</td>\n",
       "      <td>[(la land, 0.5985), (dreamy land, 0.5537), (la...</td>\n",
       "      <td>[(land beautiful, 0.7193), (chazelle amazing, ...</td>\n",
       "      <td>[(beautiful, 0.5414), (really touched, 0.418),...</td>\n",
       "      <td>[(dreamy land, 0.6607), (land dreamy, 0.6456),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt3783958</td>\n",
       "      <td>3752426</td>\n",
       "      <td>I can't believe this movie was nominated for b...</td>\n",
       "      <td>[(movie nominated, 0.621), (budget film, 0.550...</td>\n",
       "      <td>[(budget film, 0.4371), (movie nominated, 0.38...</td>\n",
       "      <td>[(picture slow, 0.4537), (read budget, 0.3471)...</td>\n",
       "      <td>[(movie nominated, 0.678), (budget film, 0.617...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Movie_ID Review_ID                                Preprocessed_Review  \\\n",
       "0  tt3783958   4027807  I simply cannot understand why this film rated...   \n",
       "1  tt3783958   3709335  I love movie musicals. They're hard to find th...   \n",
       "2  tt3783958   6633178  Let's just say during the approximately thirty...   \n",
       "3  tt3783958   3755401  La La Land is so beautiful, poetic and full of...   \n",
       "4  tt3783958   3752426  I can't believe this movie was nominated for b...   \n",
       "\n",
       "                                       keywords_base  \\\n",
       "0  [(film rated, 0.5212), (film musicals, 0.4781)...   \n",
       "1  [(movie musicals, 0.6412), (movie musical, 0.6...   \n",
       "2  [(plot acting, 0.4788), (acting bland, 0.3978)...   \n",
       "3  [(la land, 0.5985), (dreamy land, 0.5537), (la...   \n",
       "4  [(movie nominated, 0.621), (budget film, 0.550...   \n",
       "\n",
       "                                   keywords_reranker  \\\n",
       "0  [(film rated, 0.3777), (stone acting, 0.2705),...   \n",
       "1  [(movie musical, 0.7473), (movie musicals, 0.7...   \n",
       "2  [(acting bland, 0.6721), (plot acting, 0.4434)...   \n",
       "3  [(land beautiful, 0.7193), (chazelle amazing, ...   \n",
       "4  [(budget film, 0.4371), (movie nominated, 0.38...   \n",
       "\n",
       "                                  keywords_sentiment  \\\n",
       "0  [(rating, 0.3109), (gosling, 0.2734), (problem...   \n",
       "1  [(singing dancing, 0.6298), (musicals, 0.6156)...   \n",
       "2  [(acting bland, 0.6721), (unhappy decision, 0....   \n",
       "3  [(beautiful, 0.5414), (really touched, 0.418),...   \n",
       "4  [(picture slow, 0.4537), (read budget, 0.3471)...   \n",
       "\n",
       "                                   keywords_metadata  \n",
       "0  [(film rated, 0.6326), (film musicals, 0.5996)...  \n",
       "1  [(movie musicals, 0.6979), (movie musical, 0.6...  \n",
       "2  [(plot acting, 0.5633), (acting bland, 0.4953)...  \n",
       "3  [(dreamy land, 0.6607), (land dreamy, 0.6456),...  \n",
       "4  [(movie nominated, 0.678), (budget film, 0.617...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the input path for the keywords DataFrame\n",
    "input_path = os.path.join(\"../Dataset/Extracted_Keywords\", f\"kw_{movie_name}.pkl\")\n",
    "\n",
    "# Load the DataFrame\n",
    "try:\n",
    "    loaded_df = pd.read_pickle(input_path)\n",
    "    print(f\"Successfully loaded file: {input_path}\\n\")\n",
    "    print(f\"DataFrame shape: {loaded_df.shape}\")\n",
    "    display(loaded_df.head())  # Show the first few rows if in Jupyter\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {input_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
