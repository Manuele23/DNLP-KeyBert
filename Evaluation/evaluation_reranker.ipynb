{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d24b907",
   "metadata": {},
   "source": [
    "# Evaluation of keyBERTSentimentReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd94d3",
   "metadata": {},
   "source": [
    "This notebook evaluates the **Reranker** model, which reorders the keywords predicted by the base KeyBERT model to improve their ranking.\n",
    "\n",
    "Since the reranker **does not alter the keyword set** but only changes their order, traditional set-based metrics such as **Precision**, **Recall**, and **F1-score** will be **identical to those of the Base model**. Therefore, comparing these metrics is not meaningful in this case.\n",
    "\n",
    "Instead, we perform a **global evaluation** using metrics that are sensitive to the **confidence scores** and **ranking** of the predicted keywords:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**:  \n",
    "  These metrics incorporate the confidence assigned to each predicted keyword. A high score assigned to an incorrect keyword negatively impacts performance, while correctly ranked and confident predictions improve the result. This provides a more nuanced view than simple binary matching.\n",
    "\n",
    "- **nDCG@5 with Graded Relevance**:  \n",
    "  This ranking-based metric measures how effectively the most important ground truth keywords are placed near the top of the predicted list. The evaluation uses **graded relevance**, which prioritizes early and meaningful matches. It rewards rankings that better align with the ground truth keyword importance.\n",
    "\n",
    "All metrics are computed **globally across all reviews**, aggregating correct matches, confidence scores, and ranking positions. This allows for a robust assessment of the reranker's overall impact on keyword quality, beyond what is visible from individual reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef28a1",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7576bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm is already installed.\n",
      "numpy is already installed.\n",
      "pandas is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"tqdm\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d9ee24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "import math    # Mathematical functions (e.g., logarithms for nDCG calculation)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c509981",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32f764",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8edb62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05e8a4",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ba7ec",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d122c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"SW_Episode6\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3496a",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics and nDCG@k with Graded Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb085a5",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### **Score-Aware Metrics**\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the model’s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage.\n",
    "- **nDCG@k (Normalized Discounted Cumulative Gain)**: A ranking metric that rewards placing relevant keywords near the top of the prediction list. It uses **graded relevance**, which accounts for the importance of ground truth keywords based on their position.\n",
    "\n",
    "#### **How nDCG@k with Graded Relevance is Computed**\n",
    "\n",
    "1. **Assign graded relevance to ground truth keywords** based on their position $pos_{GT}$ (starting from 0):\n",
    "\n",
    "$$\n",
    "rel_{GT} = \\frac{1}{\\log_2(pos_{GT} + 2)}\n",
    "$$\n",
    "\n",
    "2. **Assign relevance to each predicted keyword at position $i$** (starting from 0), using approximate matching:\n",
    "\n",
    "$$\n",
    "rel_i = \\begin{cases}\n",
    "\\frac{1}{\\log_2(pos_{GT} + 2)} & \\text{if predicted keyword matches GT keyword at } pos_{GT} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. **Compute DCG@k** (Discounted Cumulative Gain):\n",
    "\n",
    "$$\n",
    "DCG@k = \\sum_{i=0}^{k-1} \\frac{rel_i}{\\log_2(i + 2)}\n",
    "$$\n",
    "\n",
    "4. **Compute IDCG@k** (Ideal DCG using the best ranking):\n",
    "\n",
    "$$\n",
    "IDCG@k = \\sum_{i=0}^{k-1} \\frac{rel^*_i}{\\log_2(i + 2)}\n",
    "$$\n",
    "\n",
    "5. **Compute normalized nDCG**:\n",
    "\n",
    "$$\n",
    "nDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "$$\n",
    "\n",
    "#### **Example ($k=5$)**\n",
    "\n",
    "**Ground truth keywords (ranked):**  \n",
    "`[\"fraud\", \"poverty\", \"scam\"]`\n",
    "\n",
    "**Their graded relevance (using $rel_{GT} = 1/\\log_2(pos_{GT}+2)$):**\n",
    "\n",
    "- fraud (position 0): $1 / \\log_2(0+2) = 1.0$\n",
    "- poverty (position 1): $1 / \\log_2(1+2) \\approx 0.6309$\n",
    "- scam (position 2): $1 / \\log_2(2+2) = 0.5$\n",
    "\n",
    "#### **First predicted list**:\n",
    "`[\"scam\", \"family\", \"poverty\", \"cinematography\", \"fraud\"]`\n",
    "\n",
    "**Matches and assigned relevances:**\n",
    "\n",
    "| Predicted keyword | Match         | Relevance |\n",
    "|-------------------|---------------|-----------|\n",
    "| scam              | yes (pos 2)   | 0.5       |\n",
    "| family            | no            | 0         |\n",
    "| poverty           | yes (pos 1)   | 0.6309    |\n",
    "| cinematography    | no            | 0         |\n",
    "| fraud             | yes (pos 0)   | 1.0       |\n",
    "\n",
    "**Compute DCG:**\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0.5}{\\log_2(0 + 2)} + \\frac{0}{\\log_2(1 + 2)} + \\frac{0.6309}{\\log_2(2 + 2)} + \\frac{0}{\\log_2(3 + 2)} + \\frac{1.0}{\\log_2(4 + 2)} \\\\\n",
    "= \\frac{0.5}{1.0} + 0 + \\frac{0.6309}{2.0} + 0 + \\frac{1.0}{2.58496} \\approx 0.5 + 0 + 0.31545 + 0 + 0.38685 = \\mathbf{1.2023}\n",
    "$$\n",
    "\n",
    "**Compute IDCG:**\n",
    "\n",
    "Best possible ranking: `[\"fraud\", \"poverty\", \"scam\"]`  \n",
    "Relevance list: $[1.0, 0.6309, 0.5]$\n",
    "\n",
    "$$\n",
    "IDCG = \\frac{1.0}{\\log_2(0 + 2)} + \\frac{0.6309}{\\log_2(1 + 2)} + \\frac{0.5}{\\log_2(2 + 2)} \\\\\n",
    "= \\frac{1.0}{1.0} + \\frac{0.6309}{1.58496} + \\frac{0.5}{2.0} \\approx 1.0 + 0.3979 + 0.25 = \\mathbf{1.6479}\n",
    "$$\n",
    "\n",
    "**nDCG@5:**\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.2023}{1.6479} \\approx \\mathbf{0.7294}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Second predicted list**:\n",
    "`[\"fraud\", \"poverty\", \"scam\", \"family\", \"cinematography\"]`\n",
    "\n",
    "**All matches in top-3, correct order:**\n",
    "\n",
    "| Predicted keyword | Match         | Relevance |\n",
    "|-------------------|---------------|-----------|\n",
    "| fraud             | yes (pos 0)   | 1.0       |\n",
    "| poverty           | yes (pos 1)   | 0.6309    |\n",
    "| scam              | yes (pos 2)   | 0.5       |\n",
    "| family            | no            | 0         |\n",
    "| cinematography    | no            | 0         |\n",
    "\n",
    "**Compute DCG:**\n",
    "\n",
    "$$\n",
    "DCG = \\frac{1.0}{\\log_2(0 + 2)} + \\frac{0.6309}{\\log_2(1 + 2)} + \\frac{0.5}{\\log_2(2 + 2)} + 0 + 0 \\\\\n",
    "= 1.0 + 0.3979 + 0.25 = \\mathbf{1.6479}\n",
    "$$\n",
    "\n",
    "**nDCG@5:**\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.6479}{1.6479} = \\mathbf{1.0}\n",
    "$$\n",
    "\n",
    "\n",
    "#### **Interpretation**\n",
    "\n",
    "- When relevant keywords appear early in the predicted list, the score increases due to less discounting.\n",
    "- When relevant keywords are ranked lower, the score decreases due to higher discounting.\n",
    "- **nDCG@k rewards both correct predictions and their correct ranking**, making it suitable for evaluating keyword extractors that produce ranked lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16ed8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_kw(kw):\n",
    "    \"\"\"\n",
    "    Normalize a keyword string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation and non-alphanumeric characters (except spaces)\n",
    "    - Stripping leading and trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        kw (str): The keyword string to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized keyword.\n",
    "    \"\"\"\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumeric characters and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    \"\"\"\n",
    "    Check if a predicted keyword approximately matches any ground truth keyword.\n",
    "\n",
    "    A match is considered approximate if:\n",
    "    - The predicted keyword is exactly equal to a ground truth keyword\n",
    "    - OR the predicted keyword is a substring of a ground truth keyword\n",
    "    - OR a ground truth keyword is a substring of the predicted one\n",
    "\n",
    "    Args:\n",
    "        kw (str): The normalized predicted keyword.\n",
    "        gt_keywords (List[str]): A list of normalized ground truth keywords.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an approximate match is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_keywords_weighted(all_predicted_kw_score, all_gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate global weighted precision, recall, and F1-score across multiple reviews.\n",
    "\n",
    "    This function accounts for confidence scores assigned to predicted keywords.\n",
    "    Matching is performed using approximate matching. Each keyword score contributes\n",
    "    to the precision and recall based on whether it matches a ground truth keyword.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review.\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Weighted precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    total_score = 0.0         # Sum of all predicted keyword scores\n",
    "    match_score = 0.0         # Sum of scores of correctly predicted keywords\n",
    "    total_gt = 0              # Total number of ground truth keywords across all reviews\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize keywords\n",
    "        gt_kw = [normalize_kw(k) for k in gt_kw]\n",
    "        pred_kw_score = [\n",
    "            (normalize_kw(kw), score) for kw, score in pred_kw_score if isinstance(kw, str)\n",
    "        ]\n",
    "\n",
    "        total_score += sum(score for _, score in pred_kw_score)\n",
    "        total_gt += len(gt_kw)\n",
    "\n",
    "        matched_gts = set()  # Track ground truth keywords already matched\n",
    "\n",
    "        for kw, score in pred_kw_score:\n",
    "            for gt in gt_kw:\n",
    "                if gt not in matched_gts and is_approx_match(kw, [gt]):\n",
    "                    match_score += score\n",
    "                    matched_gts.add(gt)\n",
    "                    break\n",
    "\n",
    "    precision = match_score / total_score if total_score > 0 else 0\n",
    "    recall = match_score / total_gt if total_gt > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def compute_global_ndcg(all_predicted_kw_score, all_gt_keywords, k=5):\n",
    "    \"\"\"\n",
    "    Compute global average nDCG@k (Normalized Discounted Cumulative Gain) over multiple reviews.\n",
    "\n",
    "    The relevance of each predicted keyword is based on the position of its best matching\n",
    "    ground truth keyword. Matching is done via approximate matching. The ideal DCG assumes\n",
    "    the best possible ranking of ground truth keywords.\n",
    "\n",
    "    Args:\n",
    "        all_predicted_kw_score (List[List[Tuple[str, float]]]): \n",
    "            A list of predicted keyword-score pairs per review (ranked list).\n",
    "        all_gt_keywords (List[List[str]]): \n",
    "            A list of ground truth keyword lists per review.\n",
    "        k (int): The number of top predicted keywords to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average nDCG@k across all reviews.\n",
    "    \"\"\"\n",
    "    total_ndcg = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for pred_kw_score, gt_kw in zip(all_predicted_kw_score, all_gt_keywords):\n",
    "        # Normalize predicted and ground truth keywords\n",
    "        gt_keywords_norm = [normalize_kw(k) for k in gt_kw]\n",
    "        pred_keywords_norm = [normalize_kw(kw) for kw, _ in pred_kw_score[:k]]\n",
    "\n",
    "        relevance = []  # Relevance scores assigned to predicted keywords\n",
    "\n",
    "        for pk in pred_keywords_norm:\n",
    "            # Find the best (earliest) match position in the GT list\n",
    "            match_ranks = [\n",
    "                i for i, gk in enumerate(gt_keywords_norm) if is_approx_match(pk, [gk])\n",
    "            ]\n",
    "            if match_ranks:\n",
    "                best_rank = min(match_ranks)\n",
    "                rel = 1 / math.log2(best_rank + 2)  # Graded relevance\n",
    "            else:\n",
    "                rel = 0\n",
    "            relevance.append(rel)\n",
    "\n",
    "        # Compute DCG for predicted keywords\n",
    "        dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "        # Compute IDCG based on ideal ordering of GT keywords\n",
    "        ideal_relevance = [1 / math.log2(i + 2) for i in range(min(k, len(gt_keywords_norm)))]\n",
    "        idcg = sum(ideal_relevance)\n",
    "\n",
    "        if idcg > 0:\n",
    "            total_ndcg += dcg / idcg\n",
    "            count += 1\n",
    "\n",
    "    return total_ndcg / count if count > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586110bf",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983aab1e",
   "metadata": {},
   "source": [
    "In this section, we evaluate the overall performance of each model using **score-aware metrics** computed **globally across all reviews**:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**: These metrics incorporate the **confidence scores** assigned to each predicted keyword, reflecting how much of the model’s confidence is placed on correct predictions.\n",
    "- **nDCG@5 (Normalized Discounted Cumulative Gain)**: Assesses the overall **ranking quality** of the top-5 predicted keywords, rewarding correct keywords that are ranked higher.\n",
    "\n",
    "This global evaluation provides a holistic view of each model’s effectiveness in ranking and selecting relevant keywords across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82245276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fc7b7\">\n",
       "  <caption>Global Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fc7b7_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_fc7b7_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_fc7b7_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "      <th id=\"T_fc7b7_level0_col3\" class=\"col_heading level0 col3\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fc7b7_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_fc7b7_row0_col0\" class=\"data row0 col0\" >0.9155</td>\n",
       "      <td id=\"T_fc7b7_row0_col1\" class=\"data row0 col1\" >0.1789</td>\n",
       "      <td id=\"T_fc7b7_row0_col2\" class=\"data row0 col2\" >0.2993</td>\n",
       "      <td id=\"T_fc7b7_row0_col3\" class=\"data row0 col3\" >0.6973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fc7b7_level0_row1\" class=\"row_heading level0 row1\" >reranker</th>\n",
       "      <td id=\"T_fc7b7_row1_col0\" class=\"data row1 col0\" >0.9190</td>\n",
       "      <td id=\"T_fc7b7_row1_col1\" class=\"data row1 col1\" >0.1642</td>\n",
       "      <td id=\"T_fc7b7_row1_col2\" class=\"data row1 col2\" >0.2786</td>\n",
       "      <td id=\"T_fc7b7_row1_col3\" class=\"data row1 col3\" >0.7023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12488be20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"reranker\"]\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Prepare data structures to hold predictions for each model\n",
    "all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Collect predictions and GT for each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "\n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = [(kw, score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "            # Remove duplicates per review\n",
    "            seen = set()\n",
    "            unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "            all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "\n",
    "# Dictionary to store global evaluation results\n",
    "weighted_summary = {}\n",
    "\n",
    "# Evaluate each model globally\n",
    "for model in models_to_evaluate:\n",
    "    preds = all_predicted_kw_score[model]\n",
    "\n",
    "    # Global weighted metrics\n",
    "    w_precision, w_recall, w_f1 = evaluate_keywords_weighted(preds, ground_truth_keywords)\n",
    "\n",
    "    # Global nDCG@5\n",
    "    ndcg = compute_global_ndcg(preds, ground_truth_keywords, k=5)\n",
    "\n",
    "    # Store results\n",
    "    weighted_summary[model] = {\n",
    "        \"weighted_precision\": round(w_precision, 4),\n",
    "        \"weighted_recall\": round(w_recall, 4),\n",
    "        \"weighted_f1\": round(w_f1, 4),\n",
    "        \"ndcg@5\": round(ndcg, 4)\n",
    "    }\n",
    "\n",
    "# Convert summary to DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Models as rows\n",
    "\n",
    "# Rename columns\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "    \"nDCG@5\"\n",
    "]\n",
    "\n",
    "# Display final table\n",
    "summary_df.style.format(precision=4).set_caption(\"Global Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b00c8",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82526943",
   "metadata": {},
   "source": [
    "This section processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains the predicted keywords generated by different models.\n",
    "\n",
    "For **each movie**, the evaluation proceeds as follows:\n",
    "\n",
    "1. **Ground Truth Loading**  \n",
    "   - The reference (ground truth) keywords for all reviews in the movie are loaded.\n",
    "\n",
    "2. **Model Predictions**  \n",
    "   - Predicted keywords from two models are collected and evaluated:\n",
    "     - **Base** model  \n",
    "     - **Reranker** model\n",
    "\n",
    "3. **Global Metrics Computation**  \n",
    "   Rather than evaluating each review individually, metrics are computed **globally**, by aggregating all predictions and ground truths across the entire movie.\n",
    "\n",
    "   **Score-aware Weighted Metrics**  \n",
    "   These metrics incorporate the confidence scores assigned to the predicted keywords:\n",
    "   - **Weighted Precision**: Proportion of the model’s total confidence assigned to correct predictions.\n",
    "   - **Weighted Recall**: Fraction of the ground truth captured by the confidence-weighted predictions.\n",
    "   - **Weighted F1-score**: Harmonic mean of weighted precision and recall.\n",
    "\n",
    "   **Ranking Metric**\n",
    "   Measures the effectiveness of keyword ordering:\n",
    "   - **nDCG@5** (Normalized Discounted Cumulative Gain at rank 5):  \n",
    "     Reflects how well the model ranks the most important ground truth keywords near the top of the prediction list.\n",
    "  \n",
    "The results are compiled into a **summary table** to compare the overall ranking and confidence-aware performance of the **Base** and **Reranker** models across the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3ceb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_aa685\">\n",
       "  <caption>Global Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_aa685_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_aa685_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_aa685_level0_col2\" class=\"col_heading level0 col2\" >Weighted Precision</th>\n",
       "      <th id=\"T_aa685_level0_col3\" class=\"col_heading level0 col3\" >Weighted Recall</th>\n",
       "      <th id=\"T_aa685_level0_col4\" class=\"col_heading level0 col4\" >Weighted F1-score</th>\n",
       "      <th id=\"T_aa685_level0_col5\" class=\"col_heading level0 col5\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_aa685_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_aa685_row0_col0\" class=\"data row0 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_aa685_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_aa685_row0_col2\" class=\"data row0 col2\" >0.9155</td>\n",
       "      <td id=\"T_aa685_row0_col3\" class=\"data row0 col3\" >0.1789</td>\n",
       "      <td id=\"T_aa685_row0_col4\" class=\"data row0 col4\" >0.2993</td>\n",
       "      <td id=\"T_aa685_row0_col5\" class=\"data row0 col5\" >0.6973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_aa685_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_aa685_row1_col0\" class=\"data row1 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_aa685_row1_col1\" class=\"data row1 col1\" >reranker</td>\n",
       "      <td id=\"T_aa685_row1_col2\" class=\"data row1 col2\" >0.9190</td>\n",
       "      <td id=\"T_aa685_row1_col3\" class=\"data row1 col3\" >0.1642</td>\n",
       "      <td id=\"T_aa685_row1_col4\" class=\"data row1 col4\" >0.2786</td>\n",
       "      <td id=\"T_aa685_row1_col5\" class=\"data row1 col5\" >0.7023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x124898850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Settings\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "models_to_evaluate = [\"base\", \"reranker\"]\n",
    "\n",
    "# Load ground truth\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over movie keyword predictions\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Ground truth for the film\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Init: predicted keyword lists (per review, no duplicates)\n",
    "            all_predicted_kw_score = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            for _, row in selected_film.iterrows():\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "                        predicted_kw_score = [(normalize_kw(kw), score) for kw, score in row[pred_col] if isinstance(kw, str)]\n",
    "                        seen = set()\n",
    "                        unique_pred = [(kw, score) for kw, score in predicted_kw_score if kw not in seen and not seen.add(kw)]\n",
    "                        all_predicted_kw_score[model].append(unique_pred)\n",
    "\n",
    "            for model in models_to_evaluate:\n",
    "                # Flatten keywords (global unique list)\n",
    "                flat_kw = [kw for review in all_predicted_kw_score[model] for kw, _ in review]\n",
    "                unique_pred_keywords = list(set(flat_kw))\n",
    "\n",
    "                # Max score for each keyword\n",
    "                kw_score_max = {}\n",
    "                for review in all_predicted_kw_score[model]:\n",
    "                    for kw, score in review:\n",
    "                        if kw not in kw_score_max or score > kw_score_max[kw]:\n",
    "                            kw_score_max[kw] = score\n",
    "                sorted_kw_score = sorted(kw_score_max.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Score-aware format\n",
    "                pred_kw_score_list = all_predicted_kw_score[model]\n",
    "                keyword_only_list = [[kw for kw, _ in review] for review in pred_kw_score_list]\n",
    "\n",
    "                # Compute all metrics\n",
    "                wp, wr, wf = evaluate_keywords_weighted(pred_kw_score_list, gt_keywords)\n",
    "                ndcg = compute_global_ndcg(pred_kw_score_list, gt_keywords, k=5)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Movie\": movie_name,\n",
    "                    \"Model\": model,\n",
    "                    \"Weighted Precision\": round(wp, 4),\n",
    "                    \"Weighted Recall\": round(wr, 4),\n",
    "                    \"Weighted F1-score\": round(wf, 4),\n",
    "                    \"nDCG@5\": round(ndcg, 4)\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Final summary\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Global Evaluation Summary per Movie and Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
