{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d24b907",
   "metadata": {},
   "source": [
    "# Evaluation of keyBERTSentimentReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd94d3",
   "metadata": {},
   "source": [
    "This notebook evaluates the **reranker** model, which reorders the keywords predicted by the base KeyBERT model to improve their ranking.\n",
    "\n",
    "Since the reranker **does not change the keyword set** but only their order, traditional metrics like **precision, recall, and F1-score** will be **identical to those of the base model**. Thus, comparing these metrics between base and reranker is not informative.\n",
    "\n",
    "Instead, we focus on three score-aware metrics that capture ranking quality and confidence weighting:\n",
    "\n",
    "- **Weighted Precision, Recall, and F1-score**:  \n",
    "  These metrics incorporate the confidence scores assigned by the model to each predicted keyword. They measure not only correctness but also how confidently the model ranks the relevant keywords, providing a finer-grained evaluation than unweighted scores.\n",
    "\n",
    "- **nDCG@5 with Graded Relevance**:  \n",
    "  This metric evaluates how well the reranker places the most important ground truth keywords near the top of the predicted list. It rewards rankings that better align with the ground truth keyword importance, revealing improvements in ranking quality.\n",
    "\n",
    "Together, these metrics provide a comprehensive assessment of the **ranking effectiveness, confidence weighting, and diversity** introduced by the reranker, beyond what simple set-based metrics can capture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef28a1",
   "metadata": {},
   "source": [
    "## Setup: Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7576bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm is already installed.\n",
      "pandas is already installed.\n",
      "numpy is already installed.\n",
      "Installing scikit-learn...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = {\n",
    "    \"pandas\", \"numpy\", \"scikit-learn\", \"tqdm\"\n",
    "}\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9ee24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os      # File system operations (e.g., listing files)\n",
    "import re      # Regular expressions for text processing\n",
    "import math    # Mathematical functions (e.g., logarithms for nDCG calculation)\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd                  # Data manipulation with DataFrames\n",
    "import numpy as np                   # Numerical computations and array operations\n",
    "from tqdm import tqdm                # Progress bars for loops\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c509981",
   "metadata": {},
   "source": [
    "## Load Available Movies from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32f764",
   "metadata": {},
   "source": [
    "This section lists all the available movies stored as `.pkl` files inside the review dataset directory.\n",
    "\n",
    "- It defines the root path (`../Dataset/Reviews_By_Movie`) where all review files are saved.\n",
    "- It automatically detects and lists all movie filenames (removing the `.pkl` extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available movies: ['GoodBadUgly', 'HarryPotter', 'IndianaJones', 'LaLaLand', 'Oppenheimer', 'Parasite', 'SW_Episode1', 'SW_Episode2', 'SW_Episode3', 'SW_Episode4', 'SW_Episode5', 'SW_Episode6', 'SW_Episode7', 'SW_Episode8', 'SW_Episode9']\n"
     ]
    }
   ],
   "source": [
    "# Define root directory\n",
    "root_dir = \"../Dataset/Reviews_By_Movie\"\n",
    "\n",
    "# List all available movies\n",
    "available_movies = sorted([f[:-4] for f in os.listdir(root_dir) if f.endswith(\".pkl\")])\n",
    "print(\"Available movies:\", available_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05e8a4",
   "metadata": {},
   "source": [
    "## Select a Movie and Load its Ground Truth Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ba7ec",
   "metadata": {},
   "source": [
    "In this step, we load the keyword extraction results for a specific movie and retrieve the corresponding ground truth keywords. The goal is to use these annotated keywords for evaluation and comparison with automatically extracted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d122c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the movie to be evaluated\n",
    "movie_name = \"SW_Episode6\"\n",
    "\n",
    "# Load the extracted keywords for the selected movie from a pickle file\n",
    "# The file path is dynamically built using the movie name\n",
    "selected_film = pd.read_pickle(f\"../Dataset/Extracted_Keywords/kw_{movie_name}.pkl\")\n",
    "\n",
    "# Retrieve the Movie_ID of the selected film\n",
    "# Assumes that the file contains a DataFrame with at least one row\n",
    "selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "# Load the full dataset containing the ground truth keywords\n",
    "# for all movies in the evaluation set\n",
    "keywords = pd.read_pickle(\"../Dataset/keywords_ground_truth.pkl\")\n",
    "\n",
    "# Filter the ground truth dataset to extract only the keywords for the selected movie\n",
    "kw_ground_truth = keywords[keywords[\"Movie_ID\"] == selected_film_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3496a",
   "metadata": {},
   "source": [
    "## Score-Aware Evaluation: Weighted Metrics and nDCG@k with Graded Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb085a5",
   "metadata": {},
   "source": [
    "This extended evaluation considers the **confidence scores** assigned by the model to each predicted keyword, allowing us to measure not only whether the predictions are correct but also how confidently and effectively they are ranked.\n",
    "\n",
    "#### Score-Aware Metrics\n",
    "\n",
    "- **Weighted Precision**: Reflects the proportion of the model’s total confidence assigned to correct keywords. High confidence in incorrect predictions lowers this score.\n",
    "\n",
    "- **Weighted Recall**: Measures how much of the ground truth is recovered, weighted by the confidence of correct predictions.\n",
    "\n",
    "- **Weighted F1-score**: The harmonic mean of weighted precision and recall, balancing accuracy with coverage.\n",
    "\n",
    "- **nDCG@k (Normalized Discounted Cumulative Gain)**: A ranking metric that rewards placing relevant keywords near the top of the prediction list. It uses **graded relevance**, which accounts for the importance of ground truth keywords based on their position.\n",
    "\n",
    "#### How nDCG@k with Graded Relevance is Computed\n",
    "\n",
    "1. **Assign graded relevance to ground truth keywords** based on their position $pos_{GT}$ (starting from 0). The relevance for a ground truth keyword at position $pos_{GT}$ is:\n",
    "\n",
    "   $$\n",
    "   rel_{GT} = \\frac{1}{\\log_2(pos_{GT} + 2)}\n",
    "   $$\n",
    "\n",
    "   Higher ranked keywords (lower $pos_{GT}$) have higher relevance scores.\n",
    "\n",
    "2. **For each predicted keyword at position $i$ (starting from 0)**, find the best matching ground truth keyword (using approximate matching). Assign the relevance of the predicted keyword $rel_i$ as the graded relevance of its matched ground truth keyword:\n",
    "\n",
    "   $$\n",
    "   rel_i = \\begin{cases}\n",
    "   \\frac{1}{\\log_2(pos_{GT} + 2)} & \\text{if predicted keyword matches GT keyword at } pos_{GT} \\\\\n",
    "   0 & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "3. **Compute Discounted Cumulative Gain (DCG) for the predicted keywords up to rank $k$**:\n",
    "\n",
    "   $$\n",
    "   DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i + 1)}\n",
    "   $$\n",
    "\n",
    "   This discounts the relevance by the predicted keyword’s position, rewarding relevant keywords ranked higher.\n",
    "\n",
    "4. **Compute Ideal DCG (IDCG)** as the maximum possible DCG using the top-$k$ ground truth keywords ranked by their graded relevance:\n",
    "\n",
    "   $$\n",
    "   IDCG@k = \\sum_{i=1}^{k} \\frac{rel^{*}_i}{\\log_2(i + 1)}\n",
    "   $$\n",
    "\n",
    "   where $rel^{*}_i$ are the graded relevance scores of the top-$k$ ground truth keywords sorted by importance.\n",
    "\n",
    "5. **Calculate normalized DCG (nDCG)** by dividing DCG by IDCG:\n",
    "\n",
    "   $$\n",
    "   nDCG@k = \\frac{DCG@k}{IDCG@k}\n",
    "   $$\n",
    "\n",
    "### Example ($k=5$)\n",
    "\n",
    "Ground truth keywords ranked by importance:  \n",
    "`[\"fraud\", \"poverty\", \"scam\"]`\n",
    "\n",
    "Their graded relevance:  \n",
    "- \"fraud\" at position 0 → $rel_{GT} = \\frac{1}{\\log_2(0 + 2)} = 1.0$  \n",
    "- \"poverty\" at position 1 → $rel_{GT} = \\frac{1}{\\log_2(1 + 2)} = 0.63$  \n",
    "- \"scam\" at position 2 → $rel_{GT} = \\frac{1}{\\log_2(2 + 2)} = 0.5$\n",
    "\n",
    "**Predicted keywords in order:**  \n",
    "`[\"scam\", \"family\", \"poverty\", \"cinematography\", \"fraud\"]`\n",
    "\n",
    "Matching relevances assigned to predicted keywords:  \n",
    "- \"scam\" matches GT at pos 2 → $rel_0 = 0.5$  \n",
    "- \"family\" no match → $rel_1 = 0$  \n",
    "- \"poverty\" matches GT at pos 1 → $rel_2 = 0.63$  \n",
    "- \"cinematography\" no match → $rel_3 = 0$  \n",
    "- \"fraud\" matches GT at pos 0 → $rel_4 = 1.0$\n",
    "\n",
    "Compute DCG:\n",
    "\n",
    "$$\n",
    "DCG = \\frac{0.5}{\\log_2(1 + 1)} + \\frac{0}{\\log_2(2 + 1)} + \\frac{0.63}{\\log_2(3 + 1)} + \\frac{0}{\\log_2(4 + 1)} + \\frac{1.0}{\\log_2(5 + 1)} \\approx 0.5 + 0 + 0.315 + 0 + 0.387 = 1.202\n",
    "$$\n",
    "\n",
    "Compute IDCG (ideal predicted order: \"fraud\", \"poverty\", \"scam\"):\n",
    "\n",
    "$$\n",
    "IDCG = \\frac{1.0}{\\log_2(1 + 1)} + \\frac{0.63}{\\log_2(2 + 1)} + \\frac{0.5}{\\log_2(3 + 1)} = 1.0 + 0.397 + 0.25 = 1.647\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.202}{1.647} \\approx 0.73\n",
    "$$\n",
    "\n",
    "**Change predicted order to:**  \n",
    "`[\"fraud\", \"poverty\", \"scam\", \"family\", \"cinematography\"]`\n",
    "\n",
    "Relevances for predicted keywords:\n",
    "\n",
    "- \"fraud\" matches GT at pos 0 → $rel_0 = 1.0$  \n",
    "- \"poverty\" matches GT at pos 1 → $rel_1 = 0.63$  \n",
    "- \"scam\" matches GT at pos 2 → $rel_2 = 0.5$  \n",
    "- \"family\" no match → $rel_3 = 0$  \n",
    "- \"cinematography\" no match → $rel_4 = 0$\n",
    "\n",
    "Compute DCG:\n",
    "\n",
    "$$\n",
    "DCG = \\frac{1.0}{\\log_2(1 + 1)} + \\frac{0.63}{\\log_2(2 + 1)} + \\frac{0.5}{\\log_2(3 + 1)} + 0 + 0 = 1.0 + 0.397 + 0.25 = 1.647\n",
    "$$\n",
    "\n",
    "IDCG is the same as before.\n",
    "\n",
    "$$\n",
    "nDCG@5 = \\frac{1.647}{1.647} = 1.0\n",
    "$$\n",
    "\n",
    "Changing the order of predicted keywords **does affect** the nDCG score: placing highly relevant keywords earlier leads to higher nDCG, reflecting better ranking quality.\n",
    "\n",
    "- When relevant keywords appear early in the predicted list, the score increases due to less discounting.\n",
    "- Conversely, when relevant keywords are placed lower, the score decreases because of higher discounting.\n",
    "- This metric thus rewards **both correct prediction and the quality of their ranking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ed8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple normalization function for keywords:\n",
    "# - Converts to lowercase\n",
    "# - Removes punctuation\n",
    "# - Strips leading/trailing spaces\n",
    "def normalize_kw(kw):\n",
    "    kw = kw.lower()\n",
    "    kw = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", kw)  # Keep only alphanumerics and whitespace\n",
    "    return kw.strip()\n",
    "\n",
    "\n",
    "# Approximate matching function:\n",
    "# Returns True if the predicted keyword matches any ground truth keyword\n",
    "# using a relaxed comparison: exact match or substring containment\n",
    "def is_approx_match(kw, gt_keywords):\n",
    "    for gt in gt_keywords:\n",
    "        if kw == gt or kw in gt or gt in kw:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Weighted evaluation function:\n",
    "# Calculates precision, recall, and F1-score using the confidence scores of predicted keywords\n",
    "# - High-confidence correct predictions contribute more\n",
    "# - Precision is score-weighted; recall divides by total ground truth\n",
    "def evaluate_keywords_weighted(predicted_kw_score, gt_keywords):\n",
    "    \"\"\"\n",
    "    Evaluate predicted keywords with confidence scores using weighted precision, recall, and F1.\n",
    "    \n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with associated confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords (annotated)\n",
    "    \n",
    "    Returns:\n",
    "        (precision, recall, f1): all metrics computed using score-weighted matching\n",
    "    \"\"\"\n",
    "    # Normalize both predicted and ground truth keywords\n",
    "    gt_keywords = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords = [(normalize_kw(kw), score) for kw, score in predicted_kw_score if isinstance(kw, str)]\n",
    "    \n",
    "    total_score = sum(score for _, score in pred_keywords)\n",
    "    if total_score == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    # Compute total score of predicted keywords that approximately match the ground truth\n",
    "    match_score = sum(score for kw, score in pred_keywords if is_approx_match(kw, gt_keywords))\n",
    "    \n",
    "    # Weighted precision and recall\n",
    "    precision = match_score / total_score\n",
    "    recall = match_score / len(gt_keywords) if gt_keywords else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Ranking-based evaluation function:\n",
    "# Computes the normalized Discounted Cumulative Gain (nDCG@k) for predicted keywords\n",
    "# Gives more credit when correct keywords appear earlier in the ranking\n",
    "def compute_ndcg(predicted_kw_score, gt_keywords, k=5):\n",
    "    \"\"\"\n",
    "    Compute nDCG@k between predicted keywords (with scores) and ground truth keywords,\n",
    "    using graded relevance based on ground truth ranking and approximate matching.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_kw_score (list of (str, float)): predicted keywords with confidence scores\n",
    "        gt_keywords (list of str): ground truth keywords ordered by importance\n",
    "        k (int): top-k keywords to consider\n",
    "\n",
    "    Returns:\n",
    "        float: normalized DCG score\n",
    "    \"\"\"\n",
    "    # Normalize ground truth and predicted keywords\n",
    "    gt_keywords_norm = [normalize_kw(k) for k in gt_keywords]\n",
    "    pred_keywords_norm = [normalize_kw(kw) for kw, _ in predicted_kw_score[:k]]\n",
    "\n",
    "    relevance = []\n",
    "    for pk in pred_keywords_norm:\n",
    "        # Find ranks of all GT keywords matching predicted keyword approx.\n",
    "        match_ranks = [i for i, gk in enumerate(gt_keywords_norm) if is_approx_match(pk, [gk])]\n",
    "        if match_ranks:\n",
    "            # Assign relevance inversely proportional to rank (log discount)\n",
    "            best_rank = min(match_ranks)\n",
    "            rel = 1 / math.log2(best_rank + 2)  # +2 since ranks start at 0\n",
    "        else:\n",
    "            rel = 0\n",
    "        relevance.append(rel)\n",
    "\n",
    "    # Compute DCG with graded relevance\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    # Compute ideal DCG (IDCG) assuming best possible ordering (top k GT keywords)\n",
    "    ideal_relevance = [1 / math.log2(i + 2) for i in range(min(k, len(gt_keywords_norm)))]\n",
    "    idcg = sum(ideal_relevance)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586110bf",
   "metadata": {},
   "source": [
    "### Evaluate and Compare Models on Keyword Extraction (Weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993b8c5",
   "metadata": {},
   "source": [
    "In this section, we apply the score-aware evaluation metrics to each review for both models:\n",
    "\n",
    "- **Weighted Precision, Recall, F1**: accounts for the confidence scores of each predicted keyword.\n",
    "- **nDCG@5**: evaluates the ranking quality of the top-5 keywords based on their alignment with the ground truth.\n",
    "\n",
    "Each review is evaluated individually, and the metrics are then averaged across all reviews to summarize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82245276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4f3d2\">\n",
       "  <caption>Score-Aware Evaluation Summary</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4f3d2_level0_col0\" class=\"col_heading level0 col0\" >Weighted Precision</th>\n",
       "      <th id=\"T_4f3d2_level0_col1\" class=\"col_heading level0 col1\" >Weighted Recall</th>\n",
       "      <th id=\"T_4f3d2_level0_col2\" class=\"col_heading level0 col2\" >Weighted F1-score</th>\n",
       "      <th id=\"T_4f3d2_level0_col3\" class=\"col_heading level0 col3\" >nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4f3d2_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_4f3d2_row0_col0\" class=\"data row0 col0\" >0.5539</td>\n",
       "      <td id=\"T_4f3d2_row0_col1\" class=\"data row0 col1\" >0.0052</td>\n",
       "      <td id=\"T_4f3d2_row0_col2\" class=\"data row0 col2\" >0.0102</td>\n",
       "      <td id=\"T_4f3d2_row0_col3\" class=\"data row0 col3\" >0.2272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4f3d2_level0_row1\" class=\"row_heading level0 row1\" >reranker</th>\n",
       "      <td id=\"T_4f3d2_row1_col0\" class=\"data row1 col0\" >0.5438</td>\n",
       "      <td id=\"T_4f3d2_row1_col1\" class=\"data row1 col1\" >0.0046</td>\n",
       "      <td id=\"T_4f3d2_row1_col2\" class=\"data row1 col2\" >0.0092</td>\n",
       "      <td id=\"T_4f3d2_row1_col3\" class=\"data row1 col3\" >0.2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21e541200e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to evaluate\n",
    "models_to_evaluate = [\"base\", \"reranker\"]\n",
    "\n",
    "# Initialize results dictionary for weighted metrics and nDCG\n",
    "weighted_results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "# Ground truth keywords (same for all reviews in the selected film)\n",
    "ground_truth_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "# Loop through each review\n",
    "for _, row in selected_film.iterrows():\n",
    "    for model in models_to_evaluate:\n",
    "        pred_col = f\"keywords_{model}\"\n",
    "        \n",
    "        # Skip if no prediction or wrong format\n",
    "        if pred_col in row and isinstance(row[pred_col], list):\n",
    "            predicted_kw_score = row[pred_col]  # list of (kw, score)\n",
    "\n",
    "            # Compute weighted metrics\n",
    "            w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, ground_truth_keywords)\n",
    "\n",
    "            # Compute nDCG@5\n",
    "            ndcg = compute_ndcg(predicted_kw_score, ground_truth_keywords, k=5)\n",
    "\n",
    "            # Save results\n",
    "            weighted_results[model].append({\n",
    "                \"weighted_precision\": w_precision,\n",
    "                \"weighted_recall\": w_recall,\n",
    "                \"weighted_f1\": w_f1,\n",
    "                \"ndcg@5\": ndcg\n",
    "            })\n",
    "\n",
    "# Compute average metrics across all reviews\n",
    "weighted_summary = {}\n",
    "for model in models_to_evaluate:\n",
    "    metrics = weighted_results[model]\n",
    "    weighted_summary[model] = {\n",
    "        \"avg_weighted_precision\": round(np.mean([m[\"weighted_precision\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_recall\": round(np.mean([m[\"weighted_recall\"] for m in metrics]), 4),\n",
    "        \"avg_weighted_f1\": round(np.mean([m[\"weighted_f1\"] for m in metrics]), 4),\n",
    "        \"avg_ndcg@5\": round(np.mean([m[\"ndcg@5\"] for m in metrics]), 4)\n",
    "    }\n",
    "\n",
    "# Convert the weighted summary dictionary to a pandas DataFrame\n",
    "summary_df = pd.DataFrame(weighted_summary).T  # Transpose so models are rows\n",
    "\n",
    "# Rename columns for better readability\n",
    "summary_df.columns = [\n",
    "    \"Weighted Precision\",\n",
    "    \"Weighted Recall\",\n",
    "    \"Weighted F1-score\",\n",
    "    \"nDCG@5\"\n",
    "]\n",
    "\n",
    "# Display the summary table\n",
    "summary_df.style.format(precision=4).set_caption(\"Score-Aware Evaluation Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b00c8",
   "metadata": {},
   "source": [
    "## Evaluation Across All Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82526943",
   "metadata": {},
   "source": [
    "This section processes all `.pkl` files in the `Extracted_Keywords` directory, where each file corresponds to a single movie and contains the predicted keywords generated by different models.\n",
    "\n",
    "For **each movie**:\n",
    "\n",
    "1. **Ground Truth Loading**  \n",
    "   - The reference (ground truth) keywords for each review are loaded.\n",
    "\n",
    "2. **Model Predictions**  \n",
    "   - Predicted keywords from two models are evaluated:\n",
    "     - **Base** model  \n",
    "     - **Reranker** model\n",
    "\n",
    "3. **Metrics Computation**  \n",
    "   For each review, two groups of metrics are calculated:\n",
    "\n",
    "   **Score-aware Weighted Metrics**\n",
    "   These metrics account for the confidence of each prediction:\n",
    "   - **Weighted Precision**\n",
    "   - **Weighted Recall**\n",
    "   - **Weighted F1 Score**\n",
    "\n",
    "   **Ranking Metric**\n",
    "   Evaluates the ordering of predicted keywords:\n",
    "   - **nDCG@5** (Normalized Discounted Cumulative Gain at rank 5): Rewards relevant keywords that appear higher in the prediction list.\n",
    "\n",
    "Metrics are **averaged per movie** and **per model**.\n",
    "\n",
    "All results are compiled into a **summary table** to compare the performance of the **Base** and **Reranker** models across the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a3ceb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e034a\">\n",
       "  <caption>Full Evaluation Summary per Movie and Model</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e034a_level0_col0\" class=\"col_heading level0 col0\" >Movie</th>\n",
       "      <th id=\"T_e034a_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
       "      <th id=\"T_e034a_level0_col2\" class=\"col_heading level0 col2\" >Avg_Weighted_Precision</th>\n",
       "      <th id=\"T_e034a_level0_col3\" class=\"col_heading level0 col3\" >Avg_Weighted_Recall</th>\n",
       "      <th id=\"T_e034a_level0_col4\" class=\"col_heading level0 col4\" >Avg_Weighted_F1</th>\n",
       "      <th id=\"T_e034a_level0_col5\" class=\"col_heading level0 col5\" >Avg_nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e034a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e034a_row0_col0\" class=\"data row0 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_e034a_row0_col1\" class=\"data row0 col1\" >base</td>\n",
       "      <td id=\"T_e034a_row0_col2\" class=\"data row0 col2\" >0.5539</td>\n",
       "      <td id=\"T_e034a_row0_col3\" class=\"data row0 col3\" >0.0052</td>\n",
       "      <td id=\"T_e034a_row0_col4\" class=\"data row0 col4\" >0.0102</td>\n",
       "      <td id=\"T_e034a_row0_col5\" class=\"data row0 col5\" >0.2272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e034a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e034a_row1_col0\" class=\"data row1 col0\" >SW_Episode6</td>\n",
       "      <td id=\"T_e034a_row1_col1\" class=\"data row1 col1\" >reranker</td>\n",
       "      <td id=\"T_e034a_row1_col2\" class=\"data row1 col2\" >0.5438</td>\n",
       "      <td id=\"T_e034a_row1_col3\" class=\"data row1 col3\" >0.0046</td>\n",
       "      <td id=\"T_e034a_row1_col4\" class=\"data row1 col4\" >0.0092</td>\n",
       "      <td id=\"T_e034a_row1_col5\" class=\"data row1 col5\" >0.2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x21e545a9820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "keywords_dir = \"../Dataset/Extracted_Keywords/\"\n",
    "ground_truth_path = \"../Dataset/keywords_ground_truth.pkl\"\n",
    "\n",
    "# Load the ground truth once for all movies\n",
    "keywords_ground_truth = pd.read_pickle(ground_truth_path)\n",
    "\n",
    "# List of models to evaluate\n",
    "models_to_evaluate = [\"base\", \"reranker\"]\n",
    "\n",
    "# Store results across all movies\n",
    "all_results = []\n",
    "\n",
    "# Iterate over all keyword prediction files\n",
    "for file in os.listdir(keywords_dir):\n",
    "    if file.endswith(\".pkl\") and file.startswith(\"kw_\"):\n",
    "        movie_name = file.replace(\"kw_\", \"\").replace(\".pkl\", \"\")\n",
    "        file_path = os.path.join(keywords_dir, file)\n",
    "\n",
    "        try:\n",
    "            # Load predicted keywords for the current movie\n",
    "            selected_film = pd.read_pickle(file_path)\n",
    "            selected_film_id = selected_film[\"Movie_ID\"].iloc[0]\n",
    "\n",
    "            # Retrieve ground truth keywords for the selected movie\n",
    "            kw_ground_truth = keywords_ground_truth[keywords_ground_truth[\"Movie_ID\"] == selected_film_id]\n",
    "            gt_keywords = kw_ground_truth[\"Keyword\"].tolist()\n",
    "\n",
    "            # Initialize metrics for each model\n",
    "            results = {model: [] for model in models_to_evaluate}\n",
    "\n",
    "            # Evaluate each review in the dataset\n",
    "            for _, row in selected_film.iterrows():\n",
    "                for model in models_to_evaluate:\n",
    "                    pred_col = f\"keywords_{model}\"\n",
    "\n",
    "                    # Skip if no predictions or wrong format\n",
    "                    if pred_col in row and isinstance(row[pred_col], list):\n",
    "                        predicted_kw_score = row[pred_col]\n",
    "\n",
    "                        # Extract only keyword strings for unweighted evaluation\n",
    "                        pred_kw_only = [kw for kw, _ in predicted_kw_score if isinstance(kw, str)]\n",
    "\n",
    "                        # Compute score-weighted metrics\n",
    "                        w_precision, w_recall, w_f1 = evaluate_keywords_weighted(predicted_kw_score, gt_keywords)\n",
    "\n",
    "                        # Compute ranking-based metric (nDCG@5)\n",
    "                        ndcg = compute_ndcg(predicted_kw_score, gt_keywords, k=5)\n",
    "\n",
    "                        # Store all metrics for this review\n",
    "                        results[model].append({\n",
    "                            \"w_precision\": w_precision,\n",
    "                            \"w_recall\": w_recall,\n",
    "                            \"w_f1\": w_f1,\n",
    "                            \"ndcg@5\": ndcg,\n",
    "                        })\n",
    "\n",
    "            # Compute average metrics per model for the current movie\n",
    "            for model in models_to_evaluate:\n",
    "                if results[model]:\n",
    "                    metrics_df = pd.DataFrame(results[model])\n",
    "                    all_results.append({\n",
    "                        \"Movie\": movie_name,\n",
    "                        \"Model\": model,\n",
    "                        \"Avg_Weighted_Precision\": round(metrics_df[\"w_precision\"].mean(), 4),\n",
    "                        \"Avg_Weighted_Recall\": round(metrics_df[\"w_recall\"].mean(), 4),\n",
    "                        \"Avg_Weighted_F1\": round(metrics_df[\"w_f1\"].mean(), 4),\n",
    "                        \"Avg_nDCG@5\": round(metrics_df[\"ndcg@5\"].mean(), 4),\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Create the final summary DataFrame\n",
    "final_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display table with clean index and sorted values\n",
    "final_df_sorted = final_df.sort_values(by=[\"Movie\", \"Model\"]).reset_index(drop=True)\n",
    "final_df_sorted.style.format(precision=4).set_caption(\"Full Evaluation Summary per Movie and Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
